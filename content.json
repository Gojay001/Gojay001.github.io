{"pages":[{"title":"About Me","text":"A Master of Chongqing University A Computer Vision Practicer Keep Moving","link":"/about/index.html"}],"posts":[{"title":"Overview","text":"There are the overall of papers about Deep Learning.https://github.com/Gojay001/DeepLearning-pwcn Contents Classification LeNet-5 AlexNet NIN(Network In Network) VGG GoogLeNet(Inception-v1) ResNet Inception-v4 DenseNet DLA(Deep Layer Aggregation) ShuffleNet MobileNetV3 Detection One-stage SSD YOLO YOLOv2 RetinaNet YOLOv3 CornerNet CenterNet YOLOv4 YOLOF Two-stage R-CNN SPP Fast R-CNN Faster R-CNN FPN Segmentation FCN U-Net Seg-Net DeepLab V1 PSPNet DeepLab V2 Mask R-CNN DeepLab V3 PointNet PointNet++ DeepLab V3+ DGCNet(Dual GCN) SETR(SEgmentation TRansfomer) Segmenter SegFormer FTN(Fully Transformer Networks) Tracking MOT SORT DeepSORT Tracktor FFT(Flow-Fuse Tracker) JRMOT Tracklet DMCT(Deep Multi-Camera Tracking) FairMOT CenterPoint VOT DepthTrack BinocularTrack SiamFC SiamRPN SiamRPN++ SiamMask GlobalTrack PAMCC-AOT SiamCAR SiamBAN SiamAttn TSDM SiamGAT RE-SiamNets FSS OSLSM co-FCN AMP(Adaptive Masked Proxies) SG-One(Similarity Guidance) CENet(Combinatorial Embedding Network) PANet(Prototype Alignment) CANet(Class Agnostic) PGNet(Pyramid Graph Network) CRNet(Cross-Reference Network) FGN(Fully Guided Network) OTB(On the Texture Bias) LTM(Local Transformation Module) SimPropNet(Similarity Propagation) PPNet(Part-aware Prototype) PFENet(Prior Guided Feature Enrichment Network) PMMs(Prototype Mixture Models) GFS-Seg(Generalized Few-Shot) SCL(Self-Corss Learning) ASGNet(Adaptive Superpixel-guided Network) HSNet(Hypercorrelation Squeeze) BAM 3D-Face 3DMM CameraCalibration Bilinear DDE FaceWarehouse Face2Face DynamicAvatars FLAME Nonlinear DynamicRigidityPrior Deep3D SimpleAnimation RingNet FOCUS MICA HRN Attention Transformer Non-local Image Transformer ViT(Vision Transformer) Swin Transformer ResT DS-Net(Dual Stream Network) TransCNN Shuffle Transformer RGBD-SOT UC-Net JL-DCF(Joint Learning and Densely-Cooperative Fusion) SA-Gate(Separation-and-Aggregation Gate) BiANet(Bilateral Attention Network) DSA^2F(Depth-Sensitive Attention and Automatic Multi-Modal Fusion) Unsupervised SimSiam Detection-3D PV-RCNN FSL RN(Relation Network) GAN GAN BeautyGAN Optimization ReLU Momentum Dropout Adam BN GDoptimization Survey 3D-Detection-Survey-2019 FSL-Survey-2019 MOT-Survey-2020 Transformer-Survey-2021 Image Classification Title Paper Conf Code LeNet-5 Gradient-based learning applied to document recognition IEEE(1998) [code] AlexNet ImageNet Classification with Deep Convolutional Neural Networks NIPS(2012) [code] NIN Network In Network arXiv(2013) PyTorch VGG Very Deep Convolutional Networks for Large-Scale Image Recognition ICLR(2015) [code] GoogLeNet Going deeper with convolutions CVPR(2015) PyTorch ResNet Deep Residual Learning for Image Recognition CVPR(2016) PyTorch Inception-v4 Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning AAAI(2017) [code] DenseNet Densely Connected Convolutional Networks CVPR(2017) PyTorch DLA Deep Layer Aggregation CVPR(2018) PyTorch ShuffleNet ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices CVPR(2018) [code] MobileNetV3 Searching for MobileNetV3 ICCV(2019) [code] More information can be found in Awesome - Image Classification. Object Detection Title Paper Conf Code R-CNN Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation CVPR(2014) [code] SPP Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition TPAMI(2015) [code] Fast R-CNN Fast R-CNN ICCV(2015) [code] Faster R-CNN Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks NIPS(2015) PyTorch SSD SSD: Single Shot MultiBox Detector ECCV(2016) Caffe YOLO You Only Look Once: Unified, Real-Time Object Detection CVPR(2016) [code] YOLOv2 YOLO9000: Better, Faster, Stronger CVPR(2017) [code] FPN Feature Pyramid Networks for Object Detection CVPR(2017) [code] RetinaNet Focal Loss for Dense Object Detection ICCV(2017) [code] YOLOv3 YOLOv3: An Incremental Improvement arXiv(2018) Offical CornerNet CornerNet: Detecting Objects as Paired Keypoints ECCV(2018) PyTorch CenterNet Objects as Points arXiv(2019) PyTorch YOLOv4 YOLOv4: Optimal Speed and Accuracy of Object Detection arXiv(2020) Offical YOLOF You Only Look One-level Feature CVPR(2021) PyTorch More information can be found in awesome-object-detection. Object Segmentation Title Paper Conf Code FCN Fully convolutional networks for semantic segmentation CVPR(2015) PyTorch U-Net U-Net: Convolutional Networks for Biomedical Image Segmentation MICCAI(2015) PyTorch Seg-Net SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling arXiv(2015) PyTorch DeepLab V1 Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs arXiv(2014) / ICLR(2015) PyTorch PSPNet Pyramid Scene Parsing Network CVPR(2017) PyTorch DeepLab V2 DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs arXiv(2016) / TPAMI(2017) PyTorch Mask R-CNN Mask R-CNN ICCV / TPAMI(2017) PyTorch DeepLab V3 Rethinking Atrous Convolution for Semantic Image Segmentation arXiv(2017) PyTorch PointNet PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation CVPR(2017) PyTorch PointNet++ PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space NIPS(2017) PyTorch DeepLab V3+ Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation ECCV(2018) PyTorch DGCNet Dual Graph Convolutional Network for Semantic Segmentation BMVC(2019) PyTorch SETR Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers CVPR(2021) PyTorch Segmenter Segmenter: Transformer for Semantic Segmentation arXiv(2021) PyTorch SegFormer SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers arXiv(2021) PyTorch FTN Fully Transformer Networks for Semantic ImageSegmentation arXiv(2021) [code] Object Tracking Title Paper Conf Code SORT Simple Online and Realtime Tracking ICIP(2016) PyTorch DepthTrack Real-time depth-based tracking using a binocular camera WCICA(2016) [code] DeepSORT Simple Online and Realtime Tracking with a Deep Association Metric ICIP(2017) PyTorch BinocularTrack Research on Target Tracking Algorithm Based on Parallel Binocular Camera ITAIC(2019) [code] SiamFC Fully-Convolutional Siamese Networks for Object Tracking ECCV(2016) PyTorch SiamRPN High Performance Visual Tracking with Siamese Region Proposal Network CVPR(2018) PyTorch SiamRPN++ SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks CVPR(2019) PyTorch SiamMask Fast Online Object Tracking and Segmentation: A Unifying Approach CVPR(2019) PyTorch Tracktor Tracking without bells and whistles ICCV(2019) PyTorch GlobalTrack GlobalTrack: A Simple and Strong Baseline for Long-term Tracking AAAI(2020) PyTorch SiamCAR SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking CVPR(2020) PyTorch SiamBAN Siamese Box Adaptive Network for Visual Tracking CVPR(2020) PyTorch SiamAttn Deformable Siamese Attention Networks for Visual Object Tracking CVPR(2020) PyTorch PAMCC-AOT Pose-Assisted Multi-Camera Collaboration for Active Object Tracking AAAI(2020) [code] FFT Multiple Object Tracking by Flowing and Fusing arXiv(2020) [code] JRMOT JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset arXiv(2020) [code] Tracklet Multi-object Tracking via End-to-end Tracklet Searching and Ranking arXiv(2020) [code] DMCT Real-time 3D Deep Multi-Camera Tracking arXiv(2020) [code] FairMOT A Simple Baseline for Multi-Object Tracking arXiv(2020) PyTorch TSDM TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator arXiv(2020) PyTorch CenterPoint Center-based 3D Object Detection and Tracking CVPR(2021) PyTorch SiamGAT Graph Attention Tracking CVPR(2021) PyTorch RE-SiamNets Rotation Equivariant Siamese Networks for Tracking CVPR(2021) PyTorch Few-Shot Segmentation Title Paper Conf Code OSLSM One-Shot Learning for Semantic Segmentation BMVC(2017) Caffe co-FCN Conditional Networks for Few-Shot Semantic Segmentation ICLR(2018) [code] AMP AMP: Adaptive Masked Proxies for Few-Shot Segmentation ICCV(2019) Pytorch SG-One SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation arXiv(2018) / TCYB(2020) PyTorch CENet Learning Combinatorial Embedding Networks for Deep Graph Matching ICCV(2019) Pytorch PANet PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment ICCV(2019) PyTorch CANet CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning CVPR(2019) PyTorch PGNet Pyramid Graph Networks with Connection Attentions for Region-Based One-Shot Semantic Segmentation ICCV(2019) [code] CRNet CRNet: Cross-Reference Networks for Few-Shot Segmentation CVPR(2020) [code] FGN FGN: Fully Guided Network for Few-Shot Instance Segmentation CVPR(2020) [code] OTB On the Texture Bias for Few-Shot CNN Segmentation arXiv(2020) TensorFlow LTM A New Local Transformation Module for Few-Shot Segmentation MMMM(2020) [code] SimPropNet SimPropNet: Improved Similarity Propagation for Few-shot Image Segmentation IJCAI(2020) [code] PPNet Part-aware Prototype Network for Few-shot Semantic Segmentation ECCV(2020) PyTorch PFENet PFENet: Prior Guided Feature Enrichment Network for Few-shot Segmentation TPAMI(2020) PyTorch PMMs Prototype Mixture Models for Few-shot Semantic Segmentation ECCV(2020) PyTorch GFS-Seg Generalized Few-Shot Semantic Segmentation arXiv(2020) [code] SCL Self-Guided and Cross-Guided Learning for Few-Shot Segmentation CVPR(2021) PyTorch ASGNet Adaptive Prototype Learning and Allocation for Few-Shot Segmentation CVPR(2021) PyTorch HSNet Hypercorrelation Squeeze for Few-Shot Segmenation ICCV(2021) PyTorch BAM Learning What Not to Segment: A New Perspective on Few-Shot Segmentation CVPR(2022) PyTorch More information can be found in Few-Shot-Semantic-Segmentation-Papers. 3D Face Reconstruction and Facial Animation Title Paper Conf Code 3DMM A Morphable Model For The Synthesis Of 3D Faces SIGGRAPH(1999) [code] CameraCalibration A Flexible New Technique for CameraCalibration TPAMI(2000) [code] Bilinear Bilinear Models for 3-D Face andFacial Expression Recognition TIFS(2008) [code] DDE Displaced Dynamic Expression Regression forReal-time Facial Tracking and Animation TOG(2014) [code] FaceWarehouse FaceWarehouse: a 3D Facial Expression Databasefor Visual Computing TVCG(2014) [code] Face2Face Face2Face: Real-Time Face Capture and Reenactment of RGB Videos CVPR(2016) [code] DynamicAvatars Real-time Facial Animation with Image-based Dynamic Avatars TOG(2016) [code] FLAME Learning a model of facial shape and expression from 4D scans TOG(2017) Tensorflow PyTorch Nonlinear Nonlinear 3D Face Morphable Model CVPR(2018) Tensorflow DynamicRigidityPrior Stabilized real-time face tracking via a learned dynamic rigidity prior TOG(2018) [code] Deep3D Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set CVPR(2019) Tensorflow PyTorch SimpleAnimation Face It!: A Pipeline for Real-Time Performance-Driven Facial Animation ICIP(2019) [code] RingNet Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision CVPR(2019) Tensorflow FOCUS To fit or not to fit: Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision arXiv(2021) PyTorch MICA Towards Metrical Reconstruction of Human Faces ECCV(2022) PyTorch HRN A Hierarchical Representation Network for Accurate and Detailed Face Reconstruction from In-The-Wild Images CVPR(2023) PyTorch Attention or Transformer Title Paper Conf Code Transformer Attention Is All You Need arXiv(2017) TensorFlow Non-local Non-local Neural Networks CVPR(2018) PyTorch Image Transformer Image Transformer arXiv(2018) [code] ViT An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale arXiv(2020) PyTorch Swin Transformer Swin Transformer: Hierarchical Vision Transformer using Shifted Windows arXiv(2021) PyTorch ResT ResT: An Efficient Transformer for Visual Recognition arXiv(2021) PyTorch DS-Net Dual-stream Network for Visual Recognition arXiv(2021) [code] TransCNN Transformer in Convolutional Neural Networks arXiv(2021) PyTorch Shuffle Transformer Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer arXiv(2021) PyTorch Salient Object Detection Title Paper Conf Code UC-Net UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders CVPR(2020) PyTorch JL-DCF JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection CVPR(2020) PyTorch SA-Gate Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation ECCV(2020) PyTorch BiANet Bilateral Attention Network for RGB-D Salient Object Detection TIP(2021) [Code] DSA^2F Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion CVPR(2021) [Code] Unsupervised Learning Title Paper Conf Code SimSiam Exploring Simple Siamese Representation Learning CVPR(2021) PyTorch 3D Object Detection Title Paper Conf Code PV-RCNN PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection CVPR(2020) PyTorch Few-Shot Learning Title Paper Conf Code RN Learning to Compare: Relation Network for Few-Shot Learning CVPR(2018) PyTorch Generative Adversarial Network Title Paper Conf Code GAN Generative Adversarial Networks arXiv(2014) [code] BeautyGAN BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network ACM MM(2018) TensorFlow Optimization Title Paper Conf Code ReLU Deep Sparse Rectifier Neural Networks JMLR(2011) [code] Momentum On the importance of initialization and momentum in deep learning ICML(2013) [code] Dropout Dropout: a simple way to prevent neural networks from overfitting JMLR(2014) [code] Adam Adam: A Method for Stochastic Optimization ICLR(2015) [code] BN Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ICML(2015) [code] GDoptimization An overview of gradient descent optimization algorithms arXiv(2016) [code] Survey Title Paper Conf 3D-Detection-Survey-2019 A Survey on 3D Object Detection Methods for Autonomous Driving Applications ITS(2019) FSL-Survey-2019 Generalizing from a Few Examples: A Survey on Few-Shot Learning CSUR(2019) MOT-Survey-2020 Deep Learning in Video Multi-Object Tracking: A Survey Neurocomputing(2020) Transformer-Survey-2021 A Survey of Transformers arXiv(2021)","link":"/2020/04/01/Overview/"},{"title":"Toolkit for DL","text":"There are the overall of toolkit for Deep Learning.https://github.com/Gojay001/toolkit-DeepLearning Contents. |- DataProcess/ |- data_aug.py |- data_config.py |- data_loader.py |- divide_data.py |- img2video.py |- img_resize.py |- video_extract.py |- utils/ |- bbox_iou.py |- nms |- nms_cpu.py |- ... |- test.py |- train.py DataProcess DataProcess/data_aug.py: augment data with transforms to aug file.DataProcess/data_config.py: config (hyper-)parameters.DataProcess/data_loader.py: load data to tensor of DataSet type.DataProcess/divide_data.py: divide data to train and valid files.DataProcess/img2video.py: transform images set to video using opencv.DataProcess/img_resize.py: resize images to specific size using opencv.DataProcess/video_extract.py: extract each frame of video to images file. utils utils/bbox_iou.py: calculate iou between two bounding-box.utils/nms/nms_cpu.py: remove useless bounding-box by nms(Non-maximum suppression). (Updating…)","link":"/2020/06/18/Toolkit-for-DL/"},{"title":"剑指Offer-49-丑数","text":"题目 题目描述我们把只包含质因子 2、3 和 5 的数称作丑数（Ugly Number）。求按从小到大的顺序的第 n 个丑数。 示例1 输入: n = 10输出: 12解释: 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 是前 10 个丑数。 题解 动态规划 计算当前各质数的丑数，选取最小丑数作为当前值，并将对应质数的位数加1。 dp数组存储第i个位置的丑数; a,b,c分别对应2,3,5前进次数; n2,n3,n5对应2,3,5对应的当前丑数; 时间复杂度: O(n)，遍历dp数组;空间复杂度: O(n)，dp数组占用O(n)的额外空间; class Solution: def nthUglyNumber(self, n: int) -&gt; int: dp, a, b, c = [1] * n, 0, 0, 0 for i in range(1, n): n2, n3, n5 = dp[a] * 2, dp[b] * 3, dp[c] * 5 dp[i] = min(n2, n3, n5) if dp[i] == n2: a += 1 if dp[i] == n3: b += 1 if dp[i] == n5: c += 1 return dp[-1]","link":"/2020/12/15/剑指Offer-49-丑数/"},{"title":"剑指Offer-48-最长不含重复字符的子字符串","text":"题目 题目描述请从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。 示例1 输入: “abcabcbb”输出: 3解释: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。 示例2 输入: “bbbbb”输出: 1解释: 因为无重复字符的最长子串是 “b”，所以其长度为 1。 示例3 输入: “pwwkew”输出: 3解释: 因为无重复字符的最长子串是 “wke”，所以其长度为 3。请注意，你的答案必须是 子串 的长度，”pwke” 是一个子序列，不是子串。 题解 (引用腐烂的橘子) 滑动窗口（双指针） 遍历字符串，用 双指针标记当前滑动窗口，滑动窗口包含当前元素时，窗口右移。 遍历字符串，tail指针右移: 窗口不包含当前元素: 更新窗口最大长度; 窗口包含当前元素: head指针右移，直到窗口不包含当前元素; 时间复杂度: O($n^2$)，遍历字符串占用O(n)，窗口右移O(n);空间复杂度: O(1)，变量占用常数大小的额外空间; class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: head, tail, res = 0, 0, 1 if len(s) &lt; 2: return len(s) while tail &lt; len(s) - 1: tail += 1 if s[tail] not in s[head:tail]: res = max(tail - head + 1, res) else: while s[tail] in s[head:tail]: head += 1 return res 优化滑动窗口（哈希表） 使用 哈希表记录每个字符的下一个索引，然后尽量向右移动尾指针来拓展窗口，并更新窗口的最大长度。如果尾指针指向的元素重复，则将 头指针直接移动到窗口中重复元素的右侧。 遍历字符串，tail指针右移: 若当前元素存在哈希表中: head指针跳跃到重复字符的下一位; 更新 哈希表和窗口长度; 时间复杂度: O(n)，遍历字符串;空间复杂度: O(n)，哈希表占用O(n)额外空间; class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: head, res = 0, 0 hashmap = {} for tail in range(len(s)): if s[tail] in hashmap: head = max(hashmap[s[tail]], head) hashmap[s[tail]] = tail + 1 res = max(tail - head + 1, res) return res","link":"/2020/12/15/剑指Offer-48-最长不含重复字符的子字符串/"},{"title":"剑指Offer-47-礼物的最大价值","text":"题目 题目描述在一个 m*n 的棋盘的每一格都放有一个礼物，每个礼物都有一定的价值（价值大于 0）。你可以从棋盘的左上角开始拿格子里的礼物，并每次向右或者向下移动一格、直到到达棋盘的右下角。给定一个棋盘及其上面的礼物的价值，请计算你最多能拿到多少价值的礼物？ 示例1 输入:[ [1,3,1], [1,5,1], [4,2,1]]输出: 12解释: 路径 1→3→5→2→1 可以拿到最多价值的礼物 题解 动态规划 当前位置的最大值为当前值+max(左方，上方)。 边界: dp[0][i] = dp[0][i-1] + grid[0][i]; dp[i][0] = dp[i-1][0] + grid[i][0]; 转移方程: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) + grid[i][j]; 时间复杂度: O(mn)，遍历grid矩阵;空间复杂度: O(mn)，dp数组占用O(mn)额外空间; class Solution: def maxValue(self, grid: List[List[int]]) -&gt; int: m, n = len(grid), len(grid[0]) dp = [[0] * n for i in grid] dp[0][0] = grid[0][0] for i in range(1, m): dp[i][0] = dp[i-1][0] + grid[i][0] for i in range(1, n): dp[0][i] = dp[0][i-1] + grid[0][i] for i in range(1, m): for j in range(1, n): dp[i][j] = max(dp[i-1][j], dp[i][j-1]) + grid[i][j] return dp[-1][-1] 原地dp: 时间复杂度: O(mn)，遍历grid矩阵;空间复杂度: O(1)，更新原数组值，不占用额外空间; class Solution: def maxValue(self, grid: List[List[int]]) -&gt; int: m, n = len(grid), len(grid[0]) for i in range(1, m): grid[i][0] += grid[i-1][0] for i in range(1, n): grid[0][i] += grid[0][i-1] for i in range(1, m): for j in range(1, n): grid[i][j] += max(grid[i][j-1], grid[i-1][j]) return grid[-1][-1]","link":"/2020/12/15/剑指Offer-47-礼物的最大价值/"},{"title":"剑指Offer-46-把数字翻译成字符串","text":"题目 题目描述给定一个数字，我们按照如下规则把它翻译为字符串：0 翻译成 “a” ，1 翻译成 “b”，……，11 翻译成 “l”，……，25 翻译成 “z”。一个数字可能有多个翻译。请编程实现一个函数，用来计算一个数字有多少种不同的翻译方法。 示例1 输入: 12258输出: 5解释: 12258有5种不同的翻译，分别是”bccfi”, “bwfi”, “bczi”, “mcfi”和”mzi” 题解 动态规划 遍历字符串，判断当前字符与上一字符是否能够组合成字母。 时间复杂度: O(n)，遍历字符串;空间复杂度: O(n)，字符串数组占用O(n)额外空间，dp数组占用O(n)额外空间; class Solution: def translateNum(self, num: int) -&gt; int: strs = str(num) dp = [] dp.append(1) dp.append(2 if strs[:2] &gt;= '10' and strs[:2] &lt;= '25' else 1) for i in range(2, len(strs)): if strs[i-1:i+1] &gt;= '10' and strs[i-1:i+1] &lt;= '25': cur = dp[i-1] + dp[i-2] else: cur = dp[i-1] dp.append(cur) return dp[-1] 原地dp:class Solution: def translateNum(self, num: int) -&gt; int: strs = str(num) a = b = 1 for i in range(1, len(strs)): a, b = (a + b) if '10' &lt;= strs[i-1:i+1] &lt;= '25' else a, a return a 动态规划（空间优化） 字符串数组占用O(n)额外空间，因为从左到右计算和从右到左计算为对称操作，即可通过 求余操作得到当前数字和前一数字，而不需要先转换为字符串。 时间复杂度: O(n)，遍历字符串;空间复杂度: O(1)，变量占用常数大小的额外空间; class Solution: def translateNum(self, num: int) -&gt; int: a = b = 1 y = num % 10 while num != 0: num //= 10 x = num % 10 a, b = (a + b) if 10 &lt;= 10*x+y &lt;= 25 else a, a y = x return a","link":"/2020/12/15/剑指Offer-46-把数字翻译成字符串/"},{"title":"PPNet","text":"PPNet(Part-aware Prototype Network for Few-shot Semantic Segmentation)[1] decompose the holistic class representation into a set of part-aware prototypes, and leverage unlabeled data to better modeling of intra-class variations. Besides, graph neural network model is used to generate and enhance the proposed part-aware prototypes. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Part-aware Prototype Network for Few-shot Semantic SegmentationCode: PyTorchNote: Mendeley Paper Abstract Problem Description Problem Solution Conceptual Understanding Core Conception Part Generation Part Refinement Mask Generation Experiments Code The complete code can be found in PPNet-PyTorch[2]. [Updating] Note decomposed objects to a set of part based on prototype for few-shot segmentation. References [1] Liu Y, Zhang X, Zhang S, et al. Part-aware prototype network for few-shot semantic segmentation[C]//European Conference on Computer Vision. Springer, Cham, 2020: 142-158.[2] PPNet-PyTorch. https://github.com/Xiangyi1996/PPNet-PyTorch.","link":"/2020/12/02/PPNet/"},{"title":"PANet","text":"PANet(PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment)[1] learns class-specific prototype representations for images and matches each pixel to the learned prototypes. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment(ICCV 2019 paper)Code: PyTorchNote: Mendeley Paper Abstract Problem Solution Conceptual Understanding Prototype learning Non-parametric metric learning Core Conception Experiments Code [Updating] Note provided a baseline in prototype learning for few-shot segmentation. References [1] Wang K, Liew J H, Zou Y, et al. Panet: Few-shot image semantic segmentation with prototype alignment[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 9197-9206.[2] PANet. https://github.com/kaixin96/PANet.","link":"/2020/12/02/PANet/"},{"title":"剑指Offer-45-把数组排成最小的数","text":"题目 题目描述输入一个非负整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。 示例1 输入: [10,2]输出: “102” 示例2 输入: [3,30,34,5,9]输出: “3033459” 题解 (引用Krahets) 位数规律 将数字数组转为 字符串 数组，然后将其 排序。 使用Python内置函数functools.cmp_to_key，自定义字符串排序规则; 时间复杂度: O(nlogn)，排序占用O(nlogn);空间复杂度: O(n)，字符串数组占O用O(n)额外空间; class Solution: def minNumber(self, nums: List[int]) -&gt; str: def sort_rule(x, y): a, b = x + y, y + x if a &gt; b: return 1 elif a &lt; b: return -1 else: return 0 strs = [str(num) for num in nums] strs.sort(key = functools.cmp_to_key(sort_rule)) return ''.join(strs)","link":"/2020/11/23/剑指Offer-45-把数组排成最小的数/"},{"title":"剑指Offer-44-数字序列中某一位的数字","text":"题目 题目描述数字以0123456789101112131415…的格式序列化到一个字符序列中。在这个序列中，第5位（从下标0开始计数）是5，第13位是1，第19位是4，等等。 请写一个函数，求任意第n位对应的数字。 示例1 输入: n = 3输出: 3 示例2 输入: n = 11输出: 0 题解 (引用Krahets) 位数规律 计算当前所在的位数范围，其次确定对应数字，最后找出对应该数字的第几位。 确定所求数位的所在数字的位数 digit; 确定所求数位所在的数字 num; 确定所求数位在 num 的哪一数位; 时间复杂度: O(logn)，循环最多logn次，num转为字符串占用O(n);空间复杂度: O(logn)，num转为字符串占O(logn)额外空间; class Solution: def findNthDigit(self, n: int) -&gt; int: digit, start, count = 1, 1, 9 while n &gt; count: n -= count digit += 1 start *= 10 count = 9 * start * digit num = start + (n - 1) // digit return int(str(num)[(n - 1) % digit])","link":"/2020/11/23/剑指Offer-44-数字序列中某一位的数字/"},{"title":"剑指Offer-43-1～n 整数中 1 出现的次数","text":"题目 题目描述输入一个整数 n ，求1～n这n个整数的十进制表示中1出现的次数。 例如，输入12，1～12这些整数中包含1 的数字有1、10、11和12，1一共出现了5次。 示例1 输入: n = 12输出: 5 示例2 输入: n = 13输出: 6 题解 (引用Krahets) 位数规律 依次遍历数字n的每一位数，根据规律统计出现1的个数。 当前位 = 0: 只由高位决定，即 high * digit; 当前位 = 1: 由高位和低位共同决定，即 high * digit + low + 1; 否则: 只由高位决定，即 (high + 1) * digit; 时间复杂度: O(logn)，遍历数字n的位数，需要循环logn次;空间复杂度: O(1)，变量占用常数级额外空间; class Solution: def countDigitOne(self, n: int) -&gt; int: low, cur, high = 0, n % 10, n // 10 digit, res = 1, 0 while high != 0 or cur != 0: if cur == 0: res += high * digit elif cur == 1: res += high * digit + low + 1 else: res += (high + 1) * digit low += cur * digit cur = high % 10 high //= 10 digit *= 10 return res","link":"/2020/11/18/剑指Offer-43-1～n-整数中-1-出现的次数/"},{"title":"剑指Offer-42-连续子数组的最大和","text":"题目 题目描述输入一个整型数组，数组中的一个或连续多个整数组成一个子数组。求所有子数组的和的最大值。 要求时间复杂度为O(n)。 示例1 输入: nums = [-2,1,-3,4,-1,2,1,-5,4]输出: 6解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 题解 动态规划 依次遍历数组，根据前一索引dp，保存当前索引的子数组最大和。 时间复杂度: O(n)，遍历整个数组;空间复杂度: O(n)，保存dp数组; class Solution: def maxSubArray(self, nums: List[int]) -&gt; int: dp = [] dp.append(nums[0]) for i in range(1, len(nums)): max_num = nums[i] + max(dp[i-1], 0) dp.append(max_num) return max(dp) 原地优化（动态规划） 将原地nums数组元素更改为当前索引的子数组最大和。 时间复杂度: O(n)，遍历整个数组;空间复杂度: O(1)，原地更改数组，不占用额外空间; class Solution: def maxSubArray(self, nums: List[int]) -&gt; int: for i in range(1, len(nums)): nums[i] += max(nums[i-1], 0) return max(nums)","link":"/2020/11/18/剑指Offer-42-连续子数组的最大和/"},{"title":"剑指Offer-41-数据流中的中位数","text":"题目 题目描述如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。 例如， [2,3,4] 的中位数是 3 [2,3] 的中位数是 (2 + 3) / 2 = 2.5 设计一个支持以下两种操作的数据结构： void addNum(int num) - 从数据流中添加一个整数到数据结构中。 double findMedian() - 返回目前所有元素的中位数。 示例1 输入:[“MedianFinder”,”addNum”,”addNum”,”findMedian”,”addNum”,”findMedian”][[],[1],[2],[],[3],[]]输出: [null,null,null,1.50000,null,2.00000] 示例2 输入:[“MedianFinder”,”addNum”,”findMedian”,”addNum”,”findMedian”][[],[2],[],[3],[]]输出: [null,null,2.00000,null,2.50000] 题解 堆 维护同样大小的 小顶堆和大顶堆，分别存放较大和较小的一半元素，根据两个堆顶元素得到数据流的中位数。 新元素进堆（允许大值堆/小顶堆比小值堆/大顶堆元素个数多1）: 当 大值堆不等于小值堆 元素个数时: 即大值堆个数多一个，上一轮中位数为大值堆堆顶元素，则将新元素插入大值堆，再将大值堆堆顶元素弹出并插入到小值堆，此时大值堆与小值堆元素个数相等，由两个堆顶元素共同确定中位数; 当 大值堆等于小值堆 元素个数时: 将新元素插入小值堆，再将小值堆堆顶元素弹出插入到大值堆，此时大值堆元素个数多1，堆顶元素为中位数; 时间复杂度: O(logn)，堆的插入和弹出均为O(logn);空间复杂度: O(n)，大值堆和小值堆共占用额外空间O(n); class MedianFinder: def __init__(self): \"\"\" initialize your data structure here. \"\"\" self.max_heap, self.min_heap = [], [] def addNum(self, num: int) -&gt; None: if len(self.max_heap) != len(self.min_heap): heappush(self.max_heap, num) heappush(self.min_heap, -heappop(self.max_heap)) else: heappush(self.min_heap, -num) heappush(self.max_heap, -heappop(self.min_heap)) def findMedian(self) -&gt; float: return self.max_heap[0] if len(self.max_heap) != len(self.min_heap) else (self.max_heap[0]-self.min_heap[0]) / 2.0# Your MedianFinder object will be instantiated and called as such:# obj = MedianFinder()# obj.addNum(num)# param_2 = obj.findMedian() Python 中 heapq 模块是小顶堆。实现 大顶堆 方法： 小顶堆的插入和弹出操作均将元素 取反（负数） 即可。","link":"/2020/11/14/剑指Offer-41-数据流中的中位数/"},{"title":"剑指Offer-40-最小的k个数","text":"题目 题目描述输入整数数组 arr ，找出其中最小的 k 个数。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。 示例1 输入: arr = [3,2,1], k = 2输出: [1,2] 或者 [2,1] 示例2 输入: arr = [0,1,2,1], k = 1输出: [0] 题解 Python函数 topk: 将数组排序，选取前k个值。 时间复杂度: O(nlogn)，排序时间复杂度为O(nlogn);空间复杂度: O(1); class Solution: def getLeastNumbers(self, arr: List[int], k: int) -&gt; List[int]: return heapq.nsmallest(k, arr) class Solution: def getLeastNumbers(self, arr: List[int], k: int) -&gt; List[int]: arr.sort() return arr[:k] 堆排序 维护一个大小为k的大顶堆，将第k+1个元素后都与堆顶比较，当前元素小于堆顶时，则弹出堆顶并插入当前元素。 初始化大顶堆: 将数组前k个元素构建大顶堆，即从非叶子节点向上构建，大值元素上浮; 更新大顶堆: 从数组第k+1个元素依次与堆顶元素比较，若小于堆顶，即当前元素属于topk，弹出堆顶元素并插入当前元素，重新构建大顶堆。 时间复杂度: O(nlogn)，将一个元素插入大顶堆占用O(logn)，遍历整个数组占用O(n);空间复杂度: O(k)，维护大小为k的大顶堆占用k额外空间; class Solution: def heapify(self, arr, i): # 大顶堆化 left, right = 2 * i + 1, 2 * i + 2 largest = i if left &lt; len(arr) and arr[left] &gt; arr[largest]: largest = left if right &lt; len(arr) and arr[right] &gt; arr[largest]: largest = right if largest != i: arr[largest], arr[i] = arr[i], arr[largest] self.heapify(arr, largest) # 交换后再次与左右子节点比较 def build_heap(self, nums): # 从非叶子节点初始化堆 for i in range(len(nums) // 2 - 1, -1, -1): self.heapify(nums, i) def getLeastNumbers(self, arr: List[int], k: int) -&gt; List[int]: if not arr or k &lt;= 0: return [] if len(arr) &lt;= k: return arr heap = arr[:k] self.build_heap(heap) for i in range(k, len(arr)): if arr[i] &lt; heap[0]: # 当前元素比k堆顶小时，弹出堆顶并插入当前元素 heap[0] = arr[i] self.heapify(heap, 0) return heap 快排 选取基准元素将数组分为左右两侧，根据基准元素下标和k比较递归划分，直到基准元素下标为k，则左侧k个元素为topk。 基准划分: 选取数组第一个元素为基准，i 标记比基准小的元素下标，j 标记比基准大的元素下标，最后将基准交换并返回基准元素下标; 快速排序: 1)当基准元素下标 pivot+1 == k 时，则基准左侧k-1元素都比基准元素小，即为topk; 2)若不相等，根据基准元素下标对左（右）侧进一步划分。 时间复杂度: O(nlogn)，每轮遍历数组与基准比较占用O(n)，共需要logn轮比较;空间复杂度: O(nlogn)，递归占用nlogn额外空间; class Solution: def partition(self, arr, left, right): pivot = left i = j = pivot + 1 while j &lt;= right: if arr[j] &lt; arr[pivot]: arr[i], arr[j] = arr[j], arr[i] i += 1 j += 1 arr[pivot], arr[i-1] = arr[i-1], arr[pivot] return i-1 def quick_sort(self, arr, left, right, k): pivot = self.partition(arr, left, right) if pivot + 1 == k: return elif pivot + 1 &gt; k: # topk元素全在基准左侧 self.quick_sort(arr, left, pivot - 1, k) else: # topk元素全在基准右侧 self.quick_sort(arr, pivot + 1, right, k) def getLeastNumbers(self, arr: List[int], k: int) -&gt; List[int]: if not arr or k &lt;= 0: return [] if len(arr) &lt;= k: return arr self.quick_sort(arr, 0, len(arr) - 1, k) return arr[:k]","link":"/2020/11/09/剑指Offer-40-最小的k个数/"},{"title":"剑指Offer-39-数组中出现次数超过一半的数字","text":"题目 题目描述数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。 你可以假设数组是非空的，并且给定的数组总是存在多数元素。 示例 输入: [1, 2, 3, 2, 2, 2, 5, 4, 2]输出: 2 题解 排序找中间数 将数组排序后，目标数字个数超过数组长度一半，则中间数为目标数。 时间复杂度: O(nlogn)，排序时间复杂度为O(nlogn);空间复杂度: O(1); class Solution: def majorityElement(self, nums: List[int]) -&gt; int: nums.sort() return nums[len(nums) // 2] HashMap 使用 HashMap 记录每个数字出现次数，当出现次数大于数组长度一半时，则为目标数。 时间复杂度: O(n)，遍历整个数组;空间复杂度: O(n)，辅助HashMap最多占用n/2额外空间; class Solution: def majorityElement(self, nums: List[int]) -&gt; int: count = {} for num in nums: if num not in count: count[num] = 0 count[num] += 1 if count[num] &gt; len(nums) // 2: return num 摩尔投票法(引用Krahets) 默认首位数为目标数，票数+1，当下位数字与当前目标数不同时，则票数-1；当票数为0时，重新使下一位数字为新的目标数，最终票数仍为正则当前数为目标数。 目标数个数大于数组长度一半，则抵消其他所有数后，票数仍未正；反之，非目标数无法抵消目标数所有个数。 时间复杂度: O(n)，遍历整个数组;空间复杂度: O(1)，变量votes占用常数额外空间; class Solution: def majorityElement(self, nums: List[int]) -&gt; int: votes = 0 for num in nums: if votes == 0: res = num votes += 1 if num == res else -1 return res","link":"/2020/11/05/剑指Offer-39-数组中出现次数超过一半的数字/"},{"title":"剑指Offer-38-字符串的排列","text":"题目 题目描述输入一个字符串，打印出该字符串中字符的所有排列。 你可以以任意顺序返回这个字符串数组，但里面不能有重复元素。 示例 输入: s = “abc”输出: [“abc”,”acb”,”bac”,”bca”,”cab”,”cba”] 题解 (引用Krahets) 回溯DFS 深度优先搜索，先固定高位，后固定低位。当字符重复时剪枝。 每个当前位置新建一个set，用于存储字符序列并去除重复字符。 时间复杂度: O(n!)，共有 n(n-1)…21 种方案;空间复杂度: O($n^2$)，DFS递归深度为n，递归辅助set累计存储 $n+(n-1)+..+2+1=(n+1)n/2$，即占用O($n^2$)额外空间; class Solution: def permutation(self, s: str) -&gt; List[str]: path, res = list(s), [] def dfs(loc): if loc == len(path) - 1: res.append(''.join(path)) # 添加排列方案 return exist = set() # 存储从第loc位开始，已经出现过的字符 for i in range(loc, len(path)): if path[i] in exist: # 重复，剪枝 continue exist.add(path[i]) path[i], path[loc] = path[loc], path[i] # 交换，将path[i]固定在第loc位 dfs(loc+1) # 固定下一位字符 path[i], path[loc] = path[loc], path[i] # 恢复交换 dfs(0) return res","link":"/2020/11/02/剑指Offer-38-字符串的排列/"},{"title":"剑指Offer-37-序列化二叉树","text":"题目 题目描述 请实现两个函数，分别用来序列化和反序列化二叉树。 示例你可以将以下二叉树： 1 / \\ 2 3 / \\ 4 5序列化为 &quot;[1,2,3,null,null,4,5]&quot; 题解 (引用Krahets) 层序遍历BFS 层序遍历二叉树，使用队列实现BFS。 题目要求的 序列化 和 反序列化 是可逆操作，因此，序列化的字符串应携带完整的二叉树信息。为完整表示二叉树，考虑将叶节点下的 null 也记录。在此基础上，对于列表中任意某节点 node ，其左子节点 node.left 和右子节点 node.right 在序列中的位置都是 唯一确定 的。 序列化 Serialize: 使用队列，对二叉树做层序遍历，将节点值转为字符串存储，再将当前节点的左右子树加入队列；越过叶节点的 null 也将存储; 反序列化 Deserialize: 利用队列按层构建二叉树，借助一个指针 i 指向节点 node 的左、右子节点，每构建一个 node 的左、右子节点，指针 i 就向右移动 1 位。 时间复杂度: O(n)，遍历完全二叉树，长度最大为2n+1;空间复杂度: O(n)，队列最多同时存储节点数为(2n+1)/2; # Definition for a binary tree node.# class TreeNode(object):# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Codec: def serialize(self, root): \"\"\"Encodes a tree to a single string. :type root: TreeNode :rtype: str \"\"\" if not root: return '[]' queue = collections.deque() queue.append(root) res = [] while queue: node = queue.popleft() if node: res.append(str(node.val)) queue.append(node.left) queue.append(node.right) else: res.append('null') return '[' + ','.join(res) + ']' def deserialize(self, data): \"\"\"Decodes your encoded data to tree. :type data: str :rtype: TreeNode \"\"\" if data == '[]': return vals, i = data[1:-1].split(','), 1 root = TreeNode(int(vals[0])) queue = collections.deque() queue.append(root) while queue: node = queue.popleft() if vals[i] != 'null': node.left = TreeNode(int(vals[i])) queue.append(node.left) i += 1 if vals[i] != 'null': node.right = TreeNode(int(vals[i])) queue.append(node.right) i += 1 return root# Your Codec object will be instantiated and called as such:# codec = Codec()# codec.deserialize(codec.serialize(root))","link":"/2020/10/27/剑指Offer-37-序列化二叉树/"},{"title":"剑指Offer-36-二叉搜索树与双向链表","text":"题目 题目描述 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的循环双向链表。要求不能创建任何新的节点，只能调整树中节点指针的指向。 示例以下面的二叉搜索树为例：我们希望将这个二叉搜索树转化为双向循环链表。链表中的每个节点都有一个前驱和后继指针。对于双向循环链表，第一个节点的前驱是最后一个节点，最后一个节点的后继是第一个节点。 下图展示了上面的二叉搜索树转化成的链表。“head” 表示指向链表中有最小元素的节点。特别地，我们希望可以就地完成转换操作。当转化完成以后，树中节点的左指针需要指向前驱，树中节点的右指针需要指向后继。还需要返回链表中的第一个节点的指针。 题解 中序遍历 中序遍历实现排序链表，每个节点进行前向指针和后向指针的连接，最后将头节点和尾节点连接。 dfs(cur): 递归实现中序遍历; 终止条件: 当前节点为空时，return; 递归左子树: dfs(cur.left); 构建链表: 1) 当pre为空时，即当前节点标记为头节点; 2)当pre不为空，更改当前节点与上一节点的双向连接关系; 3) 将当前节点更新为pre; 递归右子树: dfs(cur.right); 时间复杂度: O(n)，中序遍历需遍历所有节点;空间复杂度: O(n)，递归占用额外空间; \"\"\"# Definition for a Node.class Node: def __init__(self, val, left=None, right=None): self.val = val self.left = left self.right = right\"\"\"class Solution: def treeToDoublyList(self, root: 'Node') -&gt; 'Node': def dfs(cur): if not cur: return dfs(cur.left) if self.pre: self.pre.right, cur.left = cur, self.pre else: self.head = cur self.pre = cur dfs(cur.right) if not root: return self.pre = None dfs(root) self.head.left, self.pre.right = self.pre, self.head return self.head","link":"/2020/10/26/剑指Offer-36-二叉搜索树与双向链表/"},{"title":"剑指Offer-35-复杂链表的复制","text":"题目 题目描述 请实现 copyRandomList 函数，复制一个复杂链表。在复杂链表中，每个节点除了有一个 next 指针指向下一个节点，还有一个 random 指针指向链表中的任意节点或者 null。 示例1 输入: head = [[7,null],[13,0],[11,4],[10,2],[1,0]]输出: [[7,null],[13,0],[11,4],[10,2],[1,0]] 示例2 输入: head = [[1,1],[2,1]]输出: [[1,1],[2,1]] 题解 Python深拷贝 直接调用Python深拷贝函数。 浅拷贝: 只复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存; 深拷贝: 创造一个一模一样的对象，新对象跟原对象不共享内存; \"\"\"# Definition for a Node.class Node: def __init__(self, x: int, next: 'Node' = None, random: 'Node' = None): self.val = int(x) self.next = next self.random = random\"\"\"class Solution: def copyRandomList(self, head: 'Node') -&gt; 'Node': return copy.deepcopy(head) 哈希表迭代 先 遍历存储 链表所有节点到 哈希表visited， 再次遍历 为visited中拷贝的节点分配next和random指针。 哈希表visited: {原节点: 拷贝节点}; 第一次遍历: 为将每个节点存储到哈希表中，并对应其拷贝节点; 第二次遍历: 为每个拷贝节点分配 next 和 random 指针; 时间复杂度：O(n)，遍历两次，每次遍历所有节点；空间复杂度：O(n)，新建大小为n的哈希表。 class Solution: def copyRandomList(self, head: 'Node') -&gt; 'Node': if not head: return visited = {None: None} temp = head while temp: visited[temp] = Node(temp.val, None, None) temp = temp.next temp = head while temp: visited[temp].next = visited[temp.next] visited[temp].random = visited[temp.random] temp = temp.next return visited[head] DFS 先递归分配所有节点的 next 指针，再次递归返回每个节点的 random 指针。 从头节点依次递归所有节点: 从头节点开始拷贝，将拷贝节点对应存储到哈希表中; 由于一个节点可能被多个指针指到，如果 该节点已被拷贝，则不需要重复拷贝; 若 还没拷贝该节点，则创建一个新的节点进行拷贝，并将拷贝过的节点保存在哈希表中; 递归拷贝所有的 next 节点，再递归拷贝所有的 random 节点; 时间复杂度：O(n)，递归遍历所有节点；空间复杂度：O(n)，递归占用辅助空间O(n)。 class Solution: def copyRandomList(self, head: 'Node') -&gt; 'Node': def dfs(node): if not node: return None if node in visited: return visited[node] copy = Node(node.val, None, None) visited[node] = copy copy.next = dfs(node.next) copy.random = dfs(node.random) return copy visited = {} return dfs(head) BFS 依次为每个节点分配next和random指针，并将其指针 指向节点加入队列，循环遍历所有节点。 从头节点依次递归所有节点: 创建 哈希表，存储头节点及其拷贝节点; 创建 队列，并将头节点入队; 当 队列不为空 时，弹出一个结点: 如果该结点的 next 结点未被拷贝过，则拷贝 next 结点并加入队列; 如果该结点的 random 结点未被拷贝过，则拷贝 random 结点并加入队列; 为 拷贝节点 分配 next 和 random 指针; 时间复杂度：O(n)，递归遍历所有节点；空间复杂度：O(n)，递归占用辅助空间O(n)。 class Solution: def copyRandomList(self, head: 'Node') -&gt; 'Node': if not head: return visited = {head: Node(head.val)} queue = collections.deque() queue.append(head) while queue: node = queue.popleft() if node.next and node.next not in visited: visited[node.next] = Node(node.next.val, None, None) queue.append(node.next) if node.random and node.random not in visited: visited[node.random] = Node(node.random.val, None, None) queue.append(node.random) visited[node].next = visited[node.next] if node.next in visited else None visited[node].random = visited[node.random] if node.random in visited else None return visited[head]","link":"/2020/10/22/剑指Offer-35-复杂链表的复制/"},{"title":"剑指Offer-34-二叉树中和为某一值的路径","text":"题目 题目描述输入一棵二叉树和一个整数，打印出二叉树中节点值的和为输入整数的所有路径。从树的根节点开始往下一直到叶节点所经过的节点形成一条路径。 示例1 给定二叉树: 5 / \\ 4 8 / / \\ 11 13 4 / \\ / \\7 2 5 1 返回: [ [5,4,11,2], [5,8,4,5]] 题解 DFS回溯 二叉树搜索问题，使用 回溯算法，先序遍历搜索节点，同时记录节点路径。 先序遍历: [根节点, 左子树, 右子树]; 路径记录: 记录根节点到当前节点的路径; 1)当前节点为叶子节点; 2)节点值和为sum; 从根节点向下搜索子树: 路径更新: 将当前节点加入路径; 目标值更新: 目标值-当前值; 路径记录: 当该路径满足目标值，则将其路径加入到res; 子树遍历: 递归搜素左、右子树; 路径恢复: 向上回溯前，将当前节点从路径中删除。 时间复杂度：O(n)，先序遍历所有节点；空间复杂度：O(n)，递归占用辅助空间O(n)。 # Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def pathSum(self, root: TreeNode, sum: int) -&gt; List[List[int]]: res, path = [], [] def dfs(root, sum): if not root: return path.append(root.val) sum -= root.val if sum == 0 and not root.left and not root.right: res.append(list(path)) dfs(root.left, sum) dfs(root.right, sum) path.pop() dfs(root, sum) return res","link":"/2020/10/22/剑指Offer-34-二叉树中和为某一值的路径/"},{"title":"剑指Offer-33-二叉搜索树的后序遍历序列","text":"题目 题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。 给定二叉树: 5 / \\ 2 6 / \\1 3 示例1 输入: [1,6,3,2,5]输出: false 示例2 输入: [1,3,2,6,5]输出: true 题解 后序遍历: [左子树，右子树，根节点]; 二叉搜索树: 左子树节点值&lt;根节点值，右子树节点值&gt;根节点值，左右子树也为二叉搜索树； 递归 最后一个元素为根节点，通过根节点分割左右子树，递归判断左右子树是否为二叉搜索树。 将最后一个元素设为根节点，从左向右遍历列表: 左子树: 左子树元素&lt;根节点元素，当前元素&gt;根节点时 结束; 右子树: 右子树元素&gt;根节点元素，当前元素&lt;根节点时 return False; 递归: 左右子树作为新列表分别递归判断。 终止条件: 当列表长度&lt;=1时，说明判断未出错，即该列表为二叉搜索树。 时间复杂度：O($n^2$)，最差情况，即树退化为链表，每次去掉根节点，需要n次，每轮需遍历所有节点，占用O(n)；空间复杂度：O(n)，递归占用辅助空间O(n)。 class Solution: def verifyPostorder(self, postorder: List[int]) -&gt; bool: def recur(nums): if len(nums) &lt;= 1: return True root = nums[-1] for left in range(len(nums)): if nums[left] &gt; root: break for right in range(left, len(nums)): if nums[right] &lt; root: return False return recur(nums[:left]) and recur(nums[left:-1]) if not postorder: return True return recur(postorder) 单调栈(引用Krahets) 设 根节点的值为无穷大，右子树为空，而左子树为题目给定的树，这样一来仍然是二叉搜索树。 初始化: 根节点$root=+\\infty$;倒序遍历列表: 判断: 若当前元素&gt;root，不满足定义，return False; 更新: 当遇到值递减的节点，循环执行出栈，并将其节点赋值给root; 入栈: 将当前节点入栈。 时间复杂度：O(n)，每个节点都会经历一次入栈出栈操作；空间复杂度：O(n)，最差情况，单调栈存储所有节点。 class Solution: def verifyPostorder(self, postorder: [int]) -&gt; bool: stack, root = [], float(\"+inf\") for i in range(len(postorder)-1, -1, -1): if postorder[i] &gt; root: return False while(stack and postorder[i] &lt; stack[-1]): root = stack.pop() stack.append(postorder[i]) return True","link":"/2020/10/20/剑指Offer-33-二叉搜索树的后序遍历序列/"},{"title":"CANet","text":"CANet(CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning)[1] consists of a two-branch dense comparison module which performs multi-level feature comparison, and an iterative optimization module which iteratively refines the predicted results. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning(CVPR 2019 paper)Code: PyTorchNote: Mendeley Paper Abstract Problem Solution Conceptual Understanding Dense Comparison Module(DCM) Feature Extractor: features in lower layers often relate to low-level cues, e.g., edges and colors while features in higher layers relate to object-level concepts such as object categories. Instead, we focus on middle-level features that may constitute object parts shared by unseen classes. Dense Comparison: Here, they use global average pooling over the foreground area to squeeze the feature maps to a feature vector, global image features turn out to be useful in segmentation tasks. After obtained the global feature vector from the support set, they concatenate the vector with all spatial locations in the feature map generated by the query branch. Iterative Optimization Module(IOM) They propose to incorporate the predicted masks in a residual form: $$M_t = x+F(x,y_{t-1})$$They use Atrous Spatial Pyramid Pooling module (ASPP) proposed to capture multi-scale information. Core Conception Experiments Code [Updating] Note Well described the latent details in feature map and its process. References [1] Zhang C, Lin G, Liu F, et al. Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 5217-5226.[2] CaNet. https://github.com/icoz69/CaNet.","link":"/2020/10/20/CANet/"},{"title":"SG-One","text":"SG-One(SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation)[1] adopt a masked average pooling strategy for producing the guidance features, then leverage the cosine similarity to build the relationship. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation(arXiv 2018 / TCYB 2020 paper)Code: PyTorchNote: Mendeley Paper Abstract Problem Description Current existing methods are all based on the Siamese framework, that is, a pair of parallel networks is trained for extracting the features of labeled support images and query images. The parameters of using the two parallel networks are redundant, which is prone to overfitting and leading to the waste of computational resources. Combining the features of support and query images by mere multiplication is inadequate for guiding the query network to learn high-quality segmentation masks. Problem Solution Conceptual Understanding Similarity Guidance branch: The extracted representative vectors of support images are expected to contain the high-level semantic features of a specific object. Segmentation branch: Through the concatenation, Segmentation Branch can borrow features from the paralleling branch, and these two branches can communicate information during the forward and backward stages. Core Conception Experiments Code [Updating] Note The latent distributions between the training classes and testing classes do not align, which prevents us from obtaining better features for input images. The predicted masks some-times can only cover part of the target regions and may include some background noises if the target object is too similar to the background. References [1] Zhang X, Wei Y, Yang Y, et al. Sg-one: Similarity guidance network for one-shot semantic segmentation[J]. IEEE Transactions on Cybernetics, 2020.[2] SG-One. https://github.com/xiaomengyc/SG-One.","link":"/2020/10/20/SG-One/"},{"title":"co-FCN","text":"co-FCN(Conditional Networks for Few-Shot Semantic Segmentation)[1] handle sparse pixel-wise annotations to achieve nearly the same accuracy. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Conditional Networks for Few-Shot Semantic Segmentation(ICLR 2018 paper)Code: [Code]Note: Mendeley Paper Abstract They propose the co-FCN, a conditional network learned by end-to-end optimization to perform fast, accurate few-shot segmentation. The network conditions on an annotated support set of images via feature fusion to do inference on an unannotated query image. Annotations are instead conditioned on in a single forward pass, making our method suitable for interactive use. Problem Description Some current methods rely on meta-learning, or learning to learn, in order to quickly adapt to new domains or tasks. It cannot be applied out-of-the-box to the structured output setting due to the high dimensionality of the output space. The statistical dependencies among outputs that result from the spatial correlation of pixels in the input. Problem Solution Our contributions cover handling sparse pixel-wise annotations, conditioning features vs. parameters. Our method achieves nearly the same accuracy with only one positive and one negative pixel. Conceptual Understanding Experiments Code [Updating] Note [Updating] References [1] Rakelly K, Shelhamer E, Darrell T, et al. Conditional networks for few-shot semantic segmentation[J]. 2018.","link":"/2020/10/19/co-FCN/"},{"title":"OSLSM","text":"OSLSM(One-Shot Learning for Semantic Segmentation)[1] firstly proposed two-branch approach to one-shot semantic segmentation. Conditioning branch trains a network to get parameter $\\theta$, and Segmentaion branch outputs the final mask based on parameter $\\theta$. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: One-Shot Learning for Semantic Segmentation(BMVC 2017 paper)Code: CaffeNote: Mendeley Paper Abstract They extend low-shot methods to support semantic segmentation. They train a network that produces parameters for a FCN. They use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. It outperforms the state-of-the-art method on the PASCAL VOC 2012 dataset. Problem Description A simple approach to performing one-shot semantic image segmentation is to fine-tune a pre-trained segmentation network on the labeled image. This approach is prone to over-fitting due to the millions of parameters being updated. The fine tuning approach to one-shot learning, which may require many iterations of SGD to learn parameters for the segmentation network. Besides, thousands of dense features are computed from a single image and one-shot methods do not scale well to this many features. Problem Solution Conceptual Understanding Core Conception Experiments Code [Updating] Note It takes inspiration from few-shot learning and firstly proposes a novel two-branched approach to one-shot semantic segmentation. References [1] Shaban A, Bansal S, Liu Z, et al. One-shot learning for semantic segmentation[J]. arXiv preprint arXiv:1709.03410, 2017.[2] OSLSM. https://github.com/lzzcd001/OSLSM.","link":"/2020/10/19/OSLSM/"},{"title":"剑指Offer-32-从上到下打印二叉树","text":"题目I 题目描述从上到下打印出二叉树的每个节点，同一层的节点按照从左到右的顺序打印。 示例 给定二叉树: [3,9,20,null,null,15,7], 3 / \\9 20 / \\ 15 7 返回: [3,9,20,15,7] 题解 BFS / 队列 二叉树的层次遍历，采用广度优先遍历（BFS），使用队列实现。 当队列不为空时，循环: 出队: 队首元素出队; 打印: 将当前元素的值添加到res; 子结点入队: 若当前元素的左右子树不为空，则将其加入队列。 时间复杂度：O(n)，每个结点都会经历入队出队操作，即BFS将会循环n次；空间复杂度：O(n)，最差情况，即树为平衡二叉树时，最多有n/2个结点同时存储在队列。 # Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def levelOrder(self, root: TreeNode) -&gt; List[int]: if not root: return [] res, queue = [], collections.deque() queue.append(root) while queue: node = queue.popleft() res.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) return res Python 中使用 collections 中的双端队列 deque() ，其 popleft() 方法可达到 O(1) 时间复杂度；列表 list 的 pop(0) 方法时间复杂度为 O(N)。 题目II 题目描述从上到下按层打印二叉树，同一层的节点按从左到右的顺序打印，每一层打印到一行。 示例 给定二叉树: [3,9,20,null,null,15,7], 3 / \\9 20 / \\ 15 7 返回其层次遍历结果: [ [3], [9,20], [15,7]] 题解 BFS / 队列 二叉树的层次遍历，采用广度优先遍历（BFS），使用队列实现，使用列表存储每层所有结点元素。 每层打印到一行: 将本层全部结点打印到列表，再将下层结点全部加入队列。 当队列不为空时，循环: 新建列表cur,用于存储当层打印结果; 当前层打印，循环次数为当前层结点个数，即队列长度: a)出队: 队首元素出队; b)打印: 将当前元素的值添加到列表cur; c)子结点入队: 若当前元素的左右子树不为空，则将其加入队列; 将当前层列表加入到res。 时间复杂度：O(n)，每个结点都会经历入队出队操作，即BFS将会循环n次；空间复杂度：O(n)，最差情况，即树为平衡二叉树时，最多有n/2个结点同时存储在队列。 class Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] res, queue = [], collections.deque() queue.append(root) while queue: cur = [] for _ in range(len(queue)): node = queue.popleft() cur.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) res.append(cur) return res 题目III 题目描述请实现一个函数按照之字形顺序打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右到左的顺序打印，第三行再按照从左到右的顺序打印，其他行以此类推。 示例 给定二叉树: [3,9,20,null,null,15,7], 3 / \\9 20 / \\ 15 7 返回其层次遍历结果: [ [3], [20,9], [15,7]] 题解 BFS + 倒序 当res的长度为偶数时，即当前层为 偶数层，此时对当前层列表进行倒序操作。 时间复杂度：O(n)，每个结点都会经历入队出队操作，即BFS将会循环n次，占用O(n)；进行少于n个节点的倒序操作，占用O(n)；空间复杂度：O(n)，最差情况，即树为平衡二叉树时，最多有n/2个结点同时存储在队列。 class Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] res, queue = [], collections.deque() queue.append(root) while queue: cur = [] for _ in range(len(queue)): node = queue.popleft() cur.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) res.append(cur[::-1] if len(res)%2 else cur) return res BFS + 双端队列 将当前层输出改为双端队列，当为奇数层时，元素加入队列尾部；当为 偶数层 时，元素加入队列头部。 时间复杂度：O(n)，每个结点都会经历入队出队操作，即BFS将会循环n次；空间复杂度：O(n)，最差情况，即树为平衡二叉树时，最多有n/2个结点同时存储在队列。 class Solution: def levelOrder(self, root: TreeNode) -&gt; List[List[int]]: if not root: return [] res, queue = [], collections.deque() queue.append(root) while queue: cur = collections.deque() for _ in range(len(queue)): node = queue.popleft() if len(res) % 2: cur.appendleft(node.val) else: cur.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) res.append(list(cur)) return res","link":"/2020/10/18/剑指Offer-32-从上到下打印二叉树/"},{"title":"剑指Offer-31-栈的压入、弹出序列","text":"题目 题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如，序列 {1,2,3,4,5} 是某栈的压栈序列，序列 {4,5,3,2,1} 是该压栈序列对应的一个弹出序列，但 {4,3,5,1,2} 就不可能是该压栈序列的弹出序列。 示例1 输入: pushed = [1,2,3,4,5], popped = [4,5,3,2,1]输出: true解释: 我们可以按以下顺序执行：push(1), push(2), push(3), push(4), pop() -&gt; 4,push(5), pop() -&gt; 5, pop() -&gt; 3, pop() -&gt; 2, pop() -&gt; 1 示例2 输入: pushed = [1,2,3,4,5], popped = [4,3,5,1,2]输出: false解释: 1 不能在 2 之前弹出。 题解 辅助栈 借用一个 辅助栈，模拟压入/弹出操作 的排列，根据是否模拟成功，即可得到结果。 入栈操作: 按照压栈序列的顺序执行; 出栈操作: 每次入栈后，循环判断 “栈顶元素 == 弹出序列的当前元素” 是否成立，将符合弹出序列顺序的栈顶元素全部弹出。 时间复杂度：O(n)，每个元素最多入栈与出栈一次，即2n次出入栈操作；空间复杂度：O(n)，辅助栈最多存储n个元素。 class Solution: def validateStackSequences(self, pushed: List[int], popped: List[int]) -&gt; bool: stack, i = [], 0 for num in pushed: stack.append(num) while stack and stack[-1] == popped[i]: stack.pop() i += 1 return not stack 原地辅助栈 将压栈序列pushed作为辅助栈，指针i,j分别指向pushed栈顶及poped首位元素。 将pushed作为辅助栈，入栈出栈操作转为对指针操作。 时间复杂度：O(n)，每个元素最多入栈与出栈一次，即2n次出入栈操作；空间复杂度：O(1)，pushed作为辅助栈，指针i,j只需常数额外空间。 class Solution: def validateStackSequences(self, pushed: List[int], popped: List[int]) -&gt; bool: i, j = 0, 0 for num in pushed: pushed[i] = num while i &gt;= 0 and pushed[i] == popped[j]: j += 1 i -= 1 i += 1 return i == 0","link":"/2020/10/16/剑指Offer-31-栈的压入、弹出序列/"},{"title":"剑指Offer-30-包含min函数的栈","text":"题目 题目描述定义栈的数据结构，请在该类型中实现一个能够得到栈的最小元素的 min 函数在该栈中，调用 min、push 及 pop 的时间复杂度都是 O(1)。 示例 MinStack minStack = new MinStack();minStack.push(-2);minStack.push(0);minStack.push(-3);minStack.min(); –&gt; 返回 -3.minStack.pop();minStack.top(); –&gt; 返回 0.minStack.min(); –&gt; 返回 -2. 题解 辅助栈 利用辅助栈存储当前最小元素，每当push新的最小值或pop当前最小值时更新辅助栈。 主栈stack: 存储所有元素，实现 push(), pop(), top() 函数; 辅助栈stack_min: 存储stack非严格降序元素，栈顶元素为stack中当前最小元素，实现 min() 函数。 时间复杂度：O(n)，遍历二叉树；空间复杂度：O(n)，递归占用额外空间。 class MinStack: def __init__(self): \"\"\" initialize your data structure here. \"\"\" self.stack, self.stack_min = [], [] def push(self, x: int) -&gt; None: self.stack.append(x) if not self.stack_min or x &lt;= self.stack_min[-1]: self.stack_min.append(x) def pop(self) -&gt; None: if self.stack.pop() == self.stack_min[-1]: self.stack_min.pop() def top(self) -&gt; int: return self.stack[-1] def min(self) -&gt; int: return self.stack_min[-1]# Your MinStack object will be instantiated and called as such:# obj = MinStack()# obj.push(x)# obj.pop()# param_3 = obj.top()# param_4 = obj.min()","link":"/2020/10/14/剑指Offer-30-包含min函数的栈/"},{"title":"剑指Offer-29-顺时针打印矩阵","text":"题目 题目描述输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字。 示例1 输入: matrix = [[1,2,3],[4,5,6],[7,8,9]]输出: [1,2,3,6,9,8,7,4,5] 示例2 输入: matrix = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]输出: [1,2,3,4,8,12,11,10,9,5,6,7] 题解 逆时针旋转(引用SolitudeRain) 提取最前面的元素，然后将剩下元素逆时针旋转90度，再依次循环进行。 沿主对角线对称+沿水平方向对称= 将原矩阵逆时针旋转90度 ，所以每次只要pop出第一行就可以了。 class Solution: def spiralOrder(self, matrix: List[List[int]]) -&gt; List[int]: result = [] while matrix: result.extend(list(matrix.pop(0))) matrix = list(zip(*matrix)) matrix.reverse() return result 设定边界 设定矩阵的“上、下、左、右”四个边界，模拟题目要求遍历顺序。 空值处理: 当 matrix 为空时，直接return; 初始化: 矩阵 左、右、上、下边界标志 l,r,t,b 及最终结果 res; 循环遍历: 按照“左到右、上到下、右到左、下到上”循环，每个方向进行如下操作， 输出当前元素到res，即打印矩阵; 边界向内收缩，即该行或该列已被打印; 判断是否越界，是否打印完毕； 时间复杂度：O(mn)，每遍历一圈占用O(max{m,n})，共需便利min{m,n}轮；空间复杂度：O(1)，四个边界标志占用常数大小的额外空间。 class Solution: class Solution: def spiralOrder(self, matrix:[[int]]) -&gt; [int]: if not matrix: return [] l, r, t, b, res = 0, len(matrix[0])-1, 0, len(matrix)-1, [] while True: for i in range(l, r+1): res.append(matrix[t][i]) # left to right t += 1 if t &gt; b: break for i in range(t, b + 1): res.append(matrix[i][r]) # top to bottom r -= 1 if l &gt; r: break for i in range(r, l - 1, -1): res.append(matrix[b][i]) # right to left b -= 1 if t &gt; b: break for i in range(b, t - 1, -1): res.append(matrix[i][l]) # bottom to top l += 1 if l &gt; r: break return res","link":"/2020/10/14/剑指Offer-29-顺时针打印矩阵/"},{"title":"剑指Offer-28-对称的二叉树","text":"题目 题目描述请实现一个函数，用来判断一棵二叉树是不是对称的。如果一棵二叉树和它的镜像一样，那么它是对称的。 例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 1 / \\ 2 2 / \\ / \\3 4 4 3 但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的: 1 / \\2 2 \\ \\ 3 3 示例1 输入: root = [1,2,2,3,4,4,3]输出: true 示例2 输入: root = [1,2,2,null,3,null,3]输出: false 题解 递归 以三层二叉树结点为递归基础，每次将根节点的左右结点和右右结点进行交换，然后将根节点的左、右结点分别作为根节点进行递归。 对称二叉树特性： root.left == root.right: 根节点的左、右子结点相等; root.left.left == root.right.right: 左子树结点的左结点与右子树结点的右结点相等; root.left.right == root.right.left: 左子树结点的右结点与右子树结点的左结点相等; 终止条件： 根节点为空，return True; 根节点的左子树与右子树均为空，即遍历到叶子结点时二叉树仍对称，return True; 根节点的左子树或右子树一边为空，一边不为空，故二叉树不对称，return False; 根节点的左右子树结点不相等，return False; 根据对称二叉树特性，为判断root.left.left == root.right.right是否成立，则只需将root.left.right与root.right.right交换，从而比较root.left.left与root.left.right是否相等即可。 时间复杂度：O(n)，遍历二叉树；空间复杂度：O(n)，递归占用额外空间。 # Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSymmetric(self, root: TreeNode) -&gt; bool: if not root: return True if not root.left and not root.right: return True elif not root.left or not root.right: return False if root.left.val != root.right.val: return False root.left.right, root.right.right = root.right.right, root.left.right return self.isSymmetric(root.left) and self.isSymmetric(root.right)","link":"/2020/09/24/剑指Offer-28-对称的二叉树/"},{"title":"剑指Offer-27-二叉树的镜像","text":"题目 题目描述请完成一个函数，输入一个二叉树，该函数输出它的镜像。 例如输入： 4 / \\ 2 7 / \\ / \\1 3 6 9 镜像输出： 4 / \\ 7 2 / \\ / \\9 6 3 1 示例1 输入: root = [4,2,7,1,3,6,9]输出: [4,7,2,9,6,3,1] 题解 递归 遍历二叉树，将当前结点左右子结点交换，然后将左右子树递归交换。 时间复杂度：O(n)，遍历二叉树；空间复杂度：O(n)，递归占用额外空间。 # Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def mirrorTree(self, root: TreeNode) -&gt; TreeNode: if not root: return root.left, root.right = root.right, root.left self.mirrorTree(root.left) self.mirrorTree(root.right) return root 辅助栈/队列 利用栈（或队列）遍历树的所有节点，并交换当前结点的左右子节点。 辅助栈弹出当前结点，存储其左右子树; 将当前结点的左右子结点交换; 时间复杂度：O(n)，遍历二叉树；空间复杂度：O(n)，辅助栈存储遍历得到的结点。 class Solution: def mirrorTree(self, root: TreeNode) -&gt; TreeNode: if not root: return stack = [root] while stack: node = stack.pop() if node.left: stack.append(node.left) if node.right: stack.append(node.right) node.left, node.right = node.right, node.left return root","link":"/2020/09/23/剑指Offer-27-二叉树的镜像/"},{"title":"剑指Offer-26-树的子结构","text":"题目 题目描述输入两棵二叉树A和B，判断B是不是A的子结构。(约定空树不是任意一个树的子结构) B是A的子结构， 即 A中有出现和B相同的结构和节点值。 例如:给定的树 A: 3 / \\ 4 5 / \\ 1 2 给定的树 B： 4 /1 返回 true，因为 B 与 A 的一个子树拥有相同的结构和节点值。 示例1 输入: A = [1,2,3], B = [3,1]输出: false 示例2 输入: A = [3,4,5,1,2], B = [4,1]输出: true 题解 递归+DFS 遍历两棵树，左右子树分别递归判断是否为子结构。 若B树为A树的子结构，则有如下三种情况： A树与B树完全相等 A的左子树与B树完全相等 A的右子树与B树完全相等 DFS搜索两树是否相等： 若A树为空，返回False；若B树为空，则说明B树为A树子结构，返回True; 两棵树都不为空，则需比较如下三种情况： A的根节点与B的根节点是否相同 A的左子树与B的左子树是否相同 A的右子树与B的右子树是否相同 时间复杂度：O(mn)，先序遍历树A占用O(m)，每次调用DFS判断占用O(n)；空间复杂度：O(m)，递归占用额外空间。 # Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSubStructure(self, A: TreeNode, B: TreeNode) -&gt; bool: if not A or not B: return False return self.dfs(A, B) or self.isSubStructure(A.left, B) or self.isSubStructure(A.right, B) def dfs(self, A, B): if not A: return False elif not B: return True else: return A.val == B.val and self.dfs(A.left, B.left) and self.dfs(A.right, B.right)","link":"/2020/09/23/剑指Offer-26-树的子结构/"},{"title":"剑指Offer-25-合并两个排序的链表","text":"题目 题目描述输入两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的。 示例 输入: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 题解 递归 遍历整个链表， 每次选出最小结点作为l1结点，并将l1下个结点进行递归返回。 时间复杂度：O(m+n)，遍历链表；空间复杂度：O(m+n)，递归占用额外空间。 # Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: if not l1: return l2 elif not l2: return l1 else: (l1, l2) = (l2, l1) if l2.val &lt; l1.val else (l1, l2) l1.next = self.mergeTwoLists(l1.next, l2) return l1 伪结点 用两个指针指向伪结点，cur指针用于遍历最小数值结点加入新链表，最终返回dummy指向的结点，即为已拍好序的新链表（指向结点仍为原链表结点）。 指针更新: 如果l1比l2头节点的值小，则cur指向l1，l1头节点后移；反之l2同理；然后cur指针后移，进行下一次迭代。 后处理: 如果l1链表仍有结点，则cur指针指向l1；反之指向l2。 时间复杂度：O(m+n)，遍历l1和l2链表；空间复杂度：O(1)，cur,dummy指针占用常数大小空间。 class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -&gt; ListNode: cur = dummy = ListNode(0) while l1 and l2: if l1.val &lt;= l2.val: cur.next, l1 = l1, l1.next else: cur.next, l2 = l2, l2.next cur = cur.next cur.next = l1 if l1 else l2 return dummy.next","link":"/2020/09/21/剑指Offer-25-合并两个排序的链表/"},{"title":"剑指Offer-24-反转链表","text":"题目 题目描述定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。 示例 输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 题解 递归 遍历整个链表，从最后结点开始，每次将结点指向反向，然后递归返回。 将两个结点反向: head.next.next = head; head.next = None;[例]4-&gt;5-&gt;None: 1)4-&gt;5,5-&gt;4; 2)5-&gt;4-&gt;None; 时间复杂度：O(n)，遍历链表；空间复杂度：O(n)，递归占用额外空间。 # Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseList(self, head: ListNode) -&gt; ListNode: if not head or not head.next: return head node = self.reverseList(head.next) head.next.next = head head.next = None return node 多指针 遍历整个链表，pre指向已反向链表，cur指向当前结点，next指向下个结点，每次 将当前结点指向pre已反向链表，并把cur指向下个结点。 结点反向: cur.next = pre; 指针更新: next = cur.next; pre = cur; cur = next;[例]1-&gt;2-&gt;3-&gt;None: 1)None&lt;-1,2-&gt;3-&gt;None; 2)None&lt;-1&lt;-2,3-&gt;None; 3)None&lt;-1&lt;-2&lt;-3; 时间复杂度：O(n)，遍历链表；空间复杂度：O(1)，三个指针占用常数大小空间。 class Solution: def reverseList(self, head: ListNode) -&gt; ListNode: pre, cur = None, head while cur: next = cur.next cur.next = pre pre, cur = cur, next return pre","link":"/2020/09/20/剑指Offer-24-反转链表/"},{"title":"剑指Offer-22-链表中倒数第k个节点","text":"题目 题目描述输入一个链表，输出该链表中倒数第k个节点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。例如，一个链表有6个节点，从头节点开始，它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个节点是值为4的节点。 示例 给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 k = 2.返回链表 4-&gt;5. 题解 两次遍历 第一次遍历得到链表长度，第二次遍历到倒数第k个结点返回。 时间复杂度：O(n)，两次遍历链表，每次占用O(n)；空间复杂度：O(1)，两次指针占用常数大小空间。 # Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def getKthFromEnd(self, head: ListNode, k: int) -&gt; ListNode: first, cur, lens =head, head, 0 while first: first = first.next lens += 1 for i in range(lens-k): cur = cur.next return cur 快慢指针 快指针先走k步，然后快慢指针同时向后遍历，当快指针走到链表尾部，则慢指针指向倒数第k个结点。 时间复杂度：O(n)，遍历链表；空间复杂度：O(1)，双指针占用常数大小空间。 class Solution: def getKthFromEnd(self, head: ListNode, k: int) -&gt; ListNode: fast, slow = head, head for _ in range(k): if not fast: return fast = fast.next while fast: fast, slow = fast.next, slow.next return slow","link":"/2020/09/20/剑指Offer-22-链表中倒数第k个节点/"},{"title":"剑指Offer-21-调整数组顺序使奇数位于偶数前面","text":"题目 题目描述输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有奇数位于数组的前半部分，所有偶数位于数组的后半部分。 题解 分别存储 odd 和 even 分别存储，最后合并。 时间复杂度：O(n)，遍历列表；空间复杂度：O(n)，奇数和偶数共占O(n)空间。 class Solution: def exchange(self, nums: List[int]) -&gt; List[int]: odd, even = [], [] for i in nums: if i % 2 == 0: even.append(i) else: odd.append(i) return odd + even 单指针 指针odd指向已确定奇数后的第一个数，遍历整个列表，若为奇数则交换。 时间复杂度：O(n)，遍历列表；空间复杂度：O(1)，指针odd占用常数空间。 class Solution: def exchange(self, nums: List[int]) -&gt; List[int]: odd = 0 for i in range(len(nums)): if nums[i] % 2 == 1: nums[odd], nums[i] = nums[i], nums[odd] odd += 1 return nums 双指针 指针odd指向前向奇数，指针even指向后向偶数，从前遍历整个列表，若为偶数则与后方交换。 时间复杂度：O(n)，遍历列表；空间复杂度：O(1)，指针odd和even占用常数空间。 class Solution: def exchange(self, nums: List[int]) -&gt; List[int]: odd, even = 0, len(nums)-1 while odd &lt; even: if nums[odd] % 2 == 0: nums[odd], nums[even] = nums[even], nums[odd] even -= 1 else: odd += 1 return nums","link":"/2020/09/19/剑指Offer-21-调整数组顺序使奇数位于偶数前面/"},{"title":"剑指Offer-20-表示数值的字符串","text":"题目 题目描述请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”、”5e2”、”-123”、”3.1416”、”-1E-16”、”0123”都表示数值，但”12e”、”1a3.14”、”1.2.3”、”+-5”及”12e+5.4”都不是。 题解 直接转换 python3调用try-catch，成功转化则return True。 class Solution: def isNumber(self, s: str) -&gt; bool: try: float(s) return True except ValueError: return False 有限状态自动机(引用Krahets) 根据字符类型和合法数值的特点，先定义状态，再画出状态转移图。 字符类型空格 「 」、数字「 0—9 」 、正负号 「 +- 」 、小数点 「 . 」 、幂符号 「 eE 」 。 状态定义合法的结束状态有 2, 3, 7, 8 。 状态转移表states：设states[i]，其中i为所处状态，states[i]使用哈希表存储可转移至的状态； 当前状态p：状态初始化为 p = 0； 状态转移循环：遍历字符串s的每个字符c。 1)记录字符类型t; 2)若字符类型t不在哈希表states[p]中，说明无法转移至下一状态,直接return False; 3)状态p转移至states[p][t]； 返回值：跳出循环后，2,3,7,8为合法结束状态。 时间复杂度：O(n)，遍历字符串；空间复杂度：O(1)，states和p占用常数大小的额外空间。 class Solution: def isNumber(self, s: str) -&gt; bool: states = [ { ' ': 0, 's': 1, 'd': 2, '.': 4 }, # 0. start with 'blank' { 'd': 2, '.': 4 } , # 1. 'sign' before 'e' { 'd': 2, '.': 3, 'e': 5, ' ': 8 }, # 2. 'digit' before 'dot' { 'd': 3, 'e': 5, ' ': 8 }, # 3. 'digit' after 'dot' { 'd': 3 }, # 4. 'digit' after 'dot' (‘blank’ before 'dot') { 's': 6, 'd': 7 }, # 5. 'e' { 'd': 7 }, # 6. 'sign' after 'e' { 'd': 7, ' ': 8 }, # 7. 'digit' after 'e' { ' ': 8 } # 8. end with 'blank' ] p = 0 # start with state 0 for c in s: if '0' &lt;= c &lt;= '9': t = 'd' # digit elif c in \"+-\": t = 's' # sign elif c in \"eE\": t = 'e' # e or E elif c in \". \": t = \".\" # dot, blank else: t = '?' # unknown if t not in states[p]: return False p = states[p][t] return p in (2, 3, 7, 8)","link":"/2020/09/18/剑指Offer-20-表示数值的字符串/"},{"title":"剑指Offer-19-正则表达式匹配","text":"题目 题目描述请实现一个函数用来匹配包含'.'和'*'的正则表达式。模式中的字符’.’表示任意一个字符，而'*'表示它前面的字符可以出现任意次（含0次）。在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式&quot;a.a&quot;和&quot;ab*ac*a&quot;匹配，但与&quot;aa.a&quot;和&quot;ab*a&quot;均不匹配。 示例1 输入：s = “aa”, p = “a”输出：false解释：”a” 无法匹配 “aa” 整个字符串。 示例2 输入：s = “aa”, p = “a*“输出：true解释：因为 ‘*‘ 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 ‘a’。因此，字符串 “aa” 可被视为 ‘a’ 重复了一次。 示例3 输入：s = “ab”, p = “.*“输出：true解释：”.*“ 表示可匹配零个或多个（’*‘）任意字符（’.’）。 示例4 输入：s = “aab”, p = “c*a*b”输出：true解释：因为 ‘*’ 表示零个或多个，这里 ‘c’ 为 0 个, ‘a’ 被重复一次。因此可以匹配字符串 “aab”。 示例5 输入：s = “mississippi”, p = “mis*is*p*.”输出：false 题解 re库函数 python3调用re正则模块。 class Solution: def isMatch(self, s: str, p: str) -&gt; bool: return (re.fullmatch(p, s)) != None 递归(引用WEAllen) 核心思路：遇到 x* 的情况其实只有2种，只需分别处理这两种情况。 当x不匹配时：抛弃掉整个 x*； 当x匹配时：要么抵消掉一个s，x* 保留；要么整个 x* 抛弃掉。 class Solution: def isMatch(self, s: str, p: str) -&gt; bool: if not p: return not s first_match = bool(s) and p[0] in (\".\", s[0]) if len(p) &gt;= 2 and p[1] == \"*\": if first_match: return self.isMatch(s[1:], p) or self.isMatch(s, p[2:]) else: return self.isMatch(s, p[2:]) return first_match and self.isMatch(s[1:], p[1:]) 动态规划(引用superkakayong,loick) 当 'x*' 出现时，选择抛弃或保留。 状态定义：dp为二维数组，dp[i][j]表示s的前i个字符和p的前j个字符是否匹配。 初始状态：dp[0][0] = True，两个空字符是匹配的。 转移方程1：i == 0时，表示以空字符串去匹配p的前j个字符，仅当p的字符为 ‘x*’ 时为True。 转移方程2：当p的当前j字符为'.'或者与s的当前i字符相同时，只需看dp[i-1][j-1]是否匹配。 转移方程3：当p的当前j字符为'*'时，1)p的前一个字符x与s的当前i字符不匹配，且x不为’.’，则抛弃掉p中的 ‘x*‘，只需看dp[i][j-2]是否匹配；2)否则，保留 ‘x*‘，当dp[i][j-2]，dp[i][j-1]，dp[i-1][j]任一满足时为True。 时间复杂度：O(mn)，计算二维数组dp[m][n]；空间复杂度：O(mn)，保存二维数组dp[m][n]。 class Solution: def isMatch(self, s: str, p: str) -&gt; bool: m, n = len(s)+1, len(p)+1 dp = [[False]*n for _ in range(m)] dp[0][0] = True for i in range(1, n): if p[i-1] == '*' and dp[0][i-2] == True: dp[0][i] = True for i in range(1, m): for j in range(1, n): if p[j-1] == s[i-1] or p[j-1] == '.': dp[i][j] = dp[i-1][j-1] elif p[j-1] == '*': if p[j-2] != s[i-1] and p[j-2] != '.': dp[i][j] = dp[i][j-2] else: dp[i][j] = dp[i][j-2] or dp[i][j-1] or dp[i-1][j] else: dp[i][j] == False return dp[-1][-1]","link":"/2020/09/14/剑指Offer-19-正则表达式匹配/"},{"title":"剑指Offer-18-删除链表的节点","text":"题目 题目描述给定单向链表的头指针和一个要删除的节点的值，定义一个函数 删除该节点。返回删除后的链表的头节点。 示例1 输入：head = [4,5,1,9], val = 5输出：[4,1,9]解释：给定你链表中值为 5 的第二个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 1 -&gt; 9. 示例2 输入：head = [4,5,1,9], val = 1输出：[4,5,9]解释：给定你链表中值为 1 的第三个节点，那么在调用了你的函数之后，该链表应变为 4 -&gt; 5 -&gt; 9. 题解 递归 判断 下个结点的值是否为目标值，若不是则将下个结点设为头节点，递归搜索。 特例：头节点的值为目标值，则将下个结点设为头节点，return; 找到目标结点：删除目标结点，return; 未找到目标结点：将下个结点作为头节点，递归搜索目标。 时间复杂度：O(n)，遍历整个链表；空间复杂度：O(n)，递归占用辅助空间大小。 # Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def deleteNode(self, head: ListNode, val: int) -&gt; ListNode: if head.val == val: head = head.next elif head.next.val == val: head.next = head.next.next else: self.deleteNode(head.next, val) return head 单指针 单指针遍历链表，循环搜索下个结点是否为目标结点，满足则return。 特例：头节点的值为目标值，则将下个结点设为头节点，return; 找到目标结点：删除目标结点，return; 未找到目标结点：将下个结点作为头节点，遍历搜索链表。 时间复杂度：O(n)空间复杂度：O(1) class Solution: def deleteNode(self, head: ListNode, val: int) -&gt; ListNode: cur = head if cur.val == val: return cur.next while cur.next: if cur.next.val == val: cur.next = cur.next.next return head else: cur = cur.next","link":"/2020/09/14/剑指Offer-18-删除链表的节点/"},{"title":"剑指Offer-17-打印从1到最大的n位数","text":"题目 题目描述输入数字 n，按顺序打印出 从 1 到最大的 n 位十进制数。比如输入 3，则打印出 1、2、3 一直到最大的 3 位数 999。 示例1 输入：n = 1输出：[1,2,3,4,5,6,7,8,9] 说明： 用返回一个整数列表来代替打印 n 为正整数 题解 直接输出 start=1, end=$10^n - 1$。 时间复杂度：O($10^n$)，生成长度为$10^n$的列表；空间复杂度：O(1) class Solution: def printNumbers(self, n: int) -&gt; List[int]: res = [] for i in range(1, 10 ** n): res.append(i) return res 利用Python特性，range()可生成所需序列，list()将序列转为列表返回。 class Solution: def printNumbers(self, n: int) -&gt; List[int]: return list(range(1, 10 ** n)) 大数打印(引用Krahets) DFS生成全排列：先固定高位，递归搜索所有低位数字，当个位数字被固定时，则添加数字的字符串。 [例]：当n=2时，即数字范围为1-99，先固定十位0-9，按顺序依次递归搜索；固定个位0-9时，终止递归并产生数字的字符串。 根据上述方法，可初步编写全排列代码： class Solution: def printNumbers(self, n: int) -&gt; [int]: def dfs(x): if x == n: # 终止条件：已固定完所有位 res.append(''.join(num)) # 拼接 num 并添加至 res 尾部 return for i in range(10): # 遍历 0 - 9 num[x] = str(i) # 固定第 x 位为 i dfs(x + 1) # 开启固定第 x + 1 位 num = ['0'] * n # 起始数字定义为 n 个 0 组成的字符列表 res = [] # 数字字符串列表 dfs(0) # 开启全排列递归 return ','.join(res) # 拼接所有数字字符串，使用逗号隔开，并返回 在此方法下，各数字字符串被逗号隔开，共同组成长字符串。返回的数字集字符串如下所示： 输入：n = 1输出：&quot;0,1,2,3,4,5,6,7,8,9&quot;输入：n = 2输出：&quot;00,01,02,...,10,11,12,...,97,98,99&quot;输入：n = 3输出：&quot;000,001,002,...,100,101,102,...,997,998,999&quot; 观察可知，当前的生成方法仍有以下问题：1)数字前有多余0; 2)从0开始生成列表。解决方法如下： 删除高位多余的0: 1）定义字符串左边界，以保证添加的数字字符串 num[start:] 中无高位多余的0; 2)更新左边界start，当输出数字是9时，则下个数字需要进位，此时左边界 start 需要减1，即高位多余的0减少一个。 列表从1开始: 在以上方法基础上，只需在添加数字字符串前判断其是否为 “0”。 时间复杂度：O($10^n$)空间复杂度：O($10^n$)，递归占用O(logn)额外空间。 class Solution: def printNumbers(self, n: int) -&gt; [int]: def dfs(x): if x == n: s = ''.join(num[self.start:]) if s != '0': res.append(int(s)) if n - self.start == self.count_nine: self.start -= 1 return for i in range(10): if i == 9: self.count_nine += 1 num[x] = str(i) dfs(x + 1) self.count_nine -= 1 num, res = ['0'] * n, [] self.count_nine = 0 self.start = n - 1 dfs(0) return res","link":"/2020/09/11/剑指Offer-17-打印从1到最大的n位数/"},{"title":"剑指Offer-16-数值的整数次方","text":"题目 题目描述实现函数double Power(double base, int exponent)，求base的exponent次方。不得使用库函数，同时不需要考虑大数问题。 示例1 输入：2.00000, 10输出：1024.00000 示例2 输入：2.10000, 3输出：9.26100 示例3 输入：2.00000, -2输出：0.25000解释：$2^{-2} = 1/2^{2} = 1/4 = 0.25$ 题解 快速幂 把指数n做 “二进制分解”，在 底数不断自身乘以自身 的过程中，将最终结果需要的部分保存下来。 $x^n = x^{n/2} * x^{n/2} = (x^2)^{n/2}$, 设//为向下取整除法: n为偶数：$x^n = (x^2)^{n//2}$； n为奇数：$x^n = x(x^2)^{n//2}$, 即多出一项x。 1)若x==0，则返回0; 2)初始化res=1; 3)当n&lt;0时，将x转为分数，n取负数; 4)循环：a.若n&amp;1==1,则将当前x乘入res; b.执行$x=x^2$; c.将n右移一位。 时间复杂度：O(logn)，二分对数级别时间复杂度；空间复杂度：O(1) class Solution: def myPow(self, x: float, n: int) -&gt; float: if x == 0: return 0 res = 1 if n &lt; 0: x, n = 1 / x, -n while n: if n &amp; 1: res *= x x *= x n &gt;&gt;= 1 return res 递归 将指数n每次 二分，然后分 奇偶数分别递归 求解。 递推：1)n为奇数: 提出x，并使n-1，继续递归; 2)n为偶数: 将n二分，继续递归。 终止：当 n == 0 时，x的0次幂为1。 1)若x==0，返回0; 2)若n==0，递推结束; 3)若n&lt;0，将x取分数，幂次取反; 4)分奇偶递推。 时间复杂度：O(logn)空间复杂度：O(1) class Solution: def myPow(self, x: float, n: int) -&gt; float: if x == 0: return 0 if n == 0: return 1 elif n &lt; 0: return 1 / self.myPow(x, -n) if n &amp; 1: return x * self.myPow(x, n-1) else: return self.myPow(x*x, n//2)","link":"/2020/09/07/剑指Offer-16-数值的整数次方/"},{"title":"剑指Offer-15-二进制中1的个数","text":"题目 题目描述请实现一个函数，输入一个整数，输出该数二进制表示中 1 的个数。例如，把 9 表示成二进制是 1001，有 2 位是 1。因此，如果输入 9，则该函数输出 2。 示例1 输入：11输出：3解释：输入11的二进制串 00000000000000000000000000001011 中，共有三位为 ‘1’。 示例2 输入：256输出：1解释：输入256的二进制串 00000000000000000000000010000000 中，共有一位为 ‘1’。 题解 逐位右移 输入数字n的二进制与1作 与运算&amp;，判断二进制最右位是否为1，然后将n 循环右移。 n&amp;1 == 1：数字n的二进制最右位为1； n&amp;1 == 0：数字n的二进制最右位为0。 1)判断最右位是否为1，并将结果计数; 2)将n右移一位; 3)循环结束后返回计数器值。[例]：11的二进制为0x1011，1)n&amp;1==1; 2)n&gt;&gt;1==5(0x0101); 循环。 时间复杂度：O(logn)，数字n的二进制共有logn位，故需循环logn次；空间复杂度：O(1) class Solution: def hammingWeight(self, n: int) -&gt; int: count = 0 while n: count += n &amp; 1 n &gt;&gt;= 1 return count n &amp; (n-1) n&amp;(n-1) 将最右一位1消去 变为0，计数并循环直到 消去所有1。 n-1：把数字n的二进制中最右边的1变为0，并将其右边的0全变为1。[例]：10的二进制为0x1010，n-1为0x1001。 n&amp;(n-1)：把数字n的二进制中最右边的1变为0，其余不变。[例]：10的二进制为0x1010，10&amp;9==0x1000。 1)计数器加1; 2)消去最右一位1; 3)循环结束后返回计数器值。[例]：10的二进制为0x1010，1)count+=1; 2)n=n&amp;(n-1)==8(0x1000); 循环。 时间复杂度：O(m)，只需消去所有1，m为二进制数所有1的个数；空间复杂度：O(1) class Solution: def hammingWeight(self, n: int) -&gt; int: count = 0 while n: count += 1 n &amp;= n - 1 return count Python函数 bin(n: int)返回数字n的string类型的二进制值。[例]：bin(10)==0x1010。 1)将数字转为二进制; 2)统计1的个数。 时间复杂度：根据底层实现决定；空间复杂度：O(1) class Solution: def hammingWeight(self, n: int) -&gt; int: return bin(n).count('1')","link":"/2020/09/07/剑指Offer-15-二进制中1的个数/"},{"title":"剑指Offer-14-剪绳子","text":"题目 题目描述I给你一根长度为 n 的绳子，请把绳子剪成整数长度的 m 段（m、n都是整数，n&gt;1并且m&gt;1），每段绳子的长度记为 k[0],k[1]...k[m-1] 。请问 k[0]*k[1]*...*k[m-1] 可能的 最大乘积是多少 ？例如，当绳子的长度是8时，我们把它剪成长度分别为2、3、3的三段，此时得到的最大乘积是18。 示例1 输入：2输出：1解释：2 = 1 + 1, 1 × 1 = 1 示例2 输入：10输出：36解释：10 = 3 + 3 + 4, 3 × 3 × 4 = 36 题解 动态规划 一段 长度为i 的绳子，可剪在长度为j的位置，则绳子切分为长度为 j 和 i-j 两端。 状态定义：dp为一维数组，dp[i]为长度为i的绳子的 最大乘积值。 转移方程：dp[i] = max(dp[i], dp[j] * dp[i-j])。其中，根据是否切分得到最大值，即 dp[j]=max(j,dp[j]), dp[i-j]=max(i-j,dp[i-j])。 初始状态：dp[1] = 1。 时间复杂度：O($n^2$)，计算每个长度的最大值需计算n次，每次计算需切分n/2次；空间复杂度：O(n)，一维数组dp记录每个长度的最大乘积值。 class Solution: def cuttingRope(self, n: int) -&gt; int: if n &lt;= 3: return n - 1 dp = [1] * (n+1) for i in range(2, n+1): for j in range(1, i//2 + 1): dp[i] = max(dp[i], max(j, dp[j]) * max(i-j, dp[i-j])) return dp[-1] 贪心算法 将绳子 尽可能切为多个长度为3 的片段，转化为处理最后一段绳子长度。 （引用Krahets） 推论一：将绳子 以相等的长度等分为多段，得到的乘积最大。可由算术几何均值不等式推出。推论二：尽可能将绳子 以长度3等分为多段 时，乘积最大。可求导求极值推出。 最后一段绳子长度为0：刚好全部绳子按3等分，直接返回 $3^{n/3}$ ； 最后一段绳子长度为1：$3 * 1 &lt; 2 * 2$, 故取出将最后一段绳子长度为1和一段长度为3的片段组合为2+2，即返回 $3^{n/3-1} * 2 * 2$； 最后一段绳子长度为2：最后剩一段绳子长度为2，故返回 $3^{n/3} * 2$。 时间复杂度：O(1)，仅有求整、求余、次方运算；空间复杂度：O(1)，仅使用辅助变量占用常数大小空间。 class Solution: def cuttingRope(self, n: int) -&gt; int: if n &lt;= 3: return n - 1 a, b = n // 3, n % 3 if b == 0: return int(math.pow(3, a)) elif b == 1: return int(math.pow(3, a-1) * 4) else: return int(math.pow(3, a) * 2) Python中常见有三种幂计算函数： ** 和 pow() 的时间复杂度均为 O(logn)；而 math.pow() 始终调用 C库的pow()函数，其执行浮点取幂，时间复杂度为 O(1)。 题目描述II答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 即本题考虑 “大数越界情况下的求余问题”。 时间复杂度：O(logn)，**运算；空间复杂度：O(1)，仅使用辅助变量占用常数大小空间。 class Solution: def cuttingRope(self, n: int) -&gt; int: if n &lt;= 3: return n - 1 a, b, p = n // 3, n % 3, 1000000007 if b == 0: return 3 ** a % p if b == 1: return 3 ** (a-1) * 4 % p return 3 ** a * 2 % p 由于Python语言特性，理论上变量取值范围由系统内存大小决定（无限大），因此不用考虑大数越界问题。","link":"/2020/09/04/剑指Offer-14-剪绳子/"},{"title":"剑指Offer-13-机器人的运动范围","text":"题目 题目描述地上有一个m行n列的方格，从坐标 [0,0] 到坐标 [m-1,n-1] 。一个机器人从坐标 [0, 0] 的格子开始移动，它每次可以向左、右、上、下移动一格（不能移动到方格外），也不能进入行坐标和列坐标的数位之和大于k的格子。例如，当k为18时，机器人能够进入方格 [35, 37] ，因为3+5+3+7=18。但它不能进入方格 [35, 38]，因为3+5+3+8=19。请问 该机器人能够到达多少个格子？ 示例1 输入：m = 2, n = 3, k = 1输出：3 示例2 输入：m = 3, n = 1, k = 0输出：0 题解 （引用Krahets） 数位和：数字中个位及十位数值的和（题目给出 1 &lt;= n,m &lt;=100）；数位和增量公式：机器人每次只能移动一格，即x到x+1(x-1)，设x的数位和为 $s_x$，x+1的数位和为 $s_{x+1}$；1) 当 (x+1)%10!=0 : $s_{x+1} = s_x + 1$ ; 2) 当 (x+1)%10==0 : $s_{x+1} = s_x - 8$ (例如，19的数位和为10，20的数位和为2)。可达解结构：根据可达解的结构，机器人可 仅通过向右和向下移动，访问所有可达解。 深度优先搜索DFS 矩阵搜索问题，可使用 DFS+剪枝 解决。 递归参数：i, j 为矩阵行列索引，索引的数位和 si, sj 。 终止条件：1)行列索引越界；2)数位和超过目标值k；3)当前元素已经访问过； return 0。 递推过程：1）标记当前矩阵元素：将索引(i,j)存入集合visited中；2）搜索下一单元格：分别对当前元素的下、右两个方向进行DFS递归搜索； 回溯返回值：返回 1 + 下方可达解 + 右方可达解。 时间复杂度：O(mn)，遍历矩阵中所有单元格；空间复杂度：O(mn)，visited 存储矩阵所有单元格。 class Solution: def movingCount(self, m: int, n: int, k: int) -&gt; int: def dfs(i, j, si, sj): if i &gt;= m or j &gt;= n or k &lt; si+sj or (i,j) in visited: return 0 visited.add((i,j)) return 1 + dfs(i+1, j, si+1 if (i+1)%10 else si-8, sj) + dfs(i, j+1, si, sj+1 if (j+1)%10 else sj-8) visited = set() return dfs(0, 0, 0, 0) 广度优先搜索BFS 用 队列实现BFS，按所有方向向前搜索。 初始化：将 初始点(0,0) 加入队列 queue； 终止条件：queue为空，即所有可达解全部遍历出队； 递推过程：1)单元格出队：队首单元格弹出，作为当前搜索单元格；2)判断是否跳过：a)行列索引越界; b)数位和超过目标值k; c)当前元素已经访问过; 3)标记当前单元格：将索引(i,j)存入集合visited中；4)单元格入队：将当前元素的下、右两个方向的元素加入 queue； 返回值：集合 visited的长度 为可达解数量。 时间复杂度：O(mn)，遍历矩阵中所有单元格；空间复杂度：O(mn)，visited 存储矩阵所有单元格。 class Solution: def movingCount(self, m: int, n: int, k: int) -&gt; int: queue, visited = [(0, 0, 0, 0)], set() while queue: i, j, si, sj = queue.pop(0) if i &gt;= m or j &gt;= n or k &lt; si+sj or (i,j) in visited: continue visited.add((i,j)) queue.append((i+1, j, si+1 if (i+1)%10 else si-8, sj)) queue.append((i, j+1, si, sj+1 if (j+1)%10 else sj-8)) return len(visited)","link":"/2020/09/03/剑指Offer-13-机器人的运动范围/"},{"title":"剑指Offer-12-矩阵中的路径","text":"题目 题目描述请设计一个函数，用来判断在 一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一格开始，每一步可以在矩阵中向左、右、上、下移动一格。如果一条路径经过了矩阵的某一格，那么该路径不能再次进入该格子。例如，在下面的3×4的矩阵中包含一条字符串“bfce”的路径（路径中的字母用加粗标出）。 [[“a”,”b”,”c”,”e”],[“s”,”f”,”c”,”s”],[“a”,”d”,”e”,”e”]] 但矩阵中不包含字符串“abfb”的路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入这个格子。 示例1 输入：board = [[“A”,”B”,”C”,”E”],[“S”,”F”,”C”,”S”],[“A”,”D”,”E”,”E”]], word = “ABCCED”输出：true 示例2 输入：board = [[“a”,”b”],[“c”,”d”]], word = “abcd”输出：false 题解 回溯算法（引用Krahets） 矩阵搜索问题，使用 DFS+剪枝 解决。 深度优先搜索（DFS）：递归遍历矩阵，先朝某个方向 搜索到底 ，再回溯到上个节点，沿另一个方向搜索。剪枝：在搜索过程中遇到 该路径不可能与目标匹配 时，应立即返回，称为可行性剪枝。 递归参数：i,j为矩阵行列索引，k为目标字符在word中的索引。 终止条件：1）return False ; a)行列索引越界；b)当前矩阵元素与目标字符不同；2）return True ; 字符串已经全部匹配，即 k==len(word)-1 。 递推过程：1）标记当前矩阵元素：将 board[i][j]值暂存tmp，并重新赋值为’’，标记其已被访问过；2）搜索下一单元格：分别对当前元素的上下左右四个方向进行DFS递归搜索；3）还原当前矩阵元素：若从矩阵左上角未搜索到word，将循环i,j进行后续搜索，故需还原矩阵元素。 时间复杂度：O($mn3^k$)，遍历矩阵中k字符串所有方案，包含上下左右四个方向，舍弃回头方向，剩下三种选择，时间复杂度为O($3^k$)；矩阵共有mn个初始点，时间复杂度为O(mn)；空间复杂度：O(mn)，递归深度不超过k，最坏情况下k=mn。 class Solution: def exist(self, board: List[List[str]], word: str) -&gt; bool: def dfs(i, j, k): if not 0 &lt;= i &lt; len(board) or not 0 &lt;= j &lt; len(board[i]) or board[i][j] != word[k]: return False if k == len(word) - 1: return True tmp, board[i][j] = board[i][j], '' flag = dfs(i+1, j, k+1) or dfs(i-1, j, k+1) or dfs(i, j+1, k+1) or dfs(i, j-1, k+1) board[i][j] = tmp return flag for i in range(len(board)): for j in range(len(board[i])): if dfs(i, j, 0): return True return False 非递归DFS（引用东流，击败84.5% + 99.8%） DFS用栈维护，BFS用队列维护。 class Solution: def exist(self, board: List[List[str]], word: str) -&gt; bool: # 预处理 w_l = len(word) if w_l &lt; 1: return True rows = len(board) cols = len(board[0]) # 标记数组 board_vis = [[0] * cols for i in range(rows)] # 方向数组 dir_list = [[-1, 0], [0, 1], [1, 0],[0, -1]] # 非递归DFS要用栈维护哦，先把所有头节点放进来，每个节点包括3个值（i,j,l）,i和j是它的坐标，l是它在word中的下标 word_stack = [] for i in range(rows): for j in range(cols): if board[i][j] == word[0]: word_stack.append((i, j, 0)) # 正式开始DFS咯 while len(word_stack) &gt; 0: # 获取头节点信息，先不要弹出 top = word_stack[-1] tx = top[0] ty = top[1] tl = top[2] # 访问这个节点，并开始深度遍历 board_vis[tx][ty] = 1 # 出口条件，如果word遍历完，返回True if tl == w_l - 1: return True # flag记录是否能够继续深度遍历 flag = True for di in dir_list: next_x = tx + di[0] next_y = ty + di[1] # 深度遍历的条件 if next_x &gt;= 0 and next_x &lt; rows and next_y &gt;= 0 and next_y &lt; cols \\ and board_vis[next_x][next_y] == 0 and board[next_x][next_y] == word[tl + 1]: # 注意子节点与父节点的关系 word_stack.append((next_x, next_y, tl + 1)) flag = False # 如果不能继续深度遍历，回溯，这个回溯有点复杂：需要一层一层往上回溯，回溯到有多个子节点的地方，类似于树的深度遍历 if flag: while len(word_stack): top = word_stack[-1] if top[2] != tl: break tl -= 1 # 弹出，并标记取消 word_stack.pop() board_vis[top[0]][top[1]] = 0 return False","link":"/2020/08/31/剑指Offer-12-矩阵中的路径/"},{"title":"剑指Offer-11-旋转数组的最小数字","text":"题目 题目描述把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个递增排序的数组的一个旋转， 输出旋转数组的最小元素 。例如，数组 [3,4,5,1,2] 为 [1,2,3,4,5] 的一个旋转，该数组的最小值为1。 示例1 输入：[3,4,5,1,2]输出：1 示例2 输入：[2,2,2,0,1]输出：0 题解 暴力搜索 依次 遍历 数组，寻找数组最小值。时间复杂度：O(n)空间复杂度：O(1) class Solution: def minArray(self, numbers: List[int]) -&gt; int: return min(numbers) 二分查找(引用Krahets) 每次取数组 中间元素 与左（右）排序数组中 最右侧的元素 比较，最终返回数组最小值。左排序数组：mid左边的数组；右排序数组：mid右边的数组。 当 numbers[mid] &gt; numbers[right] 时：最小元素在右排序数组[mid+1,right]中，故使 left = mid + 1； 当 numbers[mid] &lt; numbers[right] 时：最小元素在左排序数组[left,mid]中，故使 right = mid； 当 numbers[mid] == numbers[right] 时：无法判断最小元素在哪个排序数组中，故使 right = right - 1，缩小判断范围。 分析numbers[mid] == numbers[right]例：[1,0,1,1,1], [1,1,1,0,1]，无法判断最小元素在哪个排序数组中。 right=right-1 验证： 最小元素在 左排序数组 中：numbers[mid] == numbers[right]，则数组[mid,right]中所有元素值相等，right=right-1只会抛弃一个重复值，最小值仍会在[left,mid]中。 最小元素在 右排序数组 中：numbers[mid] == numbers[right]，则数组[left,mid]及[right]元素值相等。1）若最小元素在 [mid+1,right-1] 中，执行right=right-1后，最小值仍在[mid+1,right-1]中；2）若最小元素为 [right] ，[left,mid]与[right]元素值相同，执行right=right-1后，最小值仍在[left,mid]中。 时间复杂度：O($\\log_{2}n$)空间复杂度：O(1) class Solution: def minArray(self, numbers: List[int]) -&gt; int: left, right = 0, len(numbers)-1 while left &lt; right: mid = (left + right) // 2 if numbers[mid] &gt; numbers[right]: left = mid + 1 elif numbers[mid] &lt; numbers[right]: right = mid else: right -= 1 return numbers[left]","link":"/2020/08/30/剑指Offer-11-旋转数组的最小数字/"},{"title":"剑指Offer-10-II.青蛙跳台阶问题","text":"题目 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。 答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 示例1 输入：n = 2输出：2 示例2 输入：n = 7输出：21 示例3 输入：n = 0输出：1 题解 动态规划 可以跳1级和2级，则最终n级台阶可有 两种跳法 到达，即从 n-1 和 n-2 级台阶跳上来。 状态定义：dp为一维数组，dp[i]为第i个台阶时的跳法种数。 转移方程：dp[i] = dp[i-1] + dp[i-2]。 初始状态：dp[0] = 1, dp[1] = 1。 时间复杂度：O(n)，循环n次，每次O(1)。空间复杂度：O(n)，dp[n]占用O(n)。 class Solution: def numWays(self, n: int) -&gt; int: if n &lt;= 1: return 1 dp = [] dp.append(1) dp.append(1) for i in range(2, n+1): dp_tmp = (dp[i-1] + dp[i-2]) % 1000000007 dp.append(dp_tmp) return dp[n] 动态规划（空间优化） 利用 辅助变量使a, b 交替前进，节省了dp[]列表空间。时间复杂度：O(n)，循环n次，每次O(1)。空间复杂度：O(1)，变量占用O(1)。 class Solution: def numWays(self, n: int) -&gt; int: a, b = 1, 1 for _ in range(n): a, b = b, a + b return a % 1000000007","link":"/2020/08/29/剑指Offer-10-II-青蛙跳台阶问题/"},{"title":"剑指Offer-10-I.斐波那契数列","text":"题目 题目描述写一个函数，输入 n ，求斐波那契（Fibonacci）数列的第 n 项。斐波那契数列的定义如下： F(0) = 0, F(1) = 1F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1. 斐波那契数列由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。 示例1 输入：n = 2输出：1 示例2 输入：n = 5输出：5 题解 递归 把f(n) 拆解 为f(n-1)和f(n-2)递归计算，以f(1)和f(0)为终止条件。大量重复递归计算，会直接超时。时间复杂度：O($2^n$)空间复杂度：O(n) class Solution: def fib(self, n: int) -&gt; int: if n &lt;= 0: return n return self.fib(n-1) + self.fib(n-2) 动态规划 状态定义：dp为一维数组，dp[i]为斐波那契数列的第i个值。 转移方程：dp[i] = dp[i-1] + dp[i-2]。 初始状态：dp[0] = 0, dp[1] = 1。 时间复杂度：O(n)，循环n次，每次O(1)。空间复杂度：O(n)，dp[n]占用O(n)。 class Solution: def fib(self, n: int) -&gt; int: if n &lt;= 0: return n dp = [] dp.append(0) dp.append(1) for i in range(2, n+1): dp_tmp = (dp[i-1] + dp[i-2]) % 1000000007 dp.append(dp_tmp) return dp[n] 动态规划（空间优化） 利用 辅助变量使a, b 交替前进，节省了dp[]列表空间。时间复杂度：O(n)，循环n次，每次O(1)。空间复杂度：O(1)，变量占用O(1)。 class Solution: def fib(self, n: int) -&gt; int: a, b = 0, 1 for _ in range(n): a, b = b, a + b return a % 1000000007","link":"/2020/08/27/剑指Offer-10-I-斐波那契数列/"},{"title":"剑指Offer-09-用两个栈实现队列","text":"题目 题目描述 用两个栈实现一个队列。队列的声明如下，请实现它的两个函数 appendTail 和 deleteHead ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，deleteHead 操作返回 -1 ). 示例1 输入：[“CQueue”,”appendTail”,”deleteHead”,”deleteHead”][[],[3],[],[]]输出：[null,null,3,-1] 示例2 输入：[“CQueue”,”deleteHead”,”appendTail”,”appendTail”,”deleteHead”,”deleteHead”][[],[],[5],[2],[],[]]输出：[null,-1,null,null,5,2] 题解 双栈分别正序和逆序 栈：后入先出；队列：先入先出。栈1：正序（后入先出）；栈2：栈1的逆序。实现逆序：循环执行将栈1的元素出栈，并将其入栈至栈2，直到栈1为空。即 stack2.append(stack1.pop()) 。appendTail()：直接将元素 入栈至栈1 。deleteHead()：1. 栈2不为空， return stack2.pop() ；2. 栈1为空， return -1 ；3. 将栈1逆序输出到栈2， return stack2.pop() 。时间复杂度： appendTail() 为O(1)； deleteTail() 为O(n)，先遍历输出逆序。空间复杂度：O(n)，栈1和栈2共保存n个元素。 class CQueue: def __init__(self): self.stack1, self.stack2 = [], [] def appendTail(self, value: int) -&gt; None: self.stack1.append(value) def deleteHead(self) -&gt; int: if self.stack2: return self.stack2.pop() if not self.stack1: return -1 while self.stack1: self.stack2.append(self.stack1.pop()) return self.stack2.pop()# Your CQueue object will be instantiated and called as such:# obj = CQueue()# obj.appendTail(value)# param_2 = obj.deleteHead()","link":"/2020/08/26/剑指Offer-09-用两个栈实现队列/"},{"title":"剑指Offer-07-重建二叉树","text":"题目 题目描述 输入某二叉树的 前序遍历和中序遍历 的结果，请 重建该二叉树 。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 示例 输入： 前序遍历 preorder = [3,9,20,15,7] 中序遍历 inorder = [9,3,15,20,7] 输出：返回如下二叉树 3 / \\ 9 20 / \\ 15 7 题解 原地递归 前序遍历：根节点、左子树、右子树；中序遍历：左子树、根节点、右子树。已知前序遍历和中序遍历重建二叉树： 递推：前序遍历的第一个节点为 根节点 ，找到其在中序遍历中的对应位置，在根节点左边为 左子树 ，在根节点右边为 右子树 。将得到的左右子树作为新序列， 递归 划分其左右子树。 终止：前序或中序为空，即到达叶子节点，return None。 时间复杂度：O($n^2$)，以根节点遍历序列，每次递归左右子树。空间复杂度：O(n)，递归使用O(n)空间。 # Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: if not preorder or not inorder: return None loc = inorder.index(preorder[0]) root = TreeNode(preorder[0]) root.left = self.buildTree(preorder[1 : loc+1], inorder[ : loc]) root.right = self.buildTree(preorder[loc+1 : ], inorder[loc+1 : ]) return root 哈希表递归(引用Krahets) 使用哈希表预存储 中序遍历的值与索引 的映射关系，每次搜索的时间复杂度为O(1)。方法参数： pre_root ：前序根节点索引； in_left ：中序左边界； in_right ：中序右边界。子树根节点索引：左子树根节点为 前序根节点+1 ；右子树根节点为 前序根节点+1+左子树长度(中序根节点索引-左边界) 。时间复杂度：O(n)，初始化HashMap遍历inorder，占用O(n)；每层递归中的节点建立、搜索操作占用O(1)。空间复杂度：O(n)，HashMap使用O(n)额外空间；递归操作中系统需使用O(n)额外空间。 class Solution: def buildTree(self, preorder: List[int], inorder: List[int]) -&gt; TreeNode: self.dic, self.preorder = {}, preorder for i in range(len(inorder)): self.dic[inorder[i]] = i return self.recur(0, 0, len(inorder)-1) def recur(self, pre_root, in_left, in_right): if in_left &gt; in_right: return root = TreeNode(self.preorder[pre_root]) loc = self.dic[self.preorder[pre_root]] root.left = self.recur(pre_root+1, in_left, loc-1) root.right = self.recur(pre_root+1+loc-in_left, loc+1, in_right) return root","link":"/2020/08/25/剑指Offer-07-重建二叉树/"},{"title":"剑指Offer-06-从尾到头打印链表","text":"题目 题目描述 输入一个链表的头节点， 从尾到头反过来返回每个节点的值 （用数组返回）。 示例 输入：head = [1,3,2]输出：[2,3,1] 题解 递归解法 先走到链表末端，回溯时依次将节点元素加入列表。递推：每次传入 head.next ，以 head == None 为终止条件，此时返回空列表 [] 。回溯：每次返回 当前list + 当前节点元素[head.val] 。时间复杂度：O(n)，遍历链表，递归n次。空间复杂度：O(n)，递归使用O(n)空间。 # Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reversePrint(self, head: ListNode) -&gt; List[int]: if head is None: return [] return self.reversePrint(head.next) + [head.val] 辅助栈 使用Python列表中的reverse方法： list.reverse() 或 list[::-1] ，反向列表中元素。遍历链表，依次将节点元素 push入栈 （使用append方法实现）；然后将节点元素 pop出栈 （反向）。时间复杂度：O(n)空间复杂度：O(n) class Solution: def reversePrint(self, head: ListNode) -&gt; List[int]: stack = [] while head: stack.append(head.val) head = head.next return stack[::-1]","link":"/2020/08/24/剑指Offer-06-从尾到头打印链表/"},{"title":"剑指Offer-05-替换空格","text":"题目 题目描述 请实现一个函数，把 字符串 s 中的每个空格替换成&quot;%20&quot; 。 示例 输入：s = “We are happy.”输出：”We%20are%20happy.” 题解 新建字符串 在Python语言中， 字符串被设计成不可变类型 ，即无法直接修改字符串的某一位字符，需要 新建一个字符串 实现。初始化一个list； 遍历字符串s中每个字符c ，若c为空格，则在list中添加”%20”；若c不为空格，则在list中添加字符c。时间复杂度：O(n)，遍历O(n)，每轮添加O(1)。空间复杂度：O(n)，新建list使用线性大小空间。 class Solution: def replaceSpace(self, s: str) -&gt; str: res = [] for c in s: if c == ' ': res.append(\"%20\") else: res.append(c) return \"\".join(res) Python特性 使用Python字符串中的replace方法： str.replace(old, new[, max]) ，将字符串中的 str1 替换成 str2,如果 max 指定，则替换不超过 max 次。时间复杂度：O(n)空间复杂度：O(n) class Solution: def replaceSpace(self, s: str) -&gt; str: return s.replace(\" \", \"%20\");","link":"/2020/08/24/剑指Offer-05-替换空格/"},{"title":"剑指Offer-04-二维数组中的查找","text":"题目 题目描述 在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 示例 现有矩阵 matrix 如下： [ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30]] 给定 target = 5，返回 true。给定 target = 20，返回 false。 题解 暴力解法 按行依次搜索target，如果当前元素大于target，则进行下一行搜索；若当前元素等于target，则return True；未找到return False。时间复杂度：O(nm)，最好情况O(1)空间复杂度：O(1) class Solution: def findNumberIn2DArray(self, matrix: List[List[int]], target: int) -&gt; bool: for i in range(len(matrix)): for j in range(len(matrix[i])): if matrix[i][j] == target: return True elif matrix[i][j] &gt; target: continue return False 二分查找 按行依次搜索target，每次从中间元素进行匹配，如果该行未找到，则进入下一行搜索；若找到则return True；未找到return False。时间复杂度：O(nlogm)空间复杂度：O(1) class Solution: def findNumberIn2DArray(self, matrix: List[List[int]], target: int) -&gt; bool: for i in range(len(matrix)): left, right = 0, len(matrix[i])-1 while left &lt;= right: mid = (left+right) // 2 if target &lt; matrix[i][mid]: right = mid - 1 elif target &gt; matrix[i][mid]: left = mid + 1 else: return True return False 二叉搜索树 从右上角开始搜索，将矩阵转化为二叉搜索树,如上图所示。右上角元素作为根节点，如果target小于当前节点，则向左下方搜索，即向左边列搜索；如果target大于当前节点，则向右下方搜索，即向下一行搜索；否则为找到target，return True。若行索引或列索引越界，即矩阵中无target值，return False。每轮迭代相当于将矩阵删除一行（列），得到新矩阵进行迭代。时间复杂度：O(n+m)空间复杂度：O(1) class Solution: def findNumberIn2DArray(self, matrix: List[List[int]], target: int) -&gt; bool: if not matrix: return False i, j = 0, len(matrix[0])-1 while i &lt;= len(matrix)-1 and j &gt;= 0: if target &lt; matrix[i][j]: j -= 1 elif target &gt; matrix[i][j]: i += 1 else: return True return False","link":"/2020/08/21/剑指Offer-04-二维数组中的查找/"},{"title":"剑指Offer-03-数组中重复的数字","text":"题目 题目描述 找出 数组中重复的数字 。在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。 示例 输入：[2, 3, 1, 0, 2, 5, 3].输出：2 或 3 题解 排序后查看相邻元素 先 排序，然后查看相邻元素是否相同 ；若相同则元素重复，return。该方法运行时间慢。时间复杂度：O(nlogn)空间复杂度：O(1) class Solution: def findRepeatNumber(self, nums: List[int]) -&gt; int: nums.sort() for i in range(len(nums)): if nums[i] == nums[i+1]: return nums[i] 辅助空间 开辟一个新的列表或字典， 将nums数值作为新列表的索引并赋值为1 ；若该索引已经复制则元素重复，return。时间复杂度：O(n)空间复杂度：O(n) class Solution: def findRepeatNumber(self, nums: List[int]) -&gt; int: repeatDict = {} for num in nums: if num not in repeatDict: repeatDict[num] = 1 else: return num 原地交换 与辅助空间思路类似， 让索引i位置存放数值i 。如果索引位置i的元素不是i，将把i元素放在对应位置，即nums[nums[i]]与nums[i]交换。若交换时发现索引nums[i]的元素与索引相同，则元素重复，return。时间复杂度：O(n)空间复杂度：O(1) class Solution: def findRepeatNumber(self, nums: List[int]) -&gt; int: for i in range(len(nums)): while i != nums[i]: if nums[i] == nums[nums[i]]: return nums[i] nums[nums[i]], nums[i] = nums[i], nums[nums[i]]","link":"/2020/08/21/剑指Offer-03-数组中重复的数字/"},{"title":"Mask R-CNN","text":"Mask R-CNN[1] is a framework for object instance segmentation, which adds a branch for predicting an object mask in parallel with the existing branch for bounding box recognition of Faster R-CNN. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Mask R-CNN(ICCV 2017 paper)Code: PytorchNote: Mendeley Paper Abstract It is a framework for object instance segmentation. It extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. It challenges instance segmentation, bounding-box object detection, and person keypoint detection. It serves as a solid baseline in instance-level recognition. Problem Description Problem Solution Conceptual Understanding Core ConceptionLoss Mask RoIAlign Experiments Code The complete code can be found in detectron2[2]. Note More details of Mask R-CNN and its extends like RoIAlign, bilinear interpolation and etc. can be found in [3]. References [1] He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969.[2] detectron2. https://github.com/facebookresearch/detectron2[3] stone. “The amazing Mask R-CNN.” https://zhuanlan.zhihu.com/p/37998710","link":"/2020/08/17/Mask-R-CNN/"},{"title":"LTM","text":"LTM(Local Transformation Module)[1] focus on the relationship of the local features. It uses linear transformation of the relationship matrix in a high-dimensional metric embedding space to accomplish the transformation. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: A New Local Transformation Module for Few-Shot Segmentation(ICMM 2020 paper)Code: [Code]Note: Mendeley Paper Abstract The key step of few-shot segmentation is to establish the transformation module. The existing methods form transformation model based on global cues, which however ignores the local cues. This paper proposes a new transformation module based on local cues,relationship matrix with cosine distance to enhance the generalization, generalized inverse matrix to handle the challenging mapping problem. It outperforms the state-of-the-art method on the PASCAL VOC 2012 dataset. Problem Description Problem Solution Conceptual Understanding Core Conception Transformer $F^\\prime_{s}(i,j)=F_{s}(i,j) \\times {G_{s}(i,j)}$$\\hat{F^\\prime_{q}}(i,j)=\\hat{F_{q}}(i,j) \\times {A(i,j)}$$R_{ij}=\\frac{\\langle E_{si},E_{qj}\\rangle}{||E_{si}|| ||E_{qj}||}$$R_{truth}=G_{q} \\cdot G_{s}$$R=A \\cdot G_{s}$$A=R \\cdot [\\left( G_{s} \\right)^T \\left( G_{s} \\left( G_{s} \\right)^T \\right)^{-1}]$$\\hat{A}=\\frac{A-\\min{(A)}}{\\max{(A)}-\\min{(A)}}$ Loss function $L_{m}=\\sum_{i}\\sum_{j}-(Y(i,j)\\log(M(i,j))+(1-Y(i,j))\\log(1-M(i,j)))$$L_{a}=\\sum_{i}\\sum_{j}-(Y(i,j)\\log(M_{a}(i,j))+(1-Y(i,j))\\log(1-M_{a}(i,j)))$$L_{r}=||R-R_{truth}||_2^2$ $L=\\lambda_{m}L_{m}+\\lambda_{a}L_{a}+\\lambda{r}L_{r}$ Experiments Code [Updating] Note [Updating] References [1] Yang Y, Meng F, Li H, et al. A new local transformation module for few-shot segmentation[C]//International Conference on Multimedia Modeling. Springer, Cham, 2020: 76-87.","link":"/2020/07/29/LTM/"},{"title":"PGNet","text":"PGNet(Pyramid Graph Networks)[1] modeled structured segmentation data with graphs and further proposed a pyramid-like structure that models different sizes of image regions as graph nodes. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Pyramid Graph Networks with Connection Attentions for Region-Based One-Shot Semantic Segmentation(ICCV 2019 paper)Code: [Code]Note: Mendeley Paper Abstract One-shot image segmentation yields a many-to-many message passing problem with only one training image available. Previous methods described as one-to-many problem by squeezing support data to a global descriptor. In this work, they model structured segmentation data with graphs and apply attentive graph reasoning, graph attention mechanism could establish the element-to-element correspondence, pyramid-like structure is able to capture correspondence at different semantic levels. It leads to new state-of-the-art performance on 1-shot and 5-shot segmentation benchmarks of the PASCAL VOC 2012 dataset. Problem Description Problem Solution Conceptual Understanding Core Conception Experiments Code [Updating] Note [Updating] References [1] Zhang C, Lin G, Liu F, et al. Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 9587-9595.","link":"/2020/07/28/PGNet/"},{"title":"CRNet","text":"CRNet(Cross-Reference Networks)[1] make predictions for both the support image and the query image. It can better find the co-occurrent objects in the two images, thus helping the few-shot segmentation task. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: CRNet: Cross-Reference Networks for Few-Shot Segmentation(CVPR 2020 paper)Code: [Code]Note: Mendeley Paper Abstract Image segmentation algorithms are based on deep convolutional neural networks in recent years. Few-shot segmentation aims to learn a segmentation model that can be gener- alized to novel classes with only a few training images. In this work, they propose a cross-reference network (CRNet) including cross-reference module for finding the co-occurrent objects and mask refinement module for refining predictions. It achieves state-of-the-art performance on the PASCAL VOC 2012 dataset. Problem Description Problem Solution Conceptual Understanding Core Conception Experiments Code [Updating] Note [Updating] References [1] Liu W, Zhang C, Lin G, et al. CRNet: Cross-Reference Networks for Few-Shot Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 4165-4173.","link":"/2020/07/10/CRNet/"},{"title":"FSL-Survey-2019","text":"FSL-Survey[1] is a survey on Few-Shot Learning(FSL), which cotains 166 paper to review Few-Shot Learning. They categorize FSL methods from three perspectives: data, model and algorithm. There are some details of reading it. Contents Paper &amp; note Paper understanding Note References Paper &amp; note Paper: Generalizing from a Few Examples: A Survey on Few-Shot Learning(CSUR 2019 paper)Note: Mendeley Paper Abstract Starting from a formal definition of FSL, then point out that the core issue in FSL. Data: which uses prior knowledge to augment the supervised experience. model: which uses prior knowledge to reduce the size of the hypothesis space. algorithm: which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. Promising directions are also proposed to provide insights for future research. Definition Taxonomy Data Model Multitask Learning Embedding Learning Learning with External Memory Generative Modeling Algorithm Refining Existing Parameters Refining Meta-Learned Parameter Learning the Optimizer Meta-learning Note Offical Online link can be found in FewShotPapers[2]. References [1] Wang Y, Yao Q, Kwok J T, et al. Generalizing from a few examples: A survey on few-shot learning[J]. ACM Computing Surveys (CSUR), 2019.[2] FewShotPapers. https://github.com/tata1661/FewShotPapers","link":"/2020/07/07/FSL-Survey-2019/"},{"title":"PV-RCNN","text":"PV-RCNN[1] is a 3D Object Detection framework to integrate 3D voxel CNN and PointNet-based set abstraction to learn more discriminative point cloud features. The most contributions in this papar is two-stage strategy including the voxel-to-keypoint 3D scene encoding and the keypoint-to-grid RoI feature abstraction. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection(CVPR 2020 paper)Code: PyTorchNote: Mendeley Paper Abstract They present PointVoxel-RCNN(PV-RCNN) for accurate 3D object detection from point clouds. It summarizes the 3D scene with a 3D voxel CNN into a small set of keypoints via a novel voxel set abstraction(VSA) module. RoI-grid pooling is proposed to abstract proposal-specific features from the keypoints to the RoI-grid points, the RoI-grid feature points encode much richer context information. It surpasses state-of-the-art 3D detection. Problem Description The grid-based methods generally transform the irregular point clouds to regular representations such as 3D voxels, they are more computationally efficient. The point-based methods directly extract discriminative features from raw point clouds for 3D detection, they could achieve larger receptive field. Problem Solution They integrated these two types. The voxel-based operation efficiently encodes multi-scale feature representations, PointNet-based set abstraction operation preserves accurate location information with flexible receptive field. The voxel CNN with 3D sparse convolution is adopted for voxel-wise feature learning and accurate proposal generation. A small set of keypoints are selected by the furtherest point sampling (FPS) to summarize the overall 3D information from the voxel-wise features. PointNet-based set abstraction for summarizing multi-scale point cloud information. Conceptual Understanding 3D Sparse Convolution: Input the raw point clouds to learn multi-scale semantic features and generate 3D object proposals. Voxel Set Abstraction: the learned voxel-wise feature volumes at multiple neural layers are summarized into a small set of key points. RoI-grid Pooling: the keypoint features are aggregated to the RoI-grid points. Core ConceptionPredicted Keypoint Weighting RoI-grid Pooling Experiments Code The complete code can be found in PV-RCNN[2]. Another implementation can be found in vision3d[3]. [Updating] Note Provide more accurate detections by point cloud features. Integrate it to multiple object tracking framework. References [1] Shi S, Guo C, Jiang L, et al. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10529-10538.[2] PV-RCNN. https://github.com/sshaoshuai/PV-RCNN[3] vision3d. https://github.com/jhultman/vision3d","link":"/2020/06/23/PV-RCNN/"},{"title":"DeepSORT","text":"DeepSORT[1] integrates appearance information to improve the performance of SORT, learned a deep association metric. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Simple Online and Realtime Tracking with a Deep Association Metric(ICIP 2017 paper)Code: PyTorch, TensorFlowNote: Mendeley Paper Abstract SORT[2] is a pragmatic approach to multiple object tracking. In this paper, appearance information was integrated to improve the performance of SORT for tackling the long-term occlusions. They place offline pre-traning with a learned deep association metirc on person re-id dataset, while establish measurement-to-track associations using nearest neighbor queries during online application. It reduces the number of identity switches by 45%. Problem Description Traditional methods are not applicable in online scenarios and the performance of these methods comes at increased computational and implementation complexity. SORT returns a relatively high number of identity switches. Problem Solution They overcome this issue by replacing the association metric with a more informed metric that combines motion and appearance information. Core ConceptionState Estimation Assignment Problem Motion information: Mahalanobis distance $d^{(1)}(i,j)=(d_{j}-y_{i})^{T}S_{i}^{-1}(d_{j}-y_{i})$. Appearance information: smallest cosine distance $d^{(2)}(i,j)=min \\lbrace 1-r_{j}^{T}r_{k}^{(i)} |r_{k}^{(i)}\\in{R_{i}} \\rbrace$. In combination for occlusions: weighted sum $c_{i,j}=\\lambda d^{(1)}(i,j)+(1-\\lambda)d^{(2)}(i,j)$. Matching Cascade Deep Appearance Descriptor Experiments Code The complete code can be found in deep_sort[3]. Another tensorflow implementation can be found in deep_sort_yolov3[4]. [Updating] Note More details about the whole algorithm and its implementation can be found in [5]. References [1] Wojke N, Bewley A, Paulus D. Simple online and realtime tracking with a deep association metric[C]//2017 IEEE international conference on image processing (ICIP). IEEE, 2017: 3645-3649.[2] Gojay. “SORT.” https://gojay.top/2020/06/14/SORT/[3] deep_sort. https://github.com/nwojke/deep_sort[4] deep_sort_yolov3. https://github.com/Qidian213/deep_sort_yolov3[5] pprp. “Anasis for Deep SORT.” https://zhuanlan.zhihu.com/p/133678626","link":"/2020/06/20/DeepSORT/"},{"title":"SORT","text":"SORT[1] is pragmatic approach for online and realtime applications. It achieves SOTA with using Kalman filter and Hungarian algorithm. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Simple Online and Realtime Tracking(ICIP 2016 paper)Code: PyTorchNote: Mendeley Paper Abstract THis paper explores a pragmatic approach to multiple object tracking where the main focus is to associate objects. To this end, detection quality is identified as a key factor influencing tracking performance. It only use the Kalman filter and Hungarian algorithm for the tracking components. It achieves an accuracy comparable to state-of-the-art online trackers. Problem Description Traditionally methods delay making difficult decisions while there is high uncertainty over the object assignments. Recent developments still delay the decision making which makes them unsuitable for online tracking. Problem Solution Occam’s Razor: Only the bounding box position and size are used for both motion estimation and data association. Detection: CNN based, like Faster R-CNN[2]. Motion estimation: Kalman filter[3]. Data association: Hungarian algorithm[4]. Core ConceptionDetection To this end, detection quality is identified as a key factor influencing tracking performance, where changing the detector can improve tracking by up to 18.9%. Estimation Model It used to propagate a target’s identity into the next frame. It uses Kalman filter with a linear constant velocity model. The state of each target is modelled as: $x=[u,v,s,r,\\dot{u},\\dot{v},\\dot{s}]^T$. Data Association It used to assign detections to existing targets. The assignment cost matrix is then computed as the intersection-over-union (IOU) distance. The assignment is solved optimally using the Hungarian algorithm. Creation and Deletion of Track Identities Experiments Code The complete code can be found in SORT[5]. [Updating] Note Allowing for new methods to focus on object re-identification to handle long term occlusion. Future work will investigate a tightly coupled detection and tracking framework. References [1] Bewley, Alex, et al. “Simple online and realtime tracking.” 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016.[2] Gojay. “Faster R-CNN.” https://gojay.top/2019/10/19/Faster-R-CNN/[3] Bzarg, Bot. “How a Kalman filter works in pictures.” https://zhuanlan.zhihu.com/p/39912633[4] ZihaoZhao. “Hungarian algorithm and Kuhn-Munkres algorithm.” https://zhuanlan.zhihu.com/p/62981901[5] SORT. https://github.com/abewley/sort","link":"/2020/06/14/SORT/"},{"title":"FairMOT","text":"FairMOT[1] is a one-shot tracker to fuse object detection and re-identification in a single network. The most contributions in this papar are anchor-free Re-ID feture extraction, multi-layer feature aggregation and lower-dimensional re-ID fetures. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: A Simple Baseline for Multi-Object Tracking(arXiv 2020 paper)Code: PytorchNote: Mendeley Paper Abstract There has been remarkable progress on multi-object tracking with object detection and re-identification. Little attention has been focused on accomplishing the two tasks in a single network. In this work, they study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problem. It outperforms the state-of-the-art on the public datasets. Problem Description Two steps: First the detection model localizes the bounding boxes of objects, then the association model extracts Re-ID features and links it to tracks. However, those methods cannot perform inference at video rate because the two networks do not share features. One-shot: Those methods jointly detect objects and learn Re-ID features. However, the accuracy and ID switches get worse a lot. Problem Solution Anchor-Free: the anchor-based methods usually operate on a coarse grid. So there is a high chance that the features extracted at the anchor are not aligned with the object center. Multi-Layer Feature Aggregation: it helps reduce identity switches by aggregating low-level and high-level features. Lower-dimensional features: It helps reduce the risk of over-fitting to small data, and improves the tracking robustness. Conceptual Understanding Multi-Layer Feature Aggregation: It follows Deep Layer Aggregation (DLA) to fuse features from multiple layers in order to deal with objects of different scales. Anchor-free object detection: It estimates the object centers on high-resolution feature map. pixel-wise Re-identification: It learn low-dimensional Re-ID features to reduce the computation time and improve the robustness. Core ConceptionObject Dection Branch Heatmap Head: This head is responsible for estimating the locations of the object centers. Center Offset Head: This head is responsible for localizing the objects more precisely. Box Size Head: This head is responsible for estimating the height and width of the target bounding box at each anchor location. Identify Embedding Branch The goal of the identity embedding branch is to generate features that can distinguish different objects. The resulting featuresis $E\\in{R^{128\\times{W}\\times{H}}}$, the distance between different objects should be larger. Loss Functions Heatmap Loss: The loss function is defined as pixel-wise logistic regression with focal loss. Offset and Size Loss: They we enforce l1 losses for the two heads. Identity Enbedding Loss: They treat object identity embedding as a classification task, then compute the softmax loss. Online Tracking Experiments Code The complete code can be found in here with citing FairMOT[2].[Updating] Note This method achieves the SOTA under the private detector on MOT Challenge, but it still exists in experiments. It mostly improved detectional performance, when using it in actual enviroments, the IDS increase a lot than previous methods. Considering how to improve the IDS is important in real world, maybe we can improve the association module based on depth information. References [1] Zhan Y, Wang C, Wang X, et al. A Simple Baseline for Multi-Object Tracking[J]. arXiv preprint arXiv:2004.01888, 2020.[2] FairMOT. https://github.com/ifzhang/FairMOT","link":"/2020/05/25/FairMOT/"},{"title":"TSDM","text":"TSDM[1] is a RGB-D tracker which use depth information to pretreatment and fuse information to pro-processing. It is composed of a Mask-generator(M-g), SiamRPN++ and a Depth-refiner(D-r). There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator(arXiv 2020 paper)Code: PyTorchNote: Mendeley Paper Abstract Depth information provides informative cues for foreground-background separation and target bounding box regression. Few trackers have used depth information to play the important role aforementioned due to the lack of a suitable model. In this paper, a RGB-D tracker named TSDM is proposed, The M-g generates the background masks, and updates them as the target 3D position changes. The D-r optimizes the target bounding box estimated by SiamRPN++, based on the spatial depth distribution difference between the target and the surrounding background. It outperforms the state-of-the-art on the PTB and VOT. Problem Description The main obstacle is that the tracker requires constant information (such as color), but the target depth distribution may change a lot when the target moves. Problem Solution Depth mudules: M-g and D-r can overcome the obstacle above and make use of depth information effectively. Data augmentation: it helps retrain SiamRPN++ to work better with the M-g module. Conceptual Understanding Mask-generator: Input $X_d$ and $\\overline{Dt_{i-1}}$ into M-g to get $M$ and $M_c$, then use $F_m(\\cdot)$ to get $X_m$. SiamRPN++: Input $Z$ and $X_m$ into the core, then outputs the target bounding box $B_s$ ($W,H,C_x,C_y$). Depth-refiner: Cut out $R_c$ and $R_d$ from $X_c$ and $X_d$ by $B_s$ respectively. Then input $R_c$ and $R_d$ into D-r to get the refined target bounding box $B_d$ ($w,h,xr,yb$). Core ConceptionMask-generator M-g generates two background mask images, $M$ is a 2-value image for clearing out the background of $X_c$, and $M_c$ is a color image for coloring the background of $X_c$. $M_c$ color selection: $M_c$ enhances the target background difference to make the target template matching easier. M-g stop-restart strategy: M-g should automatically stop to avoid masking the real target when a transient tracking drift happens. M-g simulated data augmentation: it used to generate enough training samples ($X_m$) to retrain the SiamRPN++. SiamRPN++ It takes an image pair ($Z,X$) as input and outputs the target bounding box in the current frame, as: $f(Z,X)=\\phi(Z)\\ast\\phi(X)$. More details of SiamRPN++ can be found in previous blog [SiamRPN++][2]. Depth-refiner The bounding box estimated by the core contains the whole target, D-r improve the tracker performance just by cutting out no-target area. Information Fusion Network: It uses depth information to optmize the target state, and color information to overcomes the slight color-depth mismatch. The full architecture is as follows: Experiments Code The complete code can be found in here with citing TSDM[3].[Updating] Note How to use depth information on MOT tasks, detection or re-ID. References [1] ZHAO, Pengyao, et al. TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator. arXiv preprint arXiv:2005.04063, 2020.[2] Gojay. “SiamRPN++.” https://gojay.top/2020/05/09/SiamRPN++/[3] TSDM. https://github.com/lql-team/TSDM","link":"/2020/05/23/TSDM/"},{"title":"Image Transformer","text":"Image Transformer[1] is a sequence modeling formulation of image generation generalized by Transformer, which restricting the self-attention mechanism to attend to local neighborhoods, while maintaining large receptive field. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Image Transformer(2018 arXiv paper)Code: [Code]Note: Mendeley Paper Abstract Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. In this work, they generalize the Transformer to a sequence modeling formulation of image generation. By restricting the self-attention mechanism to attend to local neighborhoods while maintaining large receptive field. outperform the current state of the art in image generation and super-resolution. Problem Description Training RNNs(recurrent neural networks) to sequentially predict each pixel of even a small image is computationally very challenging. Thus, parallelizable models that use CNNs(convolutional neural networks) such as the PixelCNN have recently received much more attention, and have now surpassed the PixelRNN in quality. One disadvantage of CNNs compared to RNNs is their typically fairly limited receptive field. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Problem Solution self-attention can achieve a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN and its various extensions.We. Image Transformer which is a model based entirely on a self-attention mechanism allows us to use significantly larger receptive fields than the PixelCNN. Increasing the size of the receptive field plays a significant role in experiments improvement. Conceptual UnderstandingSelf-Attention Each self-attention layer computes a d-dimensional representation for each position. it first compares the position’s current representation to other positions’ representations, obtaining an attention distribution over the other positions. This distribution is then used to weight the contribution of the other positions’ representations to the next representation for the position. Local Self-Attention Inspired by CNNs, they address this by adopting a notion of locality, restricting the positions in the memory matrix M to a local neighborhood around the query position. They partition the image into query blocks and associate each of these with a larger memory block. The model attends to the same memory matrix, the self-attention is then computed for all query blocks in parallel. Core Conception Recomputing the representation $q’$ of a single channel of one pixel $q$ by attending to a memory of previously generated pixels $m_1,m_2,…$. After performing local self-attention we apply a two-layer position- wise feed-forward neural network with the same parameters for all positions in a given layer. Self-attention and the feed-forward networks are followed by dropout and bypassed by a residual connection with subsequent layer normalization. Experiments Code [Updating] Note We further hope to have provided additional evidence that even in the light of GANs(generative adversarial networks), likelihood-based models of images is very much a promising area for further research. We would like to explore a broader variety of conditioning information including free-form text, and tasks combining modalities such as language-driven editing of images. Fundamentally, we aim to move beyond still images to video and towards applications in model-based reinforcement learning. References [1] Parmar, Niki, et al. “Image transformer.” arXiv preprint arXiv:1802.05751 (2018).","link":"/2020/05/15/Image-Transformer/"},{"title":"SiamRPN++","text":"SiamRPN++[1] is a novel Siamese network based tracker to adopt deep networks that broke strict translation invariance. It performs layer-wise and depth-wise aggregations to successfully trained a ResNet-driven Siamese tracker. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks(CVPR 2019 paper)Code: PyTorchNote: Mendeley Paper Abstract Siamese trackers formulate tracking as convolutional feature cross-correlation that still have an accuracy gap to take advantage of features from deep networks. This paper proved the core reason comes from the lack ofstrict translation invariance, and break this restriction through a simple yet effective spatial aware sampling strategy. They further proposed a new model architecture to perform layer-wise and depth- wise aggregations. It obtains currently the best results on five large tracking benchmarks. Problem Description Padding in deep networks will destroy the strict translation invariance. RPN requires asymmetrical features for classification and regression. Problem Solution Sampling strategy: break the spatial invariance restriction. Layer-wise feature aggregation: predict the similarity map from features learned at multiple levels. Depth-wise separable correlation: produce multiple similarity maps associated with different semantic meanings to reduces the parameter number. Conceptual Understanding Hypothesis: the violation of strict translation invariance will lead to a spatial bias. Experiments: targets are placed in the center with different shift ranges in sepreate training experiments. Results: a strong center bias is learned, increasing shift ranges could learn more area to alleviate it. Core Conception Layer-wise Aggregation They explore multi-level features both low level and semantic information that extracted from the last three residual block, refering these outputs as $F_3(z)$, $F_4(z)$, and $F_5(z)$. The output sizes of the three RPN modules have the same spatial resolution, weighted sum is adopted directly on the RPN output. Depth-wise Cross Correlation A conv-bn block is adopted to make two feature maps with the same number of channels do the correlation operation. Another conv-bn-relu block is appended to fuse different channel outputs. Furthermore, an interesting phenomena is that the objects in the same category have high response on same channels, while responses of the rest channels are suppressed. It can be comprehended as each channel represents some semantic information. Experiments Code The complete code can be found in [pysot][2]. Note More details of SiamRPN++ and the like can be found in [3]. References [1] LI, Bo, et al. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. p. 4282-4291.[2] pysot. https://github.com/STVIR/pysot.[3] Erer Huang. “Overview of Siamese Network Methods.” https://zhuanlan.zhihu.com/p/66757733.","link":"/2020/05/09/SiamRPN++/"},{"title":"Tracklet","text":"Tracklet[1] is a novel method for optimizing tracklet consistency, which directly takes the prediction errors into account. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Multi-object Tracking via End-to-end Tracklet Searching and Ranking(arXiv 2020 paper)Code: [Pytorch][Updating]Note: Tracklet Paper Abstract Recent work use sequence model to calculate the similarity score between the detections and the previous tracklets, but the forced exposure to ground-truth in the training stage leads to the training-inference discrepancy problem. This paper directly takes the prediction errors into account to optimize tracklet consistency. It havs achieved state-of-the-art in MOT15-17 challenge benchmarks using public detection and online settings. Problem Description pairwise-detection matching based on affinity model: It has limited capability to associate long-term consistent trajectories. affinity model on sequence model: tracklet representative feature for matching can somewhat be ill-posed and ideal assumption brings up a potential vulnerability. Problem Solution They propose a global score to measure the inner appearance consistency of tracklet. It optimizes the whole tracklet with a margin loss. a novel algorithm has been established to simulate the prediction data distribution on training by introducing realistic discombobulated candidates to model. Conceptual Understanding Tracklet-level based tracking: It constructs an affinity model on the tracklet level and then uses it to associate the tracklet with detection or connect short tracklets. Pair-wise association methods: They establish an affinity model on the isolated detections, and then generate tracking results from the bottom up.The common concern of these two types of methods is to guarantee the consistency of the entire associated trajectories. Core Conception Training procedure: It follows a “searching-learning-ranking-pruning” pipeline. Scoring Network: The appearance feature of each detection are extracted with CNN(ResNet-50), and the appearance embedding of tracklet are obtained through encoder(LSTM). It trained by online hypothesis tracklet searching with margin loss and rank loss, details as follow. Experiments They report the quantitative results on the three datasets in MOT Challenge Benchmark. Code Note More details of Tracklet optimization and the like can be found in [2]. References [1] Hu, T., Huang, L., &amp; Shen, H. (2020). Multi-object Tracking via End-to-end Tracklet Searching and Ranking. arXiv preprint arXiv:2003.02795.[2] Change_ZH. “Tracklet: MOT Scoring Network.” https://blog.csdn.net/qq_36449741/article/details/104815321?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task.","link":"/2020/03/26/Tracklet/"},{"title":"FFT(Flow-Fuse Tracker)","text":"FFT(Flow-Fuse Tracker)[1] is an end-to-end DNN tracking approach, that jointly learns both target motions and associations for MOT(multiple object tracking). There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Multiple Object Tracking by Flowing and Fusing(arXiv 2020 paper)Code: [Pytorch][Updating]Note: FFT Paper Abstract Previous: estimating target-wise motions and conducting pair-wise Re-Identification(Re-ID). This paper: target flowing and target fusing. Achievment: SOTA on 2DMOT15, MOT16 and MOT17. Problem Description Reccent approaches: First produce target motion and appearance features respectively, then conduct target association between frames.It is very difficult for computating both features. Problem Solution It includes two techniques: target flowing and target fusing.For target flowing: FlowTracker extract target-wise motions from pixel-level optical flows.For target fusing: FuseTracker refines and fuse targets. Conceptual Understanding Experiments Code Algorithm Note References [1] Zhang, Jimuyang, et al. “Multiple Object Tracking by Flowing and Fusing.” arXiv preprint arXiv:2001.11180 (2020).","link":"/2020/03/05/FFT-Flow-Fuse-Tracker/"},{"title":"JRMOT","text":"JRMOT[1] is a novel 3D MOT system that integrates information from 2D RGB images and 3D point clouds into a real-time performing framework. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset(arXiv 2020 paper)Code: [Pytorch][Updating]Note: JRMOT Paper Abstract JRMOT is a novel 3D MOT system that integrates information from 2D RGB images and 3D point clouds into a real-time performing framework. They also released the JRDB dataset, which is a novel large scale 2D+3D dataset. It demonstrates state-of-the-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as a competitive 3D tracking baseline for our dataset and benchmark. Problem Description MOT: the agent needs to perceive the motion of the multiple dynamic objects and other agents.Reccent approaches:: perceive 2D motion from RGB video streams. Problem Solution It integrates 2D detection from Mask R-CNN and 3D information from F-PointNet. It fuses the 3D shape descriptor with a 2D RGB descriptor through Aligned-ReID. It uses optimal joint probabilistic data association step. A novel multi-modal recursive Kalman filter was proposed. Conceptual Understanding It shows all components of the JRMOT, and workflow as below. Experiments It results on online KITTI Car and Pedestrian Tracking, and gets SOTA performance. Code [Updating] Note It shows data collection platform and sample visualization of the dataset. References Shenoi A, Patel M, Gwak J Y, et al. JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset[J]. arXiv preprint arXiv:2002.08397, 2020.","link":"/2020/02/28/JRMOT/"},{"title":"PAMCC-AOT","text":"Pose-Assisted Multi-Camera Collaboration System[1] is a novel method, which enables a camera to cooperate with the others by sharing camera poses for AOT(active object tracking). There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Pose-Assisted Multi-Camera Collaboration for Active Object Tracking(AAAI 2020 paper)Code: [Pytorch][Updating]Note: PAMCC-AOT Paper Abstract PAMCC-AOT is proposed to solve complex scenarios problems. The vision-based controller tracks targets based on observed images. The pose-based controller moves the camera in accordance to the poses of the other cameras. At each step, the switcher decides which action to take from the two controllers according to the visibility of the target. Problem Description AOT: a tracker is able to control its motion so as to follow a target autonomously.Problems: high complexity of environments and limitation of camera mobility. Problem Solution It extend the independent AOT to the CMC-AOT. They proposed PAMCC-AOT sharing camera poses. They provided a set of 3D environments. Conceptual Understanding In the system, each camera is equipped with two controllers and a switcher. Vision-based Controller: it serves as an image processor and guides the camera to execute policy based on image observation. Pose-based Controller: it helps the camera who receives an imperfect observation to execute policy based on the supplementary pose information provided by other cameras. Switcher: it makes the camera switch between the vision-based controller and pose-based controller properly. Experiments Code [Updating] Note References [1] Li, Jing, et al. “Pose-Assisted Multi-Camera Collaboration for Active Object Tracking.” arXiv preprint arXiv:2001.05161 (2020).","link":"/2020/02/25/PAMCC-AOT/"},{"title":"GlobalTrack","text":"GlobalTrack[1] is a pure global tracker for long-term tracking, without temporal consistency assumption making cumulative errors. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: GlobalTrack: A Simple and Strong Baseline for Long-term Tracking(AAAI 2020 paper)Code: PytorchNote: GlobalTrack Paper AbstractAs abstract of the paper, their work mainly proposed a method called GlobalTrack, which is a pure global instance search based tracker that makes no assumption on the temporal consistency. It is developed based on two-stage object detector Faster R-CNN, with two submodules QG-RPN and QG-RCNN. it is able to perform full-image and multi-scale search of arbitrary instances with only a single query as the guide. They further propose a cross-query loss to improve the robustness of this approach against distractors. Problem Description It shows the difficults of long-term tracking and the problem of existing trackers. Problem Solution It shows the methods for solving long-term tracking problem. Conceptual Understanding It describes the overall architecture of GlobalTrack with QG-RPN and QG-RCNN. Details of implementation Offline Training: it samples frame pairs from training videos. Online Tracking: it contains initialization, tracking and results. Cross-query Loss: it choose top-1 prediction as result. Architecture Query-Guide RPN: it generating query-specific proposals. Query-Guide RCNN: it consists of feature modulation and traditional RCNN. Tracking Results: it takes top-1 prediction as results. ExperimentsThey compared this approach GlobalTrack with state-of-the-art trackers on four large-scale tracking benchmarks as follows. Code The complete code can be found in [GlobalTrack][2]. Note some free ideas that orienting future work. References [1] Huang, Lianghua, Xin Zhao, and Kaiqi Huang. “GlobalTrack: A Simple and Strong Baseline for Long-term Tracking.” arXiv preprint arXiv:1912.08531 (2019).[2] GlobalTrack. https://github.com/huanglianghua/GlobalTrack","link":"/2020/01/04/GlobalTrack/"},{"title":"SiamMask","text":"SiamMask[1] is used to detect and segment objects from videos in each frame, initializing a single bounding box and outputing binary segmentation mask and rotated objects boxes. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Fast Online Object Tracking and Segmentation: A Unifying Approach(CVPR 2019 paper)Code: PyTorchNote: SiamMask Paper AbstractAs abstract of the paper, their work mainly dubbed a method called SiamMask, which foucused on VOT(visual object tracking) and semi-supervised VOS(video object segmentation). It improved the offline training by augmenting loss with a binary segmentatin task. It solely relies on a single bounding box initialisation and produces class-agnostic object mask and rotated bounding boxes. It yield a solid evidence that SiamMask is a new state of the art among real-time trackers. Problem Description It shows the task of SiamMask focused on and the needs for tacking this problem. Problem Solution It shows improments on Initialisation and outputs for accuracy. Conceptual Understanding It describes the whole architecture of SiamMask with three brach and two branch, which adds mask branch to original siamese network. Details of implementation network architecture: it consists of backbone, head and mask refinement module. training: it divides three parts to training respectively, including FC, RPN and segmentation. inference: it evaluated once per frame with max scores. ArchitectureMore details can be found in paper. backbone: it remains the first 4-th stage of ResNet, with adding adjust layer and depth-wise cross-correlated. head: The conv5 block in both variants contains a normalisation layer and ReLU non-linearity while conv6 only consists of a 1×1 convolutional layer. refinement: It merges low and high resolution features using multi- ple refinement modules made of upsampling layers and skip connections. ExperimentsAblation study shows the contributions for VOT.More experienment results shows below. Code The complete code can be found in [SiamMask][2]. Note some free ideas that orienting future work.More details of Understanding this work from author can be found in [3]. References [1] Wang, Qiang, et al. “Fast online object tracking and segmentation: A unifying approach.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.[2] SiamMask. https://github.com/foolwood/SiamMask[3] Qiang Wang. “Thinking about SiamMask.” https://zhuanlan.zhihu.com/p/58154634","link":"/2019/11/26/SiamMask/"},{"title":"Tracktor","text":"Tracktor[1] is used to detect objects from videos in each frame, while forming tracks by linking corresponding detections across time. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: [Tracking without bells and whistles(ICCV 2019 paper)Code: PyTorchNote: tracking_wo_bnw Paper AbstractAs abstract of the paper, their work mainly converted a detector into a Tracktor, which exploit the bounding box regression of an object detector to predict the position of an object in the next frame. It extended a straightforward re-identification and camera motion compensation to improving identity preservation across frames. It achieved state-of-the-art on tackling most of the easy tracking scenarios. Besides, it also got ideal effect in tackling complex tracking scenarios. Therefore, it point out promising future research directions. Problem Description It shows the problem of multi-object tracking and exsiting solution for tacking this problem. Problem Solution It intrudued a Tracktor, converted detector into a tracktor by exploit the bounding box regression, and then extended Siamese network and motion model. Conceptual Understanding It describes the whole flow of Tracktor, including detector and two processing steps, that is initialing new tracks and killing old tracks. Then it explains each symbol of this process and how to deal with them. Details of implementation tracking multi-object tracking step: it includes detecting object and form tracks by linking frames. reID: it uses Siamese network to generate apearance feature to re-identify killed objects. motion model: it contains problems of large camera motion and low video frame rate. Experiments tracking: it choosed Faster R-CNN with ResNet-101 as backbone network, it also provided FPN and other strategy. reID: it trained TriNet with ResNet-50 as backbone network, and triplet loss as loss function. CMC and ECC to deal with large camera motion, CVA to tackle low video frame rate problem. Code The complete code can be found in here with citing tracking_wo_bnw[2]. Tracktor algorithm Note some methods to improve accuracy or accelerate speed can add into this program. References [1] Bergmann, Philipp, Tim Meinhardt, and Laura Leal-Taixe. “Tracking without bells and whistles.” arXiv preprint arXiv:1903.05625 (2019).[2] tracking_wo_bnw. https://github.com/phil-bergmann/tracking_wo_bnw","link":"/2019/11/09/Tracktor/"},{"title":"Faster R-CNN","text":"Faster R-CNN[1] is used to detect objects in images, with outputing bounding box and class scores. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks(NIPS 2015 paper)Code: PyTorchNote: Faster R-CNN Paper AbstractAs abstract of the paper, their work mainly proposed a method called Faster R-CNN, which introduced a Region Proposal Network (RPN) and further merge RPN and Fast R-CNN to detect objects. It introduced a RPN. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. Besides, RPN using the recently popular terminology of neural networks with “attention” mechanisms to generate proposals. It merged RPN and Fast R-CNN into a single network. The unified network detects objects by sharing their convolutional features enabling nearly cost-free region proposals. Problem Description It shows the purpose of Faster R-CNN and exsiting methods about solving this problem. Problem Solution It intrudued a network called RPN, including how it works and what it roles. Conceptual Understanding It describes the whole architecture of Faster R-CNN, including how it works and what ouputs in each mudules. Core Conception It denotes the most important conception of Faster R-CNN mudules, and it explains the Conv layers (conv + relu + pooling), RPN (feature maps -&gt; proposals), RoI pooling (feature maps + proposals -&gt; proposal feature maps), Classification (proposal feature maps -&gt; bbox + cls) respectively. Besides, the network architecture shows below. Details of implementation RPN anchors: it seleted k(3*3) anchor boxes with outputing 2k scores and 4k coordinates. classication + regression: it takes RPN outputs as inputs, generating positive anchors and bbox regression. proposal layers: it contains pre_nms_topN, ignore cross-boundary, NMS, topN to generate proposals. Experiments loss function: it considers classification loss and regression loss as loss function. training: it choosed alternating training, that is to say, RPN -&gt; Fast R-CNN -&gt; RPN2 -&gt; unifiled network(RPN + Fast R-CNN). Code The complete code can be found in here with citing faster-rcnn.pytorch[2]. Datasets default datasets include PASCAL_VOC and COCO files. As my own data, it should transform to VOC or COCO format files.The details of data format as follows. `PASCAL_VOC`:|- VOCdevkit2007 |- VOC2007 |- Annotations |- .xml |- ImageSets |- Main |- trainval.txt |- test.txt |- JPEGImages |- .jpg|- VOCdevkit2012 |- VOC2012 |- ... Program improvement Modified files to be compatible with my own machine. Changed custom datasets and classes to train. Note More details of Faster R-CNN conception about anchors, loss and etc. can be found in [3]. References [1] Ren, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015.[2] faster-rcnn.pytorch. https://github.com/jwyang/faster-rcnn.pytorch[3] Shang Bai. “A paper understanding Faster R-CNN.” https://zhuanlan.zhihu.com/p/31426458.","link":"/2019/10/19/Faster-R-CNN/"},{"title":"ResNet","text":"ResNet[1] is used to classify images with deep residual learning. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: [Deep Residual Learning for Image Recognition(CVPR 2016 paper)Code: PyTorchNote: ResNet Paper AbstractAs abstract of the paper, their work mainly present a residual learning framework named ResNet, which based on the residual building block for classification and detection. It reformulate the layers as learning residual functions with reference to the layer inputs. Residual networks are easier to optimize, and can gain accuracy from considerably increased depth. Problem Description Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? It shows higher error with deeper network. Problem Solution Intuitively, the residual learning needs less to learn, because the residual is generally smaller. Therefore, the learning difficulty is smaller. Mathematically speaking, the gradients will not be vanished due to the shortcut connection, that is why residual is easier to learn. Conceptual Understanding It presents two version of building block for ResNet, including BasicBlock and Bottleneck. It describes the choise about different dimensions, as to input and output with same dimensions or dimensions increase. It shows the function of 1x1 and 3x3 convolution layers works. Core Concept Experiments It includes image classification datasets ImageNet, CIFAR-10, and objection detection datasets PASCAL VOC, COCO. Code The complete code can be found in [ResNet][2]. Details of implementation Convolution layerdef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1): \"\"\"3x3 convolution with padding\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)def conv1x1(in_planes, out_planes, stride=1): \"\"\"1x1 convolution\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False) Building blockclass BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(BasicBlock, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d if groups != 1 or base_width != 64: raise ValueError( 'BasicBlock only supports groups=1 and base_width=64') if dilation &gt; 1: raise NotImplementedError( \"Dilation &gt; 1 not supported in BasicBlock\") # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = norm_layer(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = norm_layer(planes) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None): super(Bottleneck, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d width = int(planes * (base_width / 64.)) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self.conv1 = conv1x1(inplanes, width) self.bn1 = norm_layer(width) self.conv2 = conv3x3(width, width, stride, groups, dilation) self.bn2 = norm_layer(width) self.conv3 = conv1x1(width, planes * self.expansion) self.bn3 = norm_layer(planes * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): identity = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: identity = self.downsample(x) out += identity out = self.relu(out) return out ResNetclass ResNet(nn.Module): def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None): super(ResNet, self).__init__() if norm_layer is None: norm_layer = nn.BatchNorm2d self._norm_layer = norm_layer self.inplanes = 64 self.dilation = 1 if replace_stride_with_dilation is None: # each element in the tuple indicates if we should replace # the 2x2 stride with a dilated convolution instead replace_stride_with_dilation = [False, False, False] if len(replace_stride_with_dilation) != 3: raise ValueError(\"replace_stride_with_dilation should be None \" \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)) self.groups = groups self.base_width = width_per_group self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = norm_layer(self.inplanes) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]) self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]) self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_( m.weight, mode='fan_out', nonlinearity='relu') elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) # Zero-initialize the last BN in each residual branch, # so that the residual branch starts with zeros, and each residual block behaves like an identity. # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677 if zero_init_residual: for m in self.modules(): if isinstance(m, Bottleneck): nn.init.constant_(m.bn3.weight, 0) elif isinstance(m, BasicBlock): nn.init.constant_(m.bn2.weight, 0) def _make_layer(self, block, planes, blocks, stride=1, dilate=False): norm_layer = self._norm_layer downsample = None previous_dilation = self.dilation if dilate: self.dilation *= stride stride = 1 if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( conv1x1(self.inplanes, planes * block.expansion, stride), norm_layer(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer)) self.inplanes = planes * block.expansion for _ in range(1, blocks): layers.append(block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation, norm_layer=norm_layer)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x Pretraindef _resnet(arch, block, layers, pretrained, progress, **kwargs): model = ResNet(block, layers, **kwargs) if pretrained: state_dict = load_state_dict_from_url(model_urls[arch], progress=progress) model.load_state_dict(state_dict) return model Different layerdef resnet18(pretrained=False, progress=True, **kwargs): return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)def resnet34(pretrained=False, progress=True, **kwargs): return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet50(pretrained=False, progress=True, **kwargs): return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)def resnet101(pretrained=False, progress=True, **kwargs): return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs)def resnet152(pretrained=False, progress=True, **kwargs): return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs) Note More details about code implementation can be found in [3], [4]. References [1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.[2] ResNet. https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code[3] Darkeyers. ResNet implementaion by Pytorch official. https://blog.csdn.net/darkeyers/article/details/90475475[4] Little teenager. CNN model you have to know: ResNet. https://zhuanlan.zhihu.com/p/31852747","link":"/2019/09/08/ResNet/"},{"title":"GoogLeNet","text":"GoogLeNet[1] is used to classify images with inception v1. There are some details of reading and implementing it. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Going deeper with convolutions(CVPR 2015 paper)Code: PyTorchNote: GoogLeNet Paper AbstractAs abstract of the paper, their work mainly proposed a CNN architecture codenamed Inception, so that to build a inception-based network with 22 layers called GoogLeNet for classification and detection. bulid Inception architecture based on the Hebbian principle and the intuition of multi-scale processing. It increased the depth and width of the network while keeping the computational budget constant. Problem Description It shows the purpose of GoogLenet and the drawbacks of exsiting methods about solving this problem. Problem Solution It proposal a network architecture named Inception, including what it can do and how it works. Conceptual Understanding It describes two version of architecture of Inception, including naive version and inception_v1. Core Conception It denotes the most important conception of Inception mudules, and it explains convolution on multiple scales to extract features, and using spare matrix to accelerate convergence speed with a instance. Besides, the network architecture shows below. Code The complete code can be found in here. Details of implementationthe whole network architecture:the details of googlenet:the step of implementation: It includes the whole network architeture and the implementation of auxiliary classfier. Inceptionclass BasicConv2d(nn.Module): def __init__(self, in_channels, out_channels, **kwargs): super(BasicConv2d, self).__init__() self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs) self.bn = nn.BatchNorm2d(out_channels, eps=0.001) def forward(self, x): x = self.conv(x) x = self.bn(x) return F.relu(x, inplace=True) class Inception(nn.Module): def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj): super(Inception, self).__init__() self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1) self.branch2 = nn.Sequential( BasicConv2d(in_channels, ch3x3red, kernel_size=1), BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1) ) self.branch3 = nn.Sequential( BasicConv2d(in_channels, ch5x5red, kernel_size=1), BasicConv2d(ch5x5red, ch5x5, kernel_size=3, padding=1) ) self.branch4 = nn.Sequential( nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True), BasicConv2d(in_channels, pool_proj, kernel_size=1) ) def forward(self, x): branch1 = self.branch1(x) branch2 = self.branch2(x) branch3 = self.branch3(x) branch4 = self.branch4(x) outputs = [branch1, branch2, branch3, branch4] return torch.cat(outputs, 1) Auxiliary classiferclass InceptionAux(nn.Module): def __init__(self, in_channels, num_classes): super(InceptionAux, self).__init__() self.conv = BasicConv2d(in_channels, 128, kernel_size=1) self.fc1 = nn.Linear(2048, 1024) self.fc2 = nn.Linear(1024, num_classes) def forward(self, x): # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14 x = F.adaptive_avg_pool2d(x, (4, 4)) # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4 x = self.conv(x) # N x 128 x 4 x 4 x = torch.flatten(x, 1) # N x 2048 x = F.relu(self.fc1(x), inplace=True) # N x 2048 x = F.dropout(x, 0.7, training=self.training) # N x 2048 x = self.fc2(x) # N x 1024 return x GoogLeNetclass GoogLeNet(nn.Module): def __init__(self, num_classes=1000, aux_logits=True, transform_input=False, init_weights=True): super(GoogLeNet, self).__init__() self.aux_logits = aux_logits self.transform_input = transform_input self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3) self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.conv2 = BasicConv2d(64, 64, kernel_size=1) self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1) self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32) self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64) self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64) self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64) self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64) self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64) self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128) self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True) self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128) self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128) if aux_logits: self.aux1 = InceptionAux(512, num_classes) self.aux2 = InceptionAux(528, num_classes) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.dropout = nn.Dropout(0.2) self.fc = nn.Linear(1024, num_classes) if init_weights: self._initialize_weights() def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear): import scipy.stats as stats X = stats.truncnorm(-2, 2, scale=0.01) values = torch.as_tensor( X.rvs(m.weight.numel()), dtype=m.weight.dtype) values = values.view(m.weight.size()) with torch.no_grad(): m.weight.copy_(values) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) def forward(self, x): if self.transform_input: x_ch0 = torch.unsqueeze(x[:, 0], 1) * \\ (0.229 / 0.5) + (0.485 - 0.5) / 0.5 x_ch1 = torch.unsqueeze(x[:, 1], 1) * \\ (0.224 / 0.5) + (0.456 - 0.5) / 0.5 x_ch2 = torch.unsqueeze(x[:, 2], 1) * \\ (0.225 / 0.5) + (0.406 - 0.5) / 0.5 x = torch.cat((x_ch0, x_ch1, x_ch2), 1) # N x 3 x 224 x 224 x = self.conv1(x) # N x 64 x 112 x 112 x = self.maxpool1(x) # N x 64 x 56 x 56 x = self.conv2(x) # N x 64 x 56 x 56 x = self.conv3(x) # N x 192 x 56 x 56 x = self.maxpool2(x) # N x 192 x 28 x 28 x = self.inception3a(x) # N x 256 x 28 x 28 x = self.inception3b(x) # N x 480 x 28 x 28 x = self.maxpool3(x) # N x 480 x 14 x 14 x = self.inception4a(x) # N x 512 x 14 x 14 if self.training and self.aux_logits: aux1 = self.aux1(x) x = self.inception4b(x) # N x 512 x 14 x 14 x = self.inception4c(x) # N x 512 x 14 x 14 x = self.inception4d(x) # N x 528 x 14 x 14 if self.training and self.aux_logits: aux2 = self.aux2(x) x = self.inception4e(x) # N x 832 x 14 x 14 x = self.maxpool4(x) # N x 832 x 7 x 7 x = self.inception5a(x) # N x 832 x 7 x 7 x = self.inception5b(x) # N x 1024 x 7 x 7 x = self.avgpool(x) # N x 1024 x 1 x 1 x = torch.flatten(x, 1) # N x 1024 x = self.dropout(x) x = self.fc(x) # N x 1000 (num_classes) if self.training and self.aux_logits: return _GoogLeNetOutputs(x, aux2, aux1) return x Note More details of Inception about implementation can be found in [2].More details of conception about multi-scale and spare matrix can be found in [3]. References [1] Szegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.[2] Bing Xue. “Big word about CNN classic model.” https://my.oschina.net/u/876354/blog/1637819.[3] Lei Zhang. “Deep understanding GoogLeNet architecture.” https://zhuanlan.zhihu.com/p/32702031.","link":"/2019/09/05/GoogLeNet/"},{"title":"NIN(Network In Network)","text":"There are some details of reading and implementing the Network In Network for image classification. Contents Paper &amp; Code &amp; note Paper understanding Code understanding Note References Paper &amp; Code &amp; note Paper: Network In Network(arXiv 2013 paper)Code: PyTorchNote: NIN Paper AbstractAs abstract of the paper, their work mainly build a “micro network” called Network In Network (NIN) to replace traditional Convolutional Nerual Network(CNNs) and utilize global average pooling (GAP) instead of fully-connected layer(FC) to classify images. build micro neural networks with more complex structures to abstract the data within the receptive field. utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. Problem Description It shows the steps of classifing images as well as describes the traditional and state-of-the-art methods. Problem Solution It includes MLP (multilayer perceotrn) layer and GAP (global average pooling). Conceptual Understanding It describes how does mlpconv works and what does GAP means. Core Conception It denotes the most important conception of Network In Network (NIN) and explains the steps of traditional CNNs and novel NIN to classify images respectively. Besides, the comparison shows below. Experimental Resultsdatasets: CIFAR-10 CIFAR-100 SVHN MNIST notes: fine-tuned local receptive field size and weight decay. using dropout. Code Model Detail NIN( (features1): Sequential( (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): ReLU() (2): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1)) (3): ReLU() (4): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (7): Dropout(p=0.5, inplace=False) ) (features2): Sequential( (0): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): ReLU() (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1)) (3): ReLU() (4): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (7): Dropout(p=0.5, inplace=False) ) (features3): Sequential( (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): ReLU() (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1)) (3): ReLU() (4): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1)) (5): ReLU() ) (gap): AvgPool2d(kernel_size=8, stride=1, padding=0)) PyTorch Implementation The complete code can be found in here.The implementation of network as follows. class NIN(nn.Module): def __init__(self, in_channels=3, out_channels=10): super(NIN, self).__init__() self.features1 = nn.Sequential( nn.Conv2d(in_channels, 192, kernel_size=5, stride=1, padding=2), nn.ReLU(), nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(160, 96, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), nn.Dropout(0.5) ) self.features2 = nn.Sequential( nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2), nn.ReLU(), nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1), nn.Dropout(0.5) ) self.features3 = nn.Sequential( nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0), nn.ReLU(), nn.Conv2d(192, out_channels, kernel_size=1, stride=1, padding=0), nn.ReLU(), ) self.gap = nn.AvgPool2d(kernel_size=8, stride=1, padding=0) def forward(self, x): x = self.features1(x) x = self.features2(x) x = self.features3(x) x = self.gap(x) x = x.view(x.size(0), -1) return x TensorFlow Implementation the code and more details can be found in [2]. Note More details of mlpconv and cccp can be found in [3].What the effect of 1 by 1 convolution kernel can be found in [4]. References [1] Min Lin, Qiang Chen, and Shuicheng Yan. “Network in network.” arXiv preprint arXiv:1312.4400 (2013).[2] Eugene. “NETWORK-IN-NETWORK IMPLEMENTATION USING TENSORFLOW.” https://embedai.wordpress.com/2017/07/23/network-in-network-implementation-using-tensorflow/[3] Ou. “(Paper) Network Analysis of Network In Network.” https://blog.csdn.net/mounty_fsc/article/details/51746111[4] ysu. “What is the role of 1 by 1 convolution kernel?” http://www.caffecn.cn/?/question/136","link":"/2019/08/31/NIN-Network-In-Network/"},{"title":"Classification Review","text":"There are the list of typical Image Classification CNNs. Todos LeNet-5: [ , 1, 32, 32] - [6, 16, 120, 84, 10] AlexNet v1/v2: [ , 3, 224, 224] - [64, 192, 384, 256, 256, 4096, 4096, 1000] VGG 11/13/16/19: [, 3, 224, 224] - [64,64, 128,128, 256…, 512…, 512…, 4096, 4096, 1000] NIN: [MLP + AVG] instead of [CNNs + FC] ResNet 18/34/50/101/152: [ , 3, 224, 224] - [64, 643, 1284, 2566, 5123, 1000] – (1,3,1) GoogLeNet MobileNet DenseNet","link":"/2019/08/25/Classification-Review/"},{"title":"RN(Relation Network)","text":"There are some details of reading and implementing the Relation Network for few-shot learning. Contents Paper &amp; Code &amp; note Paper understanding Code understanding References Note Paper &amp; Code &amp; note Paper: Learning to Compare: Relation Network for Few-Shot Learning(CVPR 2018 paper)Code: PyTorch(Few-Shot Learning part)Note: RN for FSL Paper AbstractAs abstract of the paper, their work mainly proposed a method called Realation Network (RN) to recognise new classes given only few examples from each. It based on meta-learning. That is to say, the RN learns a deep distance metric to compare a number of images with episodes, and it is a episode-based method. It classify images of new classes by computing relation scores. That is to say, there is a score in each query image with their relations of sample images in each class. Problem Description It shows the task of few-shot learning and the exists model. Problem Solution It includes Embedding module and Relation module of the RN. References: [36, 39], RNNs: [39, 32, 29], Fine-tuning: [29, 10]. Conceptual Understanding It describes what is meta-learning and how to classify query images. Remaining Problem It is the question in my mind in terms of the paper and the code. Core Conception It denotes the most important conception of Relation Network (RN) and explains the Embedding module and Relation module respectively. Besides, the network architecture shows below. Experimental Results There are results of carring RN on Omniglot and miniImagenet datasets in paper, which shows that RN got better performance when comparing with other state-of-the-art methods. Code Program block It divides program files to three blocks, which are pre-process, train and test as well as the function list of the blocks. Program explanation It explains the details of the code blocks in each process. Program improvement Main work in my improved code are tackling problems and optimizing functions as well as train and test my personnal datasets. References [1] Sung F, Yang Y, Zhang L, et al. Learning to compare: Relation network for few-shot learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1199-1208.[2] LearningToCompare_FSL. https://github.com/floodsung/LearningToCompare_FSL.[3] Pytorch. https://github.com/pytorch/pytorch. Note","link":"/2019/08/21/RN-Realation-Network/"},{"title":"Hexo+Blog+Pages","text":"This blog records the details about building hexo blogs, as well as deploying it to Github and Coding Pages. Basic Installation Install Node.js and Gitinstall Node.js Just go to the website and you can download it. Then, it will be installed at /usr/local/bin .After that, you can validate it as follows: $ node -vv10.16.0$ npm -v6.9.0 install GitAlso, you can just download and check your git version as follows: $ Git --versiongit version 2.20.1 Install HexoNow, we can install the Hexo. Note that add sudo to solve the problem of permission. -g refers global install. $ sudo npm install -g hexo-cli I have met some problems in this way, and through another way solved it: sudo npm install -g hexo-cli --unsafe-perm Initialize Hexo Create a folder(e.g. blog ) and cd into it. Initialize blog and download a series of files. Install npm . hexo generate and server. $ mkdir blog$ cd blog$ sudo hexo init$ sudo npm install$ hexo g$ hexo s Install Themes Note that there are more than one _config.yml in the blog file. one is in root directory . and every themes directory also have one. Choose themesIt is possible for us to choose our own themes by the website.For me, what I like is the hexo-theme-icarus. Download themesNow, what we need to do is download it as follows: $ cd blog$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus Change themesThen, for the _config.yml in root directory : Changing theme: landscape to theme: yilia . Reload hexoLast, just reload hexo generate and server . Customize themes There are a lot of ways to define your own functions. For me, a simple way is finding a suitalbe theme and just download it. Besides, I visited all the issues and documents of offical github.The following is core functions. “_config.yml”This file in root directory represents the global hexo settings , and in themes directory just configure one theme. root directoryIt includes site , themes , deployment , sitemap and so on. My complete configuration pushed at the Github . icarus directorySimilarly, it contains important settings. Like images , navbar , footer , search , comment, widgets, and other plugins . Pages &amp; Domain Pages:There are Github Page and Coding Page could be choose. More details can be find in their websites. Domain:I register .top domain in AliCloud , following resolve the URL by CNAME . Sitemap baidusitemap.xml sitemap.xml(Google) Conclusion This blog just mentioned kinds of core keywords , if you have any question about that, just google or baidu it and then solve it. Notes: npm usage: sudo cnpm install xxx to use taobao mirrors. daily commands: hexo clean , hexo s -g , hexo d -g . Source Tree.├── _config.yml├── package.json├─.deploy_git├─node_modules├─public├─scaffolds├─source ├─_posts ├─about ├─gallery └─CNAME└─themes ├─landscape └─icarus ├─_config.yml ├─source └─... References [1] Hexo Documents[2] Icarus Documents &amp; Issues[3] customize Icarus[4] deploy git to Github &amp; Coding[5] push sitemap to Baidu &amp; Google[6] back up hexo datas to Github","link":"/2019/08/07/Hexo-Blog-Pages/"},{"title":"Java Back-end Interview","text":"写在前面 在大三上的期末时间段，偶然得知某滴信息安全部的后端开发实习岗位。由于厂牌和待遇的吸引，便抱着试一试的心态投了简历。下面就一面的提问进行技术总结，答案仅为个人理解，问题没有详细解释只提出大概要点。 面试细节 自我介绍首先是介绍自己，包括求职岗位，个人情况（比如学校、年级、专业等），技术学习情况（技术栈、技术能力、项目等）。 项目 项目架构 细节处理及实现 由于在简历的项目经历必不可少，首先就选了一个项目进行细节剖析。这个就是把自己的实现思路表达出来，毕竟自己写的项目对所运用到的东西心里也有点数。针对这次面试，面试官对安全方面可能看得比较重，所以从头到尾一直在挖这边的细节。 Java基础String、StringBuffered 二者区别 存储在JVM哪里 可变，线程安全；存储在常量池。 编码 Class文件编码 谈谈遇到过的乱码问题及如何解决 UTF-8和UTF-16区别 Class文件采用Unicode编码(UTF-16)；乱码问题： 页面乱码（HTML、JSP）， 传值乱码（配置过滤器）， 数据库乱码（检查Tomcat、Mysql配置）其他相关： http://www.qianxingzhem.com/post-1499.html abstract、interface 两者区别 多继承实现，全部抽象； JDK8 这个暂时不太了解，下面贴出关于新特性的官方说明和博客链接：官方说明： http://www.oracle.com/technetwork/java/javase/8-whats-new-2157071.html博客： http://blog.csdn.net/qiubabin/article/details/70256683 集合 简述集合以及底层实现 HashMap、HashTable，如何改进 ArrayList、LinkedList，区别 HashMap、TreeMap、LinkedHashMap，区别 线程安全、效率；数组查询，链表增删；HashMap：允许一条记录键为空，多条记录值为空，不同步；HashTable：不允许键值为空，同步，效率低；LinkedHashMap：保存记录的插入顺序；TreeMap：可以根据键排序。参考： http://blog.csdn.net/xin_jmail/article/details/25975085 类加载机制 (1)全盘负责(2)父类委托(3)缓存机制参考： https://www.cnblogs.com/ityouknow/p/5603287.html JVM 内存模型 数据存储位置（如int i=0;） GC（判断GC root，GC分区） JVM相关： http://blog.gojay.xin/2017/12/09/初识Java虚拟机/ 专业相关 如何保证信息没有被更改 认证、授权 数据加密 后端相关Servlet生命周期 容器加载类实例化init()service()destroy() GET、POST 二者区别 body，参数，安全性。 cookie、session 二者区别 两台服务器负载均衡处理session 客户端（浏览器）、服务器；session保持、复制、共享； 算法 Stack特性 用stack实现时间复杂度为O(1)的getMin()方法 后进先出；使用辅助栈http://blog.csdn.net/sheepmu/article/details/38459165 数据库 事务及特性 ACID https://www.cnblogs.com/nobounds/p/5409472.html 其他问题后面就根据简历上面写的东西对其他方面提了一些问题，比如看过哪些技术书籍，平时怎么学习以及解决问题等等。 写在最后 这次面试整体来说，主要是简历上写了哪些就问的相关技术点，熟悉、掌握、了解都基本会提到一点。感触较大的是什么问题他都能够一直往深处挖，到最后确实不知道或者自己看情况直接说不了解。由于这次招人比较紧急，要求不算太高，所以也就放得比较宽，没有太严格。我也是匆忙投简历，加上当天考英语六级，基本上就没怎么准备。后期会结合自身再改改简历，最重要的还是提升自己，针对技术方面还是需要稳扎稳打，夯实基础。过了几天换了一个人打电话来约二面的时候再次提到了入职和任职时间，最终因为没有协调好时间就没有进行二面。虽然有点遗憾，但是我也更加了解自己现在的水平，以及正式入职所需要具备的能力。这也算是一个转折点吧，最终我决定了考研，技术方面自然也会落下不少，技术博客最近一年也不会怎么更了吧，包括这一篇本应该一个多月前更的也拖到了现在。最后的最后，祝自己考研顺利吧，不忘初心，方能始终。","link":"/2018/02/01/Java-Back-end-Interview/"},{"title":"Learning Java Virtual Machine(JVM)","text":"前言 对于Java的学习也有一段时间了，却始终会有一些地方容易混淆，归结原因，还是偏底层的东西不太了解。前段时间便学习了关于Java虚拟机相关的内容，主要从阅读 《深入理解Java虚拟机》 进行总结。 首先Java技术体系主要由：Java第三方框架类库、Java API类库、Java程序设计语言、Class类文件格式、Java 虚拟机构成，把Java API类库、Java程序设计语言、Java虚拟机统称为JDK，用于支持Java程序开发的最小运行环境。然后从 Java内存 相关的 内存模型 、 内存分配 、 垃圾回收 、 内存溢出 ； 虚拟机执行子系统 相关的 Class类文件结构 、 类加载机制 、 字节码执行引擎 ； 高效并发 相关的 Java内存模型与线程 、 线程安全与锁优化 几个部分进行了Java虚拟机初步的学习。 Java内存 内存模型 程序计数器程序计数器(Program Counter Register) 可以看做当前线程所执行的字节码的行号指示器，每条线程都有一个独立的程序计数器，称为 线程私有 内存。如果线程正在执行一个Java方法，计数器记录的是正在执行的虚拟机字节码指令的地址；如果执行的是Native方法，则计数器值为空(Undefined)。 虚拟机栈虚拟机栈(Virtual Machine Stacks) 是 线程私有 ，每个方法在执行的同时都会创建一个栈帧(Stack Frame)，用于存储局部变量表、操作数栈、动态链接、方法出口等信息（具体内容在后面会讲到）。每个方法从调用到执行完成对应一个栈帧从入栈到出栈。 经常把Java内存分为堆内存(Heap)和栈内存(Stack)，这里指的栈就是虚拟机栈。 本地方法栈本地方法栈(Native Method Stack) 与虚拟机栈作用类似，只不过不是为Java方法（也就是字节码）服务，而是为虚拟机使用的Native方法服务。 堆堆(Heap) 是被 所有线程共享 的一块内存区域，在虚拟机启动时创建。用于存放对象实例，几乎所有的对象实例以及数组都在堆上分配内存。Java堆可以处于物理上不连续的内存空间中，只要逻辑上连续即可，也是垃圾收集器管理的主要区域。 方法区方法区(Method Area) 是 各个线程共享 的内存区域。用于存储已经被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 有别名叫Non-Heap(非堆)，也被称为“永久代”。 额外：HotSpot虚拟机对象对象的创建 类加载检查：虚拟机遇到new指令，检查这个指令的参数能否定位到一个类的符号引用；并检查这个符号引用代表的类是否已被加载、解析和初始化过。若没有，则先执行相应的类加载过程。 为对象分配内存：把一块确定大小的内存从Java堆中划分出来。由Java堆是否规整有两种划分方式， “指针碰撞” ：中间放置一个指针作为分界点的指示器，分配内存就是指针向空闲空间挪动； “空闲列表” ：维护内存块可用的列表，分配内存就是从列表找出一块足够大的空间。 初始化零值：将分配到的内存空间都初始化为零值，不包含对象头。 必要设置：将对象的对象头信息取出进行必要的设置。 ：执行new指令后会执行方法，把对象按照程序员的意愿进行初始化。 对象的内存布局 对象头：对象头包含两部分信息，“Mark Word”用于存储对象自身的运行时数据；“类型指针”用于存储对象指向它的类元数据的指针。 实例数据：在程序代码中所定义的各种类型的字段内容。 对齐填充：不是必然存在的，只是起着占位符的作用。 内存的访问定位 使用句柄：Java堆划分一块内存作为句柄池，reference存储句柄地址。句柄中包含 对象实例数据 和 类型数据的具体地址。 直接地址访问：Java堆的对象考虑如何放置访问类型数据的相关信息。reference存储的是对象地址。 内存溢出Java堆溢出Java堆用于存储对象实例，不断创建对象，当避免垃圾回收机制，在对象数量达到最大堆的容量限制后就会产生内存溢出异常。 虚拟机栈和本地方法栈溢出 线程请求的栈深度大于虚拟机所允许的最大深度，将抛出 StackOverflowError 异常。 虚拟机在扩展栈时无法申请到足够的内存空间，将抛出 OutOfMemoryError 异常。 方法区溢出方法区用于存放Class相关信息，如类名、访问修饰符、常量池、字段描述、方法描述等，当运行时产生大量的类填满方法区时会产生内存溢出。 垃圾回收对象是否可回收引用计数算法引用计数算法 是给对象添加一个引用计数器，当一个地方引用时，计数器值加1；当引用失效时，计数器值减1；当计数器值为0时说明对象不可用。 不能解决对象之间相互循环引用的问题。 可达性分析算法可达性分析 是选取 “GC Roots” 对象作为起始点，向下搜索走过的路径称为 引用链 ；当一个对象到GC Roots没有任何引用链的时候则说明对象不可用。 Java语言中可作为引用链的对象包括：虚拟机栈、方法区中类静态属性、方法区常量、本地方法栈Native方法引用的对象。 两次标记在可达性分析算法中不可达的对象还需要经历两次标记才真正回收。 是否有必要执行finalize()方法：当对象 没有覆盖finalize()方法 或者finalize()方法 已经被虚拟机调用过 则视为没有必要执行。 重新引用：如果对象有必要执行finalize()方法，则会将对象放置在F-Queue队列中，由低优先级Finalizer线程执行，在此过程中只要 重新与引用链上一个对象建立关联 则会移除回收队列。 垃圾回收算法标记-清除算法 标记所有需要回收对象。 回收所有被标记对象。 复制算法将内存按容量划分为两块，每次只使用其中一块。 标记回收对象。 将存活对象复制到另一块。 已使用内存空间全部回收。 标记-整理算法 标记回收对象。 将存活对象向一端移动。 回收边界以外内存。 分代收集算法根据对象存活周期将Java堆划分为新生代和老年代，不同年代使用不同的垃圾回收算法。 垃圾收集器 Serial收集器Serial收集器 是负责新生代的收集的单线程收集器。垃圾回收时会暂停其他所有的工作线程，直到收集结束。 ParNew收集器ParNew收集器 是Serial收集器的多线程版本。除了Serial收集器，只有它能与CMS收集器配合工作。 Parallel Scavenge收集器Parallel Scavenge收集器 也是使用复制算法的多线程收集器。目的是达到一个可控制的吞吐量， 吞吐量=运行用户代码时间 / （运行用户代码时间+垃圾回收时间） ，也被称为 “吞吐量优先”收集器。 Serial Old收集器Serial Old收集器 是Serial收集器的老年代版本，使用标记-整理算法的单线程收集器。 Parallel Old收集器Parallel Old 是Parallel Scavenge收集器的老年代版本，使用标记-整理算法的多线程收集器。 CMS收集器CMS收集器 是使用标记-清除算法的多线程收集器，目的是获取最短回收停顿时间。 初始标记(CMS initial mark) 并发标记(CMS concurrent mark) 重新标记(CMS remark) 并发清除(CMS concurrent sweep) G1收集器G1收集器 在后台维护一个优先列表，根据允许的收集时间，优先回收价值（回收所获得的空间大小以及所需时间）最大的Region。 初始标记(Initial Marking) 并发标记(Concurrent Marking) 最终标记(Final Marking) 筛选回收(Live Data Counting and Evacuation) 理解GC日志GC日志是一些人为确定的规则，每个日志格式有收集器决定。通常来看由以下几个部分组成： GC发生时间 、 GC停顿类型 、 GC发生区域 、 GC前后内存使用情况 、 GC所占用时间。 内存分配对象优先在Eden分配大多情况下，对象在新生代Eden区域中分配。Eden空间不足时，虚拟机发起一次Minor GC。Minor GC过程：将Eden区域对象放入Survivor空间，若无法放入则通过分配担保机制提前进入到老年代。 新生代GC(Minor GC)：Minor GC频繁，回收速度快。老年代GC(Major GC/Full GC)：Full GC速度一般比Minor GC慢10倍。 空间分配担保在Minor GC前，虚拟机会先检查老年代连续空间是否大于新生代对象总大小。若成立，则Minor GC安全；若不成立，虚拟机会查看是否允许担保失败。担保：取每一次进入老年代对象的平均值与老年代剩余空间比较，若不足则进行Full GC。 大对象直接进入老年代大对象指需要 大量连续内存空间 的Java对象。当所需空间大于设置值时直接进入老年代分配，目的在于避免在Eden区及两个Survivor区之间发生大量的内存复制。 长期存活的对象进入老年代年龄计数器：对象在Survivor区每过一次Minor GC则年龄加1。当年龄大于设置值(默认为15)则进入老年代。对象年龄动态判断：如果在Survivor空间中， 相同年龄 所有对象大小超过Survivor空间的一半，年龄大于或等于该年龄的对象进入老年代。 虚拟机执行子系统 Class类文件结构概述 Class文件是一组以 8个字节为单位 的二进制流，对应着类或接口的定义信息，是实现 平台无关性 和 语言无关性 的基础。 Class文件格式采用 伪结构 存储数据，这种伪结构只有两种数据类型：无符号数(u1、u2、u4、u8代表x个字节的无符号数)，表(由多个无符号数或其他表构成，习惯以_info结尾)。 Class文件格式 魔数：前 4个字节 ，值为：0xCAFEBABE。 Class版本号：紧接着魔数的 4个字节 ，分别为：次版本号、主版本号。 常量池：紧接着Class版本号，常量数量 不固定 ，入口放置一项u2类型的 常量池容量计数器 。主要存放 字面量 (Java中常量)和 符号引用 (类、接口、字段、方法的名称和描述符)。 访问标志：紧接着常量池的 2个字节 ，用于标识一些类或接口层次的 访问信息 。 类索引、父类索引、接口索引：排列着访问标志之后，类索引和父类索引用两个u2类型表示，接口索引是一组u2类型的集合；索引用于确定 全限定名 来确定这个类的 继承关系 。 字段表集合：用于描述接口或者类中声明的 变量信息 。字段信息需要 引用常量池 中的常量来描述，无法固定大小。 方法表集合：与字段表相似，用于 描述方法 定义的标志、名称索引、描述符索引。 属性表集合：Class文件、字段表、方法表都可以携带自己的属性表集合，用于 描述特定信息 。预定义包含Code、Exception、LineNumberTable、LocalVariableTable等属性。 全限定名：把类全名中的”.”替换成了”/“，如com/baidu/www/class/TestClass。简单名称：没有类型和参数修饰的方法或者字段名称，如inc()方法和m字段简称为inc和m。描述符：描述字段的数据类型、方法的参数类型、返回值。 基本类型和void用一个大写字符表示，I。 对象类型用大写字符L加对象全限定名表示，Ljava/lang/String。 数组类型的每一个维度使用一个前置的[字符描述，如[[Ljava/lang/String、[I。 方法：先参数列表，后返回值，如()V、()Ljava/lang/String、([CII[CIII)I。 字节码指令 Java虚拟机的指令由操作码(一个字节长度的数字)和操作数(零至多个代表此操作所需的参数)构成。 字节码与数据类型由于Java虚拟机的操作码长度只有1个字节，指令集将会故意被设计为非完全独立，即并非每种数据类型和每一种操作都有对应的指令。大部分的指令都没有支持boolean、byte、char、short类型的操作，实际上都是使用相应的int类型作为运算类型。 加载和存储指令加载和存储指令 用于将数据在栈帧中的局部变量表和操作数栈之间来回传输。 将局部变量加载到操作数栈：iload等。 将一个数值从操作数栈存储到局部变量表：istore等。 将一个常量加载到操作数栈：bipush、sipush、iconst_m1等。 扩充局部变量表的访问索引指令：wide。 运算指令运算或算数指令 用于对两个操作数栈的值进行某种特定运算，并把结果重新存入到操作数栈顶。都使用Java虚拟机的数据类型，boolean、byte、char、short的运算都会转为int类型。算数指令有：加法(iadd、ladd、fadd、dadd)、减法(sub)、乘法(mul)、除法(div)、求余(rem)、取反(neg)、位移、按位或、按位与、按位异或、局部变量自增、比较。 类型转换指令类型转换指令用于将两种不同的数值类型进行相互转换。 宽化 类型转换无需显式的转换指令。 窄化 必须显示转换：i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l、d2f。 其他指令其他指令可以查看虚拟机字节码指令，这里不全部列出，主要有：对象创建与访问指令、操作数栈管理指令、控制转移指令、方法调用和返回指令、异常处理指令、同步指令。 虚拟机类加载机制类加载机制指虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析、初始化，最终形成可以被虚拟机直接使用的Java类型。 类加载的时机类的生命周期 加载、连接（验证、准备、解析）、初始化、使用、卸载。 对类主动引用有且只有5中情况需要立即对类进行初始化： 遇到new、getstatic、putstatic、invokestatic这4个字节码时，如果类没有过初始化，则需要先触发其初始化。Java代码场景： new实例化对象 、 读取或设置一个类的静态字段 、 调用一个类的静态方法。 使用 java.lang.reflect包 的方法对类进行反射调用时，该类没有过初始化需要触发初始化。 当初始化一个类，其父类没有过初始化需要先初始化父类。 虚拟机启动时，需要先初始化含main()方法的主类。 java.lang.invoke.MethodHandle实例解析句柄对应的类需要初始化。 被动引用所有被动引用类都不会触发初始化。 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。 通过数组定义来引用类，不会触发此类的初始化。 调用类的常量，由于常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用类，不会触发该类的初始化。 接口不要求其父接口都完成了初始化，只有在真正使用父接口的时候(引用接口的常量)才会初始化。 类加载的过程Java虚拟机中类加载全过程：加载、验证、准备、解析、初始化。 加载 通过一个类的全限定名来获取定义此类的二进制流。 将这个字节流按照虚拟机所需的格式存储在方法区中。 在内存中生成一个代表这个类的java.lang.Class对象(HotSpot虚拟机存放在方法区中)，作为方法区这个类的各种数据的访问入口。 验证确保Class文件的字节流中包含的信息符合虚拟机的要求。 文件格式验证：对Class文件格式中魔数、版本号、常量池等进行验证，保证字节流能正常解析并存储到方法区。 元数据验证：对字节码描述的信息该类是否有父类、是否继承final类等进行语义分析，保证符合Java语言规范。 字节码验证：通过数据流和控制流分析，确定程序语义是符合逻辑的。 符号引用验证：对类自身以外(常量池中的各种符号引用)的信息进行匹配性验证。 准备为类变量(static修饰)分配内存并设置初值(数据类型的零值)。 解析虚拟机将常量池内的符号引用替换为直接引用。 符号引用：以一组符号来描述所引用的目标，引用目标并不一定已经加载到内存中。 直接引用：直接引用是直接指向目标的指针、偏移量或者是一个能够间接定位到目标的句柄。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄、调用点限定符这几类符号引用进行。 类或接口的解析假设当前代码所处的 类为D ，要把一个从未解析过的 符号引用N 解析为一个 类或接口C 的直接引用： 如果C不是一个数组类型，虚拟机会把代表N的全限定名传递给D的类加载器去加载这个类C；可能会触发其他相关类的加载，如父类或实现的接口。 如果C是一个数组类型，数组的元素类型为对象按照上一点规则加载；数组的元素类型为基本类型则由虚拟机生成一个代表此数组维度和元素的数组对象。 如果以上步骤没有异常，则C在虚拟机中已经成为一个有效的类或接口。 字段解析 解析字段所属的类或接口的符号引用。 与类中匹配目标的简单名称和字段描述符。 按照继承关系从下往上递归搜索接口和父接口。 如果不是java.lang.Object，搜索其父类。 否则，查找失败。 类方法解析 解析方法所属的类或接口的符号引用。 在类中查找简单名称和描述符。 在类的父类中查找简单名称和描述符。 在类实现的接口列表和父接口中匹配。 否则，查找失败。 接口方法解析 解析方法所属的类或接口的符号引用。 在接口中查找简单名称和描述符。 在父接口中查找简单名称和描述符。 否则，查找失败。 初始化开始执行类中定义的Java程序代码(字节码)。初始化阶段时执行类构造器()方法的过程。 ()方法是由编译器自动收集类中的类变量赋值和静态语句块。静态语句块中只能访问到定义在静态语句块之前变量， 之后的变量只能赋值不能访问。 ()方法实例构造器()方法不同，不需要显式调用父类构造器，保证父类()方法在子类()方法前执行。 由于父类先执行()方法，父类定义的静态语句块先于子类，第一个被执行()方法的类是java.lang.Object。 ()方法对呀类或接口不是必需的。 虚拟机会保证一个类的()方法在多线程环境中正确被加锁、同步。 类加载器类加载器是实现让应用程序自己决定如何去获取所需要的类的代码模块。 类与类加载器对于任意一个类，都需要由加载它的类加载器和这个类本身一起确立其在Java虚拟机中的唯一性。 双亲委派模型 启动类加载器(Bootstrap ClassLoader)：负责将存放在&lt;JAVA_HOME\\lib&gt;目录下的类库加载到虚拟机内存中。 扩展类加载器(Extension ClassLoader)：负责加载&lt;JAVA_HOME\\lib\\ext&gt;目录中的类库。 应用程序类加载器(Application ClassLoader)：负责加载用户类路径(ClassPath)指定的类库。 双亲委派模型的工作流程：如果一个类加载器收到类加载的请求，首先将这个请求委派给父类加载器去完成，最终传送到顶层的启动类加载器；当父加载器反馈无法完成这个加载请求，子加载器才会尝试加载。 破坏双亲委派模型 第一次：JDK 1.2发布前，重写loadClass()方法。 第二次：模型自身缺陷，线程上下文类加载器可以实现父类加载器请求子类加载器去完成类加载动作。 第三次：对程序动态性的追求。 字节码执行引擎运行时栈帧结构栈帧(Stack Frame) 是用于支持虚拟机进行方法调用和方法执行的数据结构，存储了局部变量表、操作数栈、动态连接、方法返回地址等信息。每一个方法从调用开始到执行完成对应栈帧在虚拟机从入栈到出栈的过程。 局部变量表局部变量表(Local Variable Table) 是一组变量值存储空间，用于存放方法参数和方法内部定义的局部变量。局部变量表的容量以变量槽(Variable Slot)为最小单位，每个Slot都能存放一个 boolean 、 byte 、 char 、 short 、 int 、 float 、 reference 、 returnAddress 类型的数据。reference类型表示对一个对象的引用，通过引用要做到两点：从此引用中直接或间接地查找到到对象在 Java堆 中的数据存放的起始 地址索引 ；此引用中直接或间接查找到对象所属数据类型在 方法区 中的存储的 类型 信息。虚拟机通过索引定位的方式使用局部变量表。如果执行的实例方法(非static)，局部变量表的第0位索引的Slot默认是用于传递方法所属对象实例的引用，在方法中可以通过关键字this访问该隐含参数。其他参数按照参数表顺序排序，局部变量表的Slot可以重用。如果一个局部变量 定义 了但没有 赋初始值 会导致类加载失败。 操作数栈操作数栈(Operand Stack) 是一个 后入先出(Last In First Out) 栈，每一个元素可以是任意的Java数据类型。在方法执行过程，各种字节码指令往操作数栈中写入和提取内容，也就是入栈/出栈操作。 动态连接每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用。在每一次运行时期将符号引用转化为直接引用称为动态连接。 返回地址方法在退出后，都需要返回到方法被调用的位置，栈帧保存返回地址信息。 方法调用方法调用的目的是确定被调用方法的版本，即调用哪一个方法。 解析所有的方法调用中的目标方法在Class文件中都是一个常量池中的符号引用。在类加载的解析阶段，将会把一部分符号引用转化为直接引用，解析前提是 “编译期可知，运行期不可变” 。 对应的调用字节码指令：invokestatic、invokespecial、invokevirtual、invokeinterface、invokedynamic。 符合条件：静态方法、私有方法、实例构造器、父类方法、final修饰方法。 分派静态分派所有依赖静态类型来定位方法执行版本的分派动作称为静态分派。(方法重载) 静态类型在编译期可知。 实际类型变化的结果在运行期确定。 重载方法匹配优先级以’a’为例： char-&gt;int-&gt;long-&gt;float-&gt;double java.lang.Character java.lang.Serializable、java.lang.Comparable 装箱转型为父类 变长参数 动态分派运行期根据实际类型确定方法执行版本的分派过程称为动态分派。(方法重写) 单分配与多分配方法宗量：方法接受者+方法参数。根据分派基于多少种宗量划分为单分配和多分配。 静态分派：选择目标方法。(静态类型+方法参数) 动态分派：方法接受者的实际类型。 Java是静态多分配、动态单分配的语言。 高效并发 Java内存模型与线程Java内存模型 主内存与工作内存Java内存模型的目的是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量的底层细节。 这里的变量与Java编程中所说的变量不同，它包括实例字段、静态字段、构成数组对象的元素，不包括局部变量和方法参数。 Java内存模型规定 所有变量都存储在主内存。 每天线程有自己的工作内存。 工作内存保存主内存的副本拷贝。 线程对变量的所有操作（读取、赋值等）都在工作内存中进行。 主内存对应于Java堆中的对象实例数据部分。工作内存对应虚拟机栈中的部分区域。 内存间交互操作 lock：把一个变量标识为一条线程独占状态。 unlock：释放锁定变量。 read：把变量从主内存读取到工作线程。 load：把read操作读取的变量值放入工作内存变量副本中。 use：把工作内存的变量值传递给执行引擎。 assign：从执行引擎接收变量值到工作线程。 store：把工作线程的变量值传到主内存中。 write：把store操作得到的变量值放入主内存的变量中。 原子性、可见性、有序性 原子性：线程从运行开始会一直到运行结束，不会被方法调度打断或进行线程切换。 可见性：当一个线程修改了共享变量的值，其他线程能够立即得到这个修改。 有序性：禁止指令重排序，在本线程中表现为串行，整体表现为指令重排序。 volatile关键字 保证了可见性和有序性。 Java运算并非原子操作，导致volatile变量的运算在并发下不安全。 需要确保运算结果并不依赖变量的当前值来保证原子性。 synchronized同步块同时保证了原子性、可见性、有序性。 先行发生原则先行发生是Java内存模型中定义的两项操作之间的偏序关系。如果操作A先行于操作B，则操作A产生的影响能够被操作B观察到。 Java内存模型预定义的先行发生关系 程序次序规则：一个线程中，按照程序代码顺序。 管程锁定规则：unlock操作先行于同一个锁的lock操作。 volatile变量规则：volatile变量的写操作先行于后面的读操作。 线程启动规则：Thread对象的start()方法优先。 线程终止规则：Thread.join()方法结束最后。 线程中断规则：interrupt()方法先行于中断事件。 对象终结规则：一个对象的初始化完成先行于finalize()方法。 传递性：A先行于B，B先行于C，则A先行于C。 时间先后顺序与先行发生规则基本没有关系。 Java与线程线程的实现各个线程既可以共享进程资源，又可以独立调度。 实现线程的方式 使用内核线程实现：直接由操作系统内核支持的线程，用内核线程支持 轻量级进程(LWP) 实现。 使用用户线程实现：用户线程(UT) 完全建立在用户空间的线程库，不需要切换到内核态。 混合实现：既存在用户线程，也存在轻量级进程。 Java线程调度线程调度指系统为线程分配处理器的使用权。 协同式调度：线程的执行时间由自己控制，线程自身执行完后主动通知系统切换到另一个线程。 抢占式调度：每个线程由系统分配执行时间，可以设置线程优先级。 状态转换 新建(New)：创建后尚未启动的线程。 运行(Runable)：包括Running和Ready。 无限期等待(Wating)：等待其他线程显式唤醒。 限期等待(Timed Wating)：一定时间后由系统自动唤醒。 阻塞(Blocked)：等待获取一个排它锁，在另一个线程放弃这个锁时发生。 结束(Terminated)：已终止线程。 线程安全与锁优化线程安全线程安全指当多个线程访问一个对象时，调用这个对象的行为都可以像单线程一样得到正确的结果。 共享数据类型 不可变：不可变的对象一定是线程安全的，如String。 绝对线程安全：在多线程环境中需要在方法调用端做额外的同步措施。 相对线程安全：需要保证这个对象单独的操作是线程安全的，如：Vector、HashTable等。 线程兼容：对象本身并不是线程安全的，可以通过在调用端正确使用同步手段保证线程安全，如ArrayList、HashMap等。 线程对立：无论调用端是否采用同步措施，都无法在多线程环境中并发使用。 线程安全的实现 互斥同步：synchronized或java.util.concurrent包中ReentrantLock实现。 非阻塞同步：先进行操作，产生了冲突再采取补偿措施，也称为“乐观锁”。 无同步方案：可重入代码、线程本地存储。 锁优化 自旋锁：不放弃处理器的执行时间，让线程执行一个忙循环（自旋）。 锁消除：对一些代码要求同步却被检测到不可能存在共享数据竞争的锁进行消除。 锁粗化：如果一系列的连续操作都对同一个对象反复加锁和解锁，则将加锁同步范围扩展到整个操作序列的外部。 轻量级锁 偏向锁","link":"/2017/12/09/Learning-Java-Virtual-Machine(JVM)/"},{"title":"跟我学Shiro（六）-Realm及相关对象","text":"Realm 定义实体及关系 用户-角色之间是多对多关系，角色-权限之间是多对多关系；且用户和权限之间通过角色建立关系。在系统中验证时通过权限验证，角色只是权限集合，即所谓的显式角色。 用户实体包括：编号id、用户名username、密码password、盐salt、是否锁定locked； 角色实体包括：、编号id、角色标识符role、描述description、是否可用available； 权限实体包括：编号id、权限标识符permission、描述description、是否可用available。 另外还有两个关系实体：用户-角色实体：用户编号、角色编号，且组合为复合主键）；角色-权限实体：角色编号、权限编号，且组合为复合主键）。 环境准备为了方便数据库操作，使用 org.springframework: spring-jdbc: 4.0.0.RELEASE 依赖。 定义Service及Dao 为了实现的简单性，只实现必须的功能，其他的可以自己实现即可。 PermissionService实现基本的创建/删除权限。 public interface PermissionService { public Permission createPermission(Permission permission); public void deletePermission(Long permissionId); } RoleService相对于PermissionService多了关联/移除关联角色-权限功能。 public interface RoleService { public Role createRole(Role role); public void deleteRole(Long roleId); // 添加角色-权限之间关系 public void correlationPermissions(Long roleId, Long... permissionIds); // 移除角色-权限之间关系 public void uncorrelationPermissions(Long roleId, Long... permissionIds); } UserService使用 findByUsername 、 findRoles 及 findPermissions 来查找用户名对应的帐号、角色及权限信息。 之后的Realm就使用这些方法来查找相关信息。 public interface UserService { // 创建账户 public User createUser(User user); // 修改密码 public void changePassword(Long userId, String newPassword); // 添加用户-角色关系 public void correlationRoles(Long userId, Long... roleIds); // 移除用户-角色关系 public void uncorrelationRoles(Long userId, Long... roleIds); // 根据用户名查找用户 public User findByUsername(String username); // 根据用户名查找其角色 public Set&lt;String&gt; findRoles(String username); // 根据用户名查找其权限 public Set&lt;String&gt; findPermissions(String username); } UserServiceImpl在创建账户及修改密码时直接把生成密码操作委托给 PasswordHelper。 public User createUser(User user) { // 加密密码 passwordUtils.encryptPassword(user); return userDao.createUser(user); } public void changePassword(Long userId, String newPassword) { User user =userDao.findOne(userId); user.setPassword(newPassword); passwordUtils.encryptPassword(user); userDao.updateUser(user); } PasswordUtils之后的CredentialsMatcher需要和此处加密的算法一样。user.getCredentialsSalt()辅助方法返回username+salt。 public class PasswordUtils { private RandomNumberGenerator randomNumberGenerator = new SecureRandomNumberGenerator(); private String algorithmName = &quot;md5&quot;; private final int hashIterations = 2; public void encryptPassword(User user) { user.setSalt(randomNumberGenerator.nextBytes().toHex()); String newPassword = new SimpleHash( algorithmName, user.getPassword(), ByteSource.Util.bytes(user.getCredentialsSalt()), hashIterations).toHex(); user.setPassword(newPassword); } } 为了节省篇幅，对于DAO/Service的接口及实现，具体请参考源码 com.github.gojay001 ；另外参考Service层的测试用例 com.github.gojay001.service.ServiceTest 。 定义RealmRetryLimitHashedCredentialsMatcher com.github.gojay001.credentials public class RetryLimitHashedCredentialsMatcher extends HashedCredentialsMatcher { private Ehcache passwordRetryCache; public RetryLimitHashedCredentialsMatcher() { CacheManager cacheManager = CacheManager.newInstance(CacheManager.class.getClassLoader().getResource(&quot;ehcache.xml&quot;)); passwordRetryCache = cacheManager.getCache(&quot;passwordRetryCache&quot;); } @Override public boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info) { String username = (String)token.getPrincipal(); // retry count + 1 Element element = passwordRetryCache.get(username); if(element == null) { element = new Element(username , new AtomicInteger(0)); passwordRetryCache.put(element); } AtomicInteger retryCount = (AtomicInteger)element.getObjectValue(); if(retryCount.incrementAndGet() &gt; 5) { // if retry count &gt; 5 throw throw new ExcessiveAttemptsException(); } boolean matches = super.doCredentialsMatch(token, info); if(matches) { // clear retry count passwordRetryCache.remove(username); } return matches; }} UserRealm com.github.gojay001.realm public class UserRealm extends AuthorizingRealm { private UserService userService = new UserServiceImpl(); @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) { String username = (String)principals.getPrimaryPrincipal(); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); authorizationInfo.setRoles(userService.findRoles(username)); authorizationInfo.setStringPermissions(userService.findPermissions(username)); return authorizationInfo; } @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { String username = (String)token.getPrincipal(); User user = userService.findByUsername(username); //没找到帐号 if(user == null) { throw new UnknownAccountException(); } //帐号锁定 if(Boolean.TRUE.equals(user.getLocked())) { throw new LockedAccountException(); } // 交给AuthenticatingRealm使用CredentialsMatcher进行密码匹配，如果觉得人家的不好可以自定义实现 return new SimpleAuthenticationInfo( user.getUsername(), user.getPassword(), // salt=username+salt ByteSource.Util.bytes(user.getCredentialsSalt()), //realm name getName() ); }} UserRealm父类AuthorizingRealm将获取Subject相关信息分成两步：获取身份验证信息 doGetAuthenticationInfo 及授权信息 doGetAuthorizationInfo ； doGetAuthenticationInfo：首先根据传入的用户名获取User信息；在组装SimpleAuthenticationInfo信息时，需要传入：身份信息用户名、凭据密文密码、盐username+salt， CredentialsMatcher 使用盐加密传入的明文密码和此处的密文密码进行匹配。 doGetAuthorizationInfo：PrincipalCollection 是一个身份集合，因为我们现在就一个Realm，所以直接调用getPrimaryPrincipal得到之前传入的用户名即可；然后根据用户名调用UserService接口获取角色及权限信息。 AuthenticationToken AuthenticationToken用于收集用户提交的身份 用户名 及凭据 密码 ： public interface AuthenticationToken extends Serializable { Object getPrincipal(); Object getCredentials(); } RememberMeAuthenticationToken：提供了 boolean isRememberMe() 记住我的功能； HostAuthenticationToken：提供了 String getHost() 方法用于获取用户主机的功能。 Shiro提供了一个直接拿来用的UsernamePasswordToken，用于实现用户名/密码Token组。实现了 RememberMeAuthenticationToken 和 HostAuthenticationToken ，可以实现记住我及主机验证的支持。 AuthenticationInfo AuthenticationInfo有两个作用： 如果Realm是 AuthenticatingRealm 子类，则提供给 AuthenticatingRealm 内部使用的 CredentialsMatcher 进行凭据验证；（如果没有继承它需要在自己的Realm中自己实现验证）。 提供给 SecurityManager 来创建 Subject （提供身份信息）。 MergableAuthenticationInfo用于提供在多Realm时合并AuthenticationInfo的功能，主要合并Principal。比如 HashedCredentialsMatcher ，在验证时会判断 AuthenticationInfo 是否是SaltedAuthenticationInfo子类，来获取盐信息。Account相当于我们之前的 User ， SimpleAccount 是其一个实现。其他情况一般返回SimpleAuthenticationInfo即可。 PrincipalCollection 因为我们可以在Shiro中同时配置多个Realm，所以身份信息可能就有多个；因此其提供了PrincipalCollection用于聚合这些身份信息： public interface PrincipalCollection extends Iterable, Serializable { // 得到主要的身份 Object getPrimaryPrincipal(); // 根据身份类型获取第一个 &lt;T&gt; T oneByType(Class&lt;T&gt; type); // 根据身份类型获取一组 &lt;T&gt; Collection&lt;T&gt; byType(Class&lt;T&gt; type); // 转换为List List asList(); // 转换为Set Set asSet(); // 根据Realm名字获取 Collection fromRealm(String realmName); // 获取所有身份验证通过的Realm名字 Set&lt;String&gt; getRealmNames(); // 判断是否为空 boolean isEmpty(); } getPrimaryPrincipal：如果只有一个Principal那么直接返回即可，如果有多个Principal，则返回第一个（因为内部使用Map存储，所以可以认为是返回任意一个）；oneByType/byType：根据凭据的类型返回相应的Principal；fromRealm根据Realm名字（每个Principal都与一个Realm关联）获取相应的Principal。 MutablePrincipalCollection是一个可变的PrincipalCollection接口，即提供了如下可变方法： public interface MutablePrincipalCollection extends PrincipalCollection { // 添加Realm-Principal的关联 void add(Object principal, String realmName); // 添加一组Realm-Principal的关联 void addAll(Collection principals, String realmName); // 添加PrincipalCollection void addAll(PrincipalCollection principals); // 清空 void clear();} AuthorizationInfo AuthorizationInfo用于聚合授权信息的： public interface AuthorizationInfo extends Serializable { // 获取角色字符串信息 Collection&lt;String&gt; getRoles(); // 获取权限字符串信息 Collection&lt;String&gt; getStringPermissions(); // 获取Permission对象信息 Collection&lt;Permission&gt; getObjectPermissions(); } 当我们使用 AuthorizingRealm 时，如果身份验证成功，在进行授权时就通过 doGetAuthorizationInfo 方法获取角色/权限信息用于授权验证。 Shiro提供了一个实现 SimpleAuthorizationInfo ，大多数时候使用这个即可。 Subject Subject是Shiro的核心对象，基本所有身份验证、授权都是通过Subject完成。 身份信息获取// Primary PrincipalObject getPrincipal(); // PrincipalCollectionPrincipalCollection getPrincipals(); 身份验证void login(AuthenticationToken token) throws AuthenticationException; boolean isAuthenticated(); boolean isRemembered(); 角色授权验证boolean hasRole(String roleIdentifier); boolean[] hasRoles(List&lt;String&gt; roleIdentifiers); boolean hasAllRoles(Collection&lt;String&gt; roleIdentifiers); void checkRole(String roleIdentifier) throws AuthorizationException; void checkRoles(Collection&lt;String&gt; roleIdentifiers) throws AuthorizationException; void checkRoles(String... roleIdentifiers) throws AuthorizationException; 权限授权验证boolean isPermitted(String permission); boolean isPermitted(Permission permission); boolean[] isPermitted(String... permissions); boolean[] isPermitted(List&lt;Permission&gt; permissions); boolean isPermittedAll(String... permissions); boolean isPermittedAll(Collection&lt;Permission&gt; permissions); void checkPermission(String permission) throws AuthorizationException; void checkPermission(Permission permission) throws AuthorizationException; void checkPermissions(String... permissions) throws AuthorizationException; void checkPermissions(Collection&lt;Permission&gt; permissions) throws AuthorizationException; 会话// 相当于getSession(true)Session getSession(); Session getSession(boolean create); 如果 create=false 如果没有会话将返回null；而 create=true 如果没有会话会强制创建一个。 退出void logout(); RunAsvoid runAs(PrincipalCollection principals) throws NullPointerException, IllegalStateException; boolean isRunAs(); PrincipalCollection getPreviousPrincipals(); PrincipalCollection releaseRunAs(); RunAs即实现允许A假设为B身份进行访问： 通过调用 subject.runAs(b) 进行访问；接着调用 subject.getPrincipals 将获取到B的身份；此时调用 isRunAs 将返回true，而a的身份需要通过 subject.getPreviousPrincipals 获取；如果不需要RunAs了调用 subject.releaseRunAs 即可。 多线程&lt;V&gt; V execute(Callable&lt;V&gt; callable) throws ExecutionException; void execute(Runnable runnable); &lt;V&gt; Callable&lt;V&gt; associateWith(Callable&lt;V&gt; callable); Runnable associateWith(Runnable runnable); 在多线程执行中需要传播到相应的线程才能获取到相应的Subject。最简单的办法就是通过 execute(runnable/callable实例) 直接调用；或者通过 associateWith(runnable/callable实例) 得到一个包装后的实例；它们都是通过：把当前线程的Subject绑定过去；在线程执行结束后自动释放。 Subject自己不会实现相应的身份验证/授权逻辑，而是通过DelegatingSubject委托给SecurityManager实现。 如果想自定义创建，可以通过： new Subject.Builder().principals(身份).authenticated(true/false).buildSubject() Subject一般流程 身份验证（login） 授权（hasRole*/isPermitted*或checkRole*/checkPermission*） 将相应的数据存储到会话（Session） 切换身份（RunAs）/多线程身份传播 退出 必须的功能就是1、2、5。到目前为止我们就可以使用Shiro进行应用程序的安全控制了，但是还是缺少如对Web验证、Java方法验证等的一些简化实现。 总结 Realm Permission Role User User-Role Role-Permission AuthenticationToken Principal Credentials RemeberMeAuthenticationToken HostAuthenticationToken UsernamePasswordToken AuthenticationInfo 提供身份信息 提供凭据验证 SimpleAuthenticationInfo PrincipalCollection Principal MutablePrincipalCollection PrincipalMap AuthorizationInfo Roles StringPermissions ObjectPermissions SimpleAuthorizationInfo Subject 身份获取 身份验证 角色授权 权限授权 会话 退出 RunAs 多线程 参考代码： https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter6","link":"/2017/11/30/跟我学Shiro（六）-Realm及相关对象/"},{"title":"跟我学Shiro（五）-编码及加密","text":"在涉及到密码存储问题上，应该加密/生成密码摘要存储，而不是存储明文密码。 编码/解码Shiro提供了 base64 和 16进制字符串 编码/解码的API支持，方便一些编码解码操作。Shiro内部的一些数据的存储/表示都使用了base64和16进制字符串。 String str = &quot;hello&quot;; String base64Encoded = Base64.encodeToString(str.getBytes()); String str2 = Base64.decodeToString(base64Encoded); Assert.assertEquals(str, str2); 通过如上方式可以进行base64编码/解码操作。 String str = &quot;hello&quot;; String base64Encoded = Hex.encodeToString(str.getBytes()); String str2 = new String(Hex.decode(base64Encoded.getBytes())); Assert.assertEquals(str, str2); 通过如上方式可以进行16进制字符串编码/解码操作。 还有一个可能经常用到的类CodecSupport，提供了toBytes(str, &quot;utf-8&quot;) / toString(bytes, &quot;utf-8&quot;)用于在byte数组/String之间转换。 散列（Hash）算法散列算法一般用于生成数据的摘要信息，是一种不可逆的算法，一般适合存储密码之类的数据，常见的散列算法如MD5、SHA等。 一般进行散列时最好提供一个salt（盐），如用户名和ID（即盐）；这样散列的对象是“密码+用户名+ID”，这样生成的散列值相对来说更难破解。 MD5String str = &quot;hello&quot;; String salt = &quot;123&quot;; String md5 = new Md5Hash(str, salt).toString();//还可以转换为 toBase64()/toHex() 如上代码使用MD5算法通过盐“123”生成MD5散列。另外散列时还可以指定散列次数，如2次表示：md5(md5(str))：new Md5Hash(str, salt, 2).toString()。 SHAString str = &quot;hello&quot;; String salt = &quot;123&quot;; String sha1 = new Sha256Hash(str, salt).toString(); 使用SHA256算法生成相应的散列数据，另外还有如SHA1、SHA512算法。 通用的散列支持String str = &quot;hello&quot;; String salt = &quot;123&quot;; //内部使用MessageDigest String simpleHash = new SimpleHash(&quot;SHA-1&quot;, str, salt).toString(); 通过调用 SimpleHash 时指定散列算法，其内部使用了Java的 MessageDigest 实现。 为了方便使用，Shiro提供了 HashService ，默认提供了 DefaultHashService 实现。 // 默认算法SHA-512DefaultHashService hashService = new DefaultHashService(); hashService.setHashAlgorithmName(&quot;SHA-512&quot;);// 私盐，默认无hashService.setPrivateSalt(new SimpleByteSource(&quot;123&quot;)); // 是否生成公盐，默认falsehashService.setGeneratePublicSalt(true);// 用于生成公盐。默认就这个hashService.setRandomNumberGenerator(new SecureRandomNumberGenerator());// 生成Hash值的迭代次数hashService.setHashIterations(1); HashRequest request = new HashRequest.Builder() .setAlgorithmName(&quot;MD5&quot;).setSource(ByteSource.Util.bytes(&quot;hello&quot;)) .setSalt(ByteSource.Util.bytes(&quot;123&quot;)).setIterations(2).build(); String hex = hashService.computeHash(request).toHex(); 首先创建一个 DefaultHashService ，默认使用SHA-512算法； 可以通过 hashAlgorithmName 属性修改算法； 可以通过 privateSalt 设置一个私盐，其在散列时自动与用户传入的公盐混合产生一个新盐； 可以通过 generatePublicSalt 属性在用户没有传入公盐的情况下是否生成公盐； 可以设置 randomNumberGenerator 用于生成公盐； 可以设置 hashIterations 属性来修改默认加密迭代次数； 需要构建一个 HashRequest ，传入算法、数据、公盐、迭代次数。 SecureRandomNumberGenerator用于生成一个随机数： SecureRandomNumberGenerator randomNumberGenerator = new SecureRandomNumberGenerator(); randomNumberGenerator.setSeed(&quot;123&quot;.getBytes()); String hex = randomNumberGenerator.nextBytes().toHex(); 加密/解密Shiro提供对称式加密/解密算法的支持，如AES、Blowfish等。当前还没有提供对非对称加密/解密算法支持，未来版本可能提供。 AES算法实现AesCipherService aesCipherService = new AesCipherService();// 设置key长度aesCipherService.setKeySize(128); //生成key Key key = aesCipherService.generateNewKey(); String text = &quot;hello&quot;; //加密 String encrptText = aesCipherService.encrypt(text.getBytes(), key.getEncoded()).toHex(); //解密 String text2 = new String(aesCipherService.decrypt(Hex.decode(encrptText), key.getEncoded()).getBytes()); Assert.assertEquals(text, text2); PasswordService/CredentialsMatcherShiro提供了 PasswordService 及 CredentialsMatcher 用于提供加密密码及验证密码服务。 public interface PasswordService { // 输入明文密码得到密文密码 String encryptPassword(Object plaintextPassword) throws IllegalArgumentException; } public interface CredentialsMatcher { // 匹配用户输入的token的凭证（未加密）与系统提供的凭证（已加密） boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info); } Shiro默认提供了PasswordService实现DefaultPasswordService；CredentialsMatcher实现PasswordMatcher及HashedCredentialsMatcher（更强大）。 DefaultPasswordService配合PasswordMatcher实现简单的密码加密与验证服务定义Realm com.github.gojay001.relam.MyRealm public class MyRealm extends AuthorizingRealm { private PasswordService passwordService; public void setPasswordService(PasswordService passwordService) { this.passwordService = passwordService; } // 省略doGetAuthorizationInfo，具体看代码 @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { return new SimpleAuthenticationInfo( &quot;root&quot;, passwordService.encryptPassword(&quot;root&quot;), getName()); } } 为了方便，直接注入一个passwordService来加密密码；实际使用时需要在Service层使用passwordService加密密码并存储到数据库。 INI配置 shiro-passwordservice.ini [main] passwordService=org.apache.shiro.authc.credential.DefaultPasswordService hashService=org.apache.shiro.crypto.hash.DefaultHashService passwordService.hashService=$hashService hashFormat=org.apache.shiro.crypto.hash.format.Shiro1CryptFormat passwordService.hashFormat=$hashFormat hashFormatFactory=org.apache.shiro.crypto.hash.format.DefaultHashFormatFactory passwordService.hashFormatFactory=$hashFormatFactory passwordMatcher=org.apache.shiro.authc.credential.PasswordMatcher passwordMatcher.passwordService=$passwordService myRealm=com.github.gojay001.realm.MyRealm myRealm.passwordService=$passwordService myRealm.credentialsMatcher=$passwordMatcher securityManager.realms=$myRealm passwordService使用DefaultPasswordService，如果有必要也可以自定义； hashService定义散列密码使用的HashService，默认使用DefaultHashService（默认SHA-256算法）； hashFormat用于对散列出的值进行格式化，默认使用Shiro1CryptFormat，另外提供了Base64Format和HexFormat，对于有salt的密码请自定义实现ParsableHashFormat然后把salt格式化到散列值中； hashFormatFactory用于根据散列值得到散列的密码和salt；因为如果使用如SHA算法，那么会生成一个salt，此salt需要保存到散列后的值中以便之后与传入的密码比较时使用；默认使用DefaultHashFormatFactory； passwordMatcher使用PasswordMatcher，其是一个CredentialsMatcher实现； 将credentialsMatcher赋值给myRealm，myRealm间接继承了AuthenticatingRealm，其在调用getAuthenticationInfo方法获取到AuthenticationInfo信息后，会使用credentialsMatcher来验证凭据是否匹配，如果不匹配将抛出IncorrectCredentialsException异常。 测试用例参考： com.github.gojay001.test.PasswordTest ，包含JdbcRealm测试用例。缺点：salt保存在散列值中，没有实现如密码重试次数限制。 HashedCredentialsMatcher实现密码验证服务Shiro提供了CredentialsMatcher的散列实现HashedCredentialsMatcher，和之前的PasswordMatcher不同的是，它只用于密码验证，且可以提供自己的盐，而不是随机生成盐，且生成密码散列值的算法需要自己写，因为能提供自己的盐。 生成密码散列值此处我们使用MD5算法，“密码+盐（用户名+随机数）”的方式生成散列值： String algorithmName = &quot;md5&quot;; String username = &quot;root&quot;; String password = &quot;root&quot;; String salt1 = username; String salt2 = new SecureRandomNumberGenerator().nextBytes().toHex(); int hashIterations = 2; SimpleHash hash = new SimpleHash(algorithmName, password, salt1 + salt2, hashIterations); String encodedPassword = hash.toHex(); 如果要写用户模块，需要在新增用户/重置密码时使用如上算法保存密码，将生成的密码及salt2存入数据库。因为我们的散列算法是：md5(密码+username+salt2)。 生成Realm自定义Realm com.github.gojay001.realm.MyRealm2 protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException { // 用户名及salt1 String username = &quot;liu&quot;; // 加密后的密码 String password = &quot;202cb962ac59075b964b07152d234b70&quot;; String salt2 = &quot;202cb962ac59075b964b07152d234b70&quot;; SimpleAuthenticationInfo ai = new SimpleAuthenticationInfo(username, password, getName()); // 盐是用户名+随机数 ai.setCredentialsSalt(ByteSource.Util.bytes(username+salt2)); return ai; } 此处就是把步骤1中生成的相应数据组装为 SimpleAuthenticationInfo ，通过 SimpleAuthenticationInfo 的 credentialsSalt 设置盐， HashedCredentialsMatcher 会自动识别这个盐。 JdbcRealm需要修改获取用户信息（包括盐）的sql： “select password, password_salt from users where username = ?” ；而我们的盐是由 username+password_salt 组成，所以需要通过如下ini配置（shiro-jdbc-hashedCredentialsMatcher.ini）修改： jdbcRealm.saltStyle=COLUMN jdbcRealm.authenticationQuery=select password, concat(username,password_salt) from users where username = ? jdbcRealm.credentialsMatcher=$credentialsMatcher saltStyle表示使用密码+盐的机制，authenticationQuery第一列是密码，第二列是盐； 通过 authenticationQuery 指定密码及盐查询SQL。 INI配置 shiro-hashedCredentialsMatcher.ini [main] credentialsMatcher=org.apache.shiro.authc.credential.HashedCredentialsMatcher credentialsMatcher.hashAlgorithmName=md5 credentialsMatcher.hashIterations=2 credentialsMatcher.storedCredentialsHexEncoded=true myRealm=com.github.gojay001.realm.MyRealm2 myRealm.credentialsMatcher=$credentialsMatcher securityManager.realms=$myRealm 通过 credentialsMatcher.hashAlgorithmName=md5 指定散列算法为md5，需要和生成密码时的一样； credentialsMatcher.hashIterations=2 ，散列迭代次数，需要和生成密码时的意义； credentialsMatcher.storedCredentialsHexEncoded=true 表示是否存储散列后的密码为16进制，需要和生成密码时的一样，默认是base64； 此处最需要注意的就是 HashedCredentialsMatcher 的算法需要和生成密码时的算法一样。另外HashedCredentialsMatcher会自动根据 AuthenticationInfo 的类型是否是 SaltedAuthenticationInfo 来获取credentialsSalt盐。 密码重试次数限制如在1个小时内密码最多重试5次，如果尝试次数超过5次就锁定1小时，1小时后可再次重试，如果还是重试失败，可以锁定如1天，以此类推，防止密码被暴力破解。我们通过继承HashedCredentialsMatcher，且使用Ehcache记录重试次数和超时时间。 com.github.gojay001.credentials.RetryLimitHashedCredentialsMatcher public boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info) { String username = (String)token.getPrincipal(); //retry count + 1 Element element = passwordRetryCache.get(username); if(element == null) { element = new Element(username , new AtomicInteger(0)); passwordRetryCache.put(element); } AtomicInteger retryCount = (AtomicInteger)element.getObjectValue(); if(retryCount.incrementAndGet() &gt; 5) { //if retry count &gt; 5 throw throw new ExcessiveAttemptsException(); } boolean matches = super.doCredentialsMatch(token, info); if(matches) { //clear retry count passwordRetryCache.remove(username); } return matches; } 如上代码逻辑比较简单，即如果密码输入正确，清除cache中的记录；否则cache中的重试次数+1，如果超出5次那么抛出异常表示超出重试次数了。 总结编码/解码 Base64 Hex Hash() 加密/解密 对称式加密/解密 加密/验证PasswordService DefaultPasswordService CredentialsMatcher PasswordMatcher HashedCredentialsMatcher DefaultPasswordService配合PasswordMatcherRealm 自定义Realm JdbcRealm ini配置 passwordService hashService hashFormat hashFormatFactory passwordMatcher myRealm HashedCredentialsMatcher生成Realm 使用MD5算法 ini配置 credentialsMatcher hashAlgorithmName hashIterations myRealm 添加密码重试次数限制 记录重试次数 参考代码： https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter5","link":"/2017/11/29/跟我学Shiro（五）-编码及加密/"},{"title":"跟我学Shiro（四）-INI配置","text":"SecurityManagerShiro是从根对象 SecurityManager 进行身份验证和授权的，这个对象是线程安全且真个应用只需要一个即可，因此Shiro提供了 SecurityUtils 让我们绑定它为全局的，方便后续操作。 因为Shiro的类都是POJO的，因此都很容易放到任何IoC容器管理。但是和一般的IoC容器的区别在于，Shiro从根对象securityManager开始导航。Shiro支持的依赖注入：public空参构造器对象的创建、setter依赖注入。 纯Java代码写法 com.github.gojay001.test.NonConfigurationCreateTest DefaultSecurityManager securityManager = new DefaultSecurityManager();//设置authenticatorModularRealmAuthenticator authenticator = new ModularRealmAuthenticator();authenticator.setAuthenticationStrategy(new AtLeastOneSuccessfulStrategy());securityManager.setAuthenticator(authenticator);//设置authorizerModularRealmAuthorizer authorizer = new ModularRealmAuthorizer();authorizer.setPermissionResolver(new WildcardPermissionResolver());securityManager.setAuthorizer(authorizer);//设置RealmDruidDataSource ds = new DruidDataSource();ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;);ds.setUrl(&quot;jdbc:mysql://localhost:3306/shiro&quot;);ds.setUsername(&quot;root&quot;);ds.setPassword(&quot;root&quot;);JdbcRealm jdbcRealm = new JdbcRealm();jdbcRealm.setDataSource(ds);jdbcRealm.setPermissionsLookupEnabled(true);securityManager.setRealms(Arrays.asList((Realm) jdbcRealm));//将SecurityManager设置到SecurityUtils 方便全局使用SecurityUtils.setSecurityManager(securityManager);Subject subject = SecurityUtils.getSubject();UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;);subject.login(token);Assert.assertTrue(subject.isAuthenticated()); 等价的INI配置shiro-config.ini：[main]#覆盖默认的securityManager#securityManager=org.apache.shiro.mgt.DefaultSecurityManager#authenticatorauthenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticatorauthenticationStrategy=org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategyauthenticator.authenticationStrategy=$authenticationStrategysecurityManager.authenticator=$authenticator#authorizerauthorizer=org.apache.shiro.authz.ModularRealmAuthorizerpermissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolverauthorizer.permissionResolver=$permissionResolversecurityManager.authorizer=$authorizer#realmdataSource=com.alibaba.druid.pool.DruidDataSourcedataSource.driverClassName=com.mysql.jdbc.DriverdataSource.url=jdbc:mysql://localhost:3306/shirodataSource.username=rootdataSource.password=rootjdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealmjdbcRealm.dataSource=$dataSourcejdbcRealm.permissionsLookupEnabled=truesecurityManager.realms=$jdbcRealm 对象名=全限定类名 相对于调用public无参构造器创建对象对象名.属性名=值 相当于调用setter方法设置常量值对象名.属性名=$对象引用 相当于调用setter方法设置对象引用 com.github.gojay001.test.ConfigurationCreateTest：Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro-config.ini&quot;);SecurityManager securityManager = factory.getInstance();//将SecurityManager设置到SecurityUtils 方便全局使用SecurityUtils.setSecurityManager(securityManager);Subject subject = SecurityUtils.getSubject();UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;);subject.login(token);Assert.assertTrue(subject.isAuthenticated()); 如上代码是从Shiro INI配置中获取相应的securityManager实例： 默认情况先创建一个名字为 securityManager ，类型为 org.apache.shiro.mgt.DefaultSecurityManager 的默认的 SecurityManager ，如果想自定义，只需要在ini配置文件中指定“securityManager=SecurityManager实现类”即可，名字必须为securityManager，它是起始的根； IniSecurityManagerFactory 是创建 securityManager 的工厂，其需要一个ini配置文件路径，其支持classpath:（类路径）、file:（文件系统）、url:（网络）三种路径格式，默认是文件系统； 接着获取SecuriyManager实例，后续步骤和之前的一样。 如上可以看出Shiro INI配置方式本身提供了一个简单的IoC/DI机制方便在配置文件配置，但是是从 securityManager 这个根对象开始导航。 INI配置ini配置文件类似于Java中的 properties（key=value） ，不过提供了将key/value分类的特性，key是每个部分不重复即可，而不是整个配置文件。如下是INI配置分类： [main] #提供了对根对象securityManager及其依赖的配置 securityManager=org.apache.shiro.mgt.DefaultSecurityManager ………… securityManager.realms=$jdbcRealm [users] #提供了对用户/密码及其角色的配置，用户名=密码，角色1，角色2 username=password,role1,role2 [roles] #提供了角色及权限之间关系的配置，角色=权限1，权限2 role1=permission1,permission2 [urls] #用于web，提供了对web url拦截相关的配置，url=拦截器[参数]，拦截器 /index.html = anon /admin/** = authc, roles[admin], perms[&quot;permission1&quot;] [main]部分提供了对根对象 securityManager 及其依赖对象的配置。 创建对象securityManager=org.apache.shiro.mgt.DefaultSecurityManager 其构造器必须是public空参构造器，通过反射创建相应的实例。 常量值setter注入dataSource.driverClassName=com.mysql.jdbc.Driver jdbcRealm.permissionsLookupEnabled=true 会自动调用 jdbcRealm.setPermissionsLookupEnabled(true) ，对于这种常量值会自动类型转换。 对象引用setter注入authenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticator authenticationStrategy=org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategy authenticator.authenticationStrategy=$authenticationStrategy securityManager.authenticator=$authenticator 会自动通过 securityManager.setAuthenticator(authenticator) 注入引用依赖。 嵌套属性setter注入securityManager.authenticator.authenticationStrategy=$authenticationStrategy 支持这种嵌套方式的setter注入。 byte数组setter注入#base64 byte[] authenticator.bytes=aGVsbG8= #hex byte[] authenticator.bytes=0x68656c6c6f 默认需要使用Base64进行编码，也可以使用0x十六进制。 Array/Set/List setter注入authenticator.array=1,2,3 authenticator.set=$jdbcRealm,$jdbcRealm 多个之间通过“，”分割。 Map setter注入authenticator.map=$jdbcRealm:$jdbcRealm,1:1,key:abc 格式是： map=key：value，key：value ，可以注入常量及引用值，常量的话都看作字符串（即使有泛型也不会自动造型）。 实例化/注入顺序realm=Realm1 realm=Realm12 authenticator.bytes=aGVsbG8= authenticator.bytes=0x68656c6c6f 后边的覆盖前边的注入。 [users]部分配置用户名/密码及其角色，格式：用户名=密码，角色1，角色2，角色部分可省略。如： [users]root=root,role1,role2gojay=test 密码一般生成其摘要/加密存储。 [roles]部分配置角色及权限之间的关系，格式：角色=权限1，权限2；如： [roles] role1=user:create,user:update role2=* 如果只有角色没有对应的权限，可以不配roles。 [urls]部分配置url及相应的拦截器之间的关系，格式：url=拦截器[参数]，拦截器[参数]，如： [urls] /admin/** = authc, roles[admin], perms[&quot;permission1&quot;] 参考代码： https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter4","link":"/2017/11/28/跟我学Shiro（四）-INI配置/"},{"title":"跟我学Shiro（三）-授权","text":"简介授权：也叫访问控制，即在应用中控制谁能访问哪些资源（如访问页面/编辑数据/页面操作等）。在授权中需了解的几个关键对象：主体（Subject）、资源（Resource）、权限（Permission）、角色（Role）。 主体：即访问应用的用户，在Shiro中使用Subject代表该用户。用户只有授权后才允许访问相应的资源。 资源：在应用中用户可以访问的任何东西，比如访问JSP页面、查看/编辑某些数据、访问某个业务方法、打印文本等等都是资源。用户只要授权后才能访问。 权限：安全策略中的原子授权单位，通过权限我们可以表示在应用中用户有没有操作某个资源的权力。 角色：角色代表了操作集合，可以理解为权限的集合，一般情况下我们会赋予用户角色而不是权限，即这样用户可以拥有一组权限，赋予权限时比较方便。 隐式角色：即直接通过角色来验证用户有没有操作权限。 显式角色：在程序中通过权限控制谁能访问某个资源，角色聚合一组权限集合。 了解更多：搜索“RBAC”和“RBAC新解”分别了解“基于角色的访问控制”和“基于资源的访问控制”。 授权方式 Shiro支持三种方式的授权。 编程式通过写if/else授权代码块完成： Subject subject = SecurityUtils.getSubject(); if(subject.hasRole(“admin”)) { //有权限 } else { //无权限 } 注解式通过在执行的Java方法上放置相应的注解完成： @RequiresRoles(&quot;admin&quot;) public void hello() { //有权限 } 没有权限将抛出相应的异常。 JSP/GSP标签在JSP/GSP页面通过相应的标签完成： &lt;shiro:hasRole name=&quot;admin&quot;&gt; &lt;!— 有权限 —&gt; &lt;/shiro:hasRole&gt; 授权基于角色的访问控制（隐式角色）在ini配置文件配置用户拥有的角色 shiro-role.ini [users]root=root,role1,role2gojay=test,role1 规则：“用户名=密码,角色1，角色2”，如果需要在应用中判断用户是否有相应角色，就需要在相应的Realm中返回角色信息；也就是说Shiro不负责维护用户-角色信息，需要应用提供，Shiro只是提供相应的接口方便验证。 测试用例 com.github.gojay001.test.RoleTest @Testpublic void testHasRole() { login(&quot;classpath:shiro-role.ini&quot;, &quot;root&quot;, &quot;root&quot;); // 判断拥有角色：role1 Assert.assertTrue(subject().hasRole(&quot;role1&quot;)); // 判断拥有角色：role1 and role2 Assert.assertTrue(subject().hasAllRoles(Arrays.asList(&quot;role1&quot;, &quot;role2&quot;))); // 判断拥有角色：role1 and role2 and !role3 boolean[] result = subject().hasRoles(Arrays.asList(&quot;role1&quot;, &quot;role2&quot;, &quot;role3&quot;)); Assert.assertEquals(true, result[0]); Assert.assertEquals(true, result[1]); Assert.assertEquals(false, result[2]);} Shiro提供了hasRole/hasAllRoles用于判断用户是否拥有某个角色/某些权限；但是没有提供如hashAnyRole用于判断是否有某些权限中的某一个。 @Test(expected = UnauthorizedException.class)public void testCheckRole() { login(&quot;classpath:shiro-role.ini&quot;, &quot;root&quot;, &quot;root&quot;); // 断言拥有角色：role1 subject().checkRole(&quot;role1&quot;); // 断言拥有角色：role1 and role3 失败抛出异常 subject().checkRoles(&quot;role1&quot;, &quot;role3&quot;);} Shiro提供的checkRole/checkRoles和hasRole/hasAllRoles不同的地方是它在判断为假的情况下会抛出UnauthorizedException异常。 基于角色的访问控制（即隐式角色）的缺点：如果很多地方进行了角色判断，但是有一天不需要了那么就需要修改相应代码把所有相关的地方进行删除；这就是粗粒度造成的问题。 基于资源的访问控制（显式角色）在ini配置文件配置用户拥有的角色及角色-权限关系 shiro-permission.ini [users]root=root,role1,role2gojay=test,role1[roles]role1=user:create,user:updaterole2=user:create,user:delete 规则：“用户名=密码，角色1，角色2”“角色=权限1，权限2”，即首先根据用户名找到角色，然后根据角色再找到权限；即角色是权限集合；Shiro同样不进行权限的维护，需要我们通过Realm返回相应的权限信息。只需要维护“用户——角色”之间的关系即可。 测试用例 com.github.gojay001.test.PermissionTest @Testpublic void testIsPermitted() { login(&quot;classpath:shiro-permission.ini&quot;, &quot;root&quot;, &quot;root&quot;); // 判断拥有权限：user:create Assert.assertTrue(subject().isPermitted(&quot;user:create&quot;)); // 判断拥有权限：user:update and user:delete Assert.assertTrue(subject().isPermittedAll(&quot;user:update&quot;, &quot;user:delete&quot;)); // 判断没有权限：user:view Assert.assertFalse(subject().isPermitted(&quot;user:view&quot;));} Shiro提供了isPermitted和isPermittedAll用于判断用户是否拥有某个权限或所有权限，也没有提供如isPermittedAny用于判断拥有某一个权限的接口。 @Test(expected = UnauthorizedException.class)public void testCheckPermission() { login(&quot;classpath:shiro-permission.ini&quot;, &quot;root&quot;, &quot;root&quot;); // 断言拥有权限：user:create subject().checkPermission(&quot;user:create&quot;); // 断言拥有权限：user:delete and user:update subject().checkPermissions(&quot;user:delete&quot;, &quot;user:update&quot;); // 断言拥有权限：user:view 失败抛出异常 subject().checkPermissions(&quot;user:view&quot;);} checkPermissions失败的情况下会抛出UnauthorizedException异常。 基于资源的访问控制（显式角色），也可以叫基于权限的访问控制，这种方式的一般规则是“资源标识符：操作”，即是资源级别的粒度；这种方式的好处就是如果要修改基本都是一个资源级别的修改，不会对其他模块代码产生影响，粒度小。但是实现起来可能稍微复杂点，需要维护“用户——角色，角色——权限（资源：操作）”之间的关系。 Permission字符串通配符权限规则：“资源标识符：操作：对象实例ID” 即对哪个资源的哪个实例可以进行什么操作。其默认支持通配符权限字符串，“:”表示资源/操作/实例的分割；“,”表示操作的分割；“*”表示任意资源/操作/实例。 单个资源单个权限subject().checkPermissions(&quot;system:user:update&quot;); 用户拥有资源“system:user”的“update”权限。 单个资源多个权限ini配置文件： role41=system:user:update,system:user:delete 通过判断： subject().checkPermissions(&quot;system:user:update&quot;, &quot;system:user:delete&quot;); 可以简写为：&quot;system:user:update,delete&quot; 用户拥有资源“system:user”的“update”和“delete”权限。 单个资源全部权限ini配置： role51=&quot;system:user:create,update,delete,view&quot; 通过判断： subject().checkPermissions(&quot;system:user:create,delete,update:view&quot;); 可以简写为：system:user:* 用户拥有资源“system:user”的“create”、“update”、“delete”和“view”所有权限。 所有资源全部权限ini配置： role61=*:view 通过判断： subject().checkPermissions(&quot;user:view&quot;); 用户拥有所有资源的“view”所有权限。 实例级别的权限单个实例单个权限ini配置： role71=user:view:1 通过判断： subject().checkPermissions(&quot;user:view:1&quot;); 对资源user的1实例拥有view权限。 单个实例多个权限ini配置： role72=&quot;user:update,delete:1&quot; 通过判断： subject().checkPermissions(&quot;user:delete,update:1&quot;); subject().checkPermissions(&quot;user:update:1&quot;, &quot;user:delete:1&quot;); 对资源user的1实例拥有update、delete权限。 单个实例所有权限ini配置： role73=user:*:1 通过判断： subject().checkPermissions(&quot;user:update:1&quot;, &quot;user:delete:1&quot;, &quot;user:view:1&quot;); 对资源user的1实例拥有所有权限。 所有实例单个权限ini配置： role74=user:auth:* 通过判断： subject().checkPermissions(&quot;user:auth:1&quot;, &quot;user:auth:2&quot;); 对资源user的1实例拥有所有权限。 所有实例所有权限ini配置： role75=user:*:* 通过判断： subject().checkPermissions(&quot;user:view:1&quot;, &quot;user:auth:2&quot;); 对资源user的1实例拥有所有权限。 Shiro对权限字符串缺失部分的处理 如user:view 等价于 user:view:*；organization 等价于 organization:* 或者 organization:*:*。可以这么理解，这种方式实现了前缀匹配。 如user:* 可以匹配 user:delete；user:delete 可以匹配 user:delete:1；user:*:1 可以匹配 user:view:1；user 可以匹配 user:view 或 user:view:1等。即*可以匹配所有，不加*可以进行前缀匹配； 如*:view 不能匹配 system:user:view；需要使用 *:*:view；即后缀匹配必须指定前缀（多个冒号就需要多个*来匹配）。 WildcardPermission如下两种方式是等价的： subject().checkPermission(&quot;menu:view:1&quot;); subject().checkPermission(new WildcardPermission(&quot;menu:view:1&quot;)); 因此没什么必要的话使用字符串更方便。 性能问题通配符匹配方式比字符串匹配来说是更复杂的，因此需要花费更长时间，但是一般系统的权限不会太多，且可以配合缓存来提供其性能，如果这样性能还达不到要求我们可以实现位操作算法实现性能更好的权限匹配。另外实例级别的权限验证如果数据量太大也不建议使用，可能造成查询权限及匹配变慢。可以考虑比如在sql查询时加上权限字符串之类的方式在查询时就完成了权限匹配。 授权流程 流程如下： 首先调用Subject.isPermitted*/hasRole*接口，其会委托给SecurityManager，而SecurityManager接着会委托给Authorizer； Authorizer是真正的授权者；如果我们调用如isPermitted(“user:view”)，其首先会通过PermissionResolver把字符串转换成相应的Permission实例； 在进行授权之前，其会调用相应的Realm获取Subject相应的角色/权限用于匹配传入的角色/权限； Authorizer会判断Realm的角色/权限是否和传入的匹配，如果有多个Realm，会委托给ModularRealmAuthorizer进行循环判断，如果匹配如isPermitted/hasRole会返回true，否则返回false表示授权失败。 ModularRealmAuthorizer进行多Realm匹配流程： 首先检查相应的Realm是否实现了Authorizer； 如果实现了Authorizer，那么接着调用其相应的isPermitted*/hasRole*接口进行匹配； 如果有一个Realm匹配那么将返回true，否则返回false。 如果Realm进行授权的话，应该继承AuthorizingRealm，其流程是： 如果调用hasRole*，则直接获取AuthorizationInfo.getRoles()与传入的角色比较即可； 如果调用如isPermitted(“user:view”)，首先通过PermissionResolver将权限字符串转换成相应的Permission实例，默认使用WildcardPermissionResolver，即转换为通配符的WildcardPermission； 通过AuthorizationInfo.getObjectPermissions()得到Permission实例集合；通过AuthorizationInfo. getStringPermissions()得到字符串集合并通过PermissionResolver解析为Permission实例；然后获取用户的角色，并通过RolePermissionResolver解析角色对应的权限集合（默认没有实现，可以自己提供）； 接着调用Permission. implies(Permission p)逐个与传入的权限比较，如果有匹配的则返回true，否则false。 AuthorizerAuthorizer的职责是进行授权（访问控制），提供了相应的角色/权限判断接口。SecurityManager继承了Authorizer接口，且提供了ModularRealmAuthorizer用于多Realm时的授权匹配。PermissionResolver用于解析权限字符串到Permission实例。RolePermissionResolver用于根据角色解析相应的权限集合。 可以通过如下ini配置更改Authorizer实现： authorizer=org.apache.shiro.authz.ModularRealmAuthorizer securityManager.authorizer=$authorizer 设置ModularRealmAuthorizer的permissionResolver，其会自动设置到相应的Realm上（其实现了PermissionResolverAware接口），如： permissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolver authorizer.permissionResolver=$permissionResolver 设置ModularRealmAuthorizer的rolePermissionResolver，其会自动设置到相应的Realm上（其实现了RolePermissionResolverAware接口），如： rolePermissionResolver=com.github.gojay001.permission.MyRolePermissionResolver authorizer.rolePermissionResolver=$rolePermissionResolver 示例ini配置 shiro-jdbc-authorizer.ini [main]# 自定义authorizerauthorizer=org.apache.shiro.authz.ModularRealmAuthorizer# 自定义permissionResolver# permissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolverpermissionResolver=com.github.gojay001.permission.BitAndWildPermissionResolverauthorizer.permissionResolver=$permissionResolver# 自定义rolePermissionResolverrolePermissionResolver=com.github.gojay001.permission.MyRolePermissionResolverauthorizer.rolePermissionResolver=$rolePermissionResolversecurityManager.authorizer=$authorizer# 自定义realm 一定要放在securityManager.authorizer赋值之后# 因为调用setRealms会将realms设置给authorizer，并给各个Realm设置permissionResolver和rolePermissionResolverjdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealmdataSource=com.alibaba.druid.pool.DruidDataSourcedataSource.driverClassName=com.mysql.jdbc.DriverdataSource.url=jdbc:mysql://localhost:3306/shirodataSource.username=rootdataSource.password=rootjdbcRealm.dataSource=$dataSourcejdbcRealm.permissionsLookupEnabled=truesecurityManager.realms=$jdbcRealm 不能使用IniSecurityManagerFactory创建的IniRealm，因为其初始化顺序的问题可能造成后续的初始化Permission造成影响。 定义BitAndWildPermissionResolver及BitPermission BitPermission用于实现位移方式的权限，如规则是：权限字符串格式：+资源字符串+权限位+实例ID；以+开头中间通过+分割；权限：0 表示所有权限；1 新增（二进制：0001）、2 修改（二进制：0010）、4 删除（二进制：0100）、8 查看（二进制：1000）；如 +user+10 表示对资源user拥有修改/查看权限。 public class BitPermission implements Permission { private String resourceIdentify; private int permissionBit; private String instanceId; public BitPermission(String permissionString) { String[] array = permissionString.split(&quot;\\\\+&quot;); if (array.length &gt; 1) { resourceIdentify = array[1]; } if (StringUtils.isEmpty(resourceIdentify)) { resourceIdentify = &quot;*&quot;; } if (array.length &gt; 2) { permissionBit = Integer.valueOf(array[2]); } if (array.length &gt; 3) { instanceId = array[3]; } if (StringUtils.isEmpty(instanceId)) { instanceId = &quot;*&quot;; } } public boolean implies(Permission permission) { if(!(permission instanceof BitPermission)) { return false; } BitPermission other = (BitPermission) permission; if(!(&quot;*&quot;.equals(this.resourceIdentify) || this.resourceIdentify.equals(other.resourceIdentify))) { return false; } if(!(this.permissionBit ==0 || (this.permissionBit &amp; other.permissionBit) != 0)) { return false; } if(!(&quot;*&quot;.equals(this.instanceId) || this.instanceId.equals(other.instanceId))) { return false; } return true; } @Override public String toString() { return &quot;BitPermission{&quot; + &quot;resourceIdentify=&apos;&quot; + resourceIdentify + &apos;\\&apos;&apos; + &quot;, permissionBit=&quot; + permissionBit + &quot;, instanceId=&apos;&quot; + instanceId + &apos;\\&apos;&apos; + &apos;}&apos;; }} Permission接口提供了boolean implies(Permission p)方法用于判断权限匹配的； public class BitAndWildPermissionResolver implements PermissionResolver { @Override public Permission resolvePermission(String permissionString) { if(permissionString.startsWith(&quot;+&quot;)) { return new BitPermission(permissionString); } return new WildcardPermission(permissionString); } } BitAndWildPermissionResolver实现了PermissionResolver接口，并根据权限字符串是否以“+”开头来解析权限字符串为BitPermission或WildcardPermission。 定义MyRolePermissionResolverRolePermissionResolver用于根据角色字符串来解析得到权限集合。 public class MyRolePermissionResolver implements RolePermissionResolver { @Override public Collection&lt;Permission&gt; resolvePermissionsInRole(String roleString) { if(&quot;role1&quot;.equals(roleString)) { return Arrays.asList((Permission)new WildcardPermission(&quot;menu:*&quot;)); } return null; } } 自定义Realm使用JdbcRealm，需要做的操作如下： 执行sql/shiro-init-data.sql 插入相关的权限数据； 使用shiro-jdbc-authorizer.ini配置文件，需要设置jdbcRealm.permissionsLookupEnabled为true来开启权限查询。 这里也可以自定义实现Realm，可参考com.github.gojay001.realm.MyRealm 测试用例@Testpublic void testIsPermitted2() { login(&quot;classpath:shiro-jdbc-authorizer.ini&quot;, &quot;root&quot;, &quot;root&quot;); // 判断拥有权限：user:create Assert.assertTrue(subject().isPermitted(&quot;user1:update&quot;)); Assert.assertTrue(subject().isPermitted(&quot;user2:update&quot;)); // 通过二进制位的方式表示权限 Assert.assertTrue(subject().isPermitted(&quot;+user1+2&quot;));// 新增权限 Assert.assertTrue(subject().isPermitted(&quot;+user1+8&quot;));// 查看权限 Assert.assertTrue(subject().isPermitted(&quot;+user2+10&quot;));// 新增及查看 Assert.assertFalse(subject().isPermitted(&quot;+user1+4&quot;));// 没有删除权限 Assert.assertTrue(subject().isPermitted(&quot;menu:view&quot;));// 通过MyRolePermissionResolver解析得到的权限} 总结授权流程 Subject.isPermitted*/hasRole* SecurityManager Authorizer PermissionResolver/RolePermissionResolver/Permission Realm Realm user role permission Security Authorizer PermissionResolver RolePermissionResolver Realm Subject hasRole/hasAllRoles checkRole/checkRoles isPermitted/isPermittedAll checkPermission/checkPermissions 参考代码：https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter3","link":"/2017/11/23/跟我学Shiro（三）-授权/"},{"title":"跟我学Shiro（二）-身份认证","text":"简介身份验证：在应用中能证明他就是他本人。一般提供一些标识信息来表明他就是他本人，如提供身份证，用户名/密码来证明。在 shiro 中，用户需要提供 principals （身份）和 credentials（证明）给 shiro，从而应用能验证用户身份。 principals：身份；即主体的标识属性，可以是任何东西，如用户名、邮箱等，唯一即可。一个主体可以有多个 principals，但只有一个 Primary principals，一般是用户名/密码/手机号。 credentials：证明/凭证；即只有主体知道的安全值，如密码/数字证书等。 最常见的 principals 和 credentials 组合就是用户名/密码了。另外两个相关的概念是之前提到的 Subject 及 Realm，分别是主体及验证主体的数据源。 环境准备使用Maven构建准备环境依赖：添加 junit、common-logging 及 shiro-core 依赖； 更新：加入slf4j-nop依赖包。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 登录/退出准备一些用户身份/凭据 shiro.ini [users]gojay=testroot=root 此处使用ini配置文件，通过[user]指定两个主体。 测试用例 com.github.gojay001.test.LoginLogoutTest更新：注意类过时。 @Testpublic void testLoginLogout() { //1、获取SecurityManager工厂，此处使用Ini配置文件初始化SecurityManager Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;); //2、得到SecurityManager实例 并绑定给SecurityUtils SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); //3、得到Subject及创建用户名/密码身份验证Token（即用户身份/凭证） Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;); try { //4、登录，即身份验证 subject.login(token); } catch (AuthenticationException e) { //5、身份验证失败 } Assert.assertEquals(true, subject.isAuthenticated()); //6、退出 subject.logout();} 首先通过new IniSecurityManagerFactory并指定一个ini配置文件来创建一个SecurityManager工厂； 接着获取SecurityManager并绑定到SecurityUtils，这是一个全局设置，设置一次即可； 通过SecurityUtils得到Subject，其会自动绑定到当前线程；如果在web环境在请求结束时需要解除绑定；然后获取身份验证的Token，如用户名/密码； 调用subject.login方法进行登录，其会自动委托给SecurityManager.login方法进行登录； 如果身份验证失败捕获AuthenticationException或其子类，常见的如： DisabledAccountException（禁用的帐号）、LockedAccountException（锁定的帐号）、UnknownAccountException（错误的帐号）、ExcessiveAttemptsException（登录失败次数过多）、IncorrectCredentialsException （错误的凭证）、ExpiredCredentialsException（过期的凭证）等，具体查看其继承关系； 最后可以调用subject.logout退出，其会自动委托给SecurityManager.logout方法退出。 总结步骤 收集用户身份/凭证，即如用户名/密码； 调用Subject.login进行登录，如果失败将得到相应的AuthenticationException异常，根据异常提示用户错误信息；否则登录成功； 最后调用Subject.logout进行退出操作。 存在问题 用户名/密码硬编码在ini配置文件，以后需要改成如数据库存储，且密码需要加密存储； 用户身份Token可能不仅仅是用户名/密码，也可能还有其他的，如登录时允许用户名/邮箱/手机号同时登录。 身份认证流程 流程如下： 首先调用Subject.login(token)进行登录，其会自动委托给Security Manager，调用之前必须通过SecurityUtils. setSecurityManager()设置； SecurityManager负责真正的身份验证逻辑；它会委托给Authenticator进行身份验证； Authenticator才是真正的身份验证者，Shiro API中核心的身份认证入口点，此处可以自定义插入自己的实现； Authenticator可能会委托给相应的AuthenticationStrategy进行多Realm身份验证，默认ModularRealmAuthenticator会调用AuthenticationStrategy进行多Realm身份验证； Authenticator会把相应的token传入Realm，从Realm获取身份验证信息，如果没有返回/抛出异常表示身份验证失败了。此处可以配置多个Realm，将按照相应的顺序及策略进行访问。 RealmRealm：域；Shiro从Realm获取安全数据（如用户、角色、权限），可以把Realm看成DataSource，即安全数据源。 org.apache.shiro.realm.Realm接口如下： String getName(); //返回一个唯一的Realm名字 boolean supports(AuthenticationToken token); //判断此Realm是否支持此Token AuthenticationInfo getAuthenticationInfo(AuthenticationToken token) throws AuthenticationException; //根据Token获取认证信息 单Realm配置自定义Realm实现 com.github.gojay001.realm.MyRealm1 public class MyRealm1 implements Realm { public String getName() { return &quot;myRealm1&quot;; } public boolean supports(AuthenticationToken authenticationToken) { return authenticationToken instanceof UsernamePasswordToken; } public AuthenticationInfo getAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException { // 得到用户名 String username = (String)authenticationToken.getPrincipal(); // 得到密码 String password = new String((char[])authenticationToken.getCredentials()); if(!&quot;root&quot;.equals(username)) { //用户名错误 throw new UnknownAccountException(); } if(!&quot;root&quot;.equals(password)) { //密码错误 throw new IncorrectCredentialsException(); } //如果身份认证验证成功，返回一个AuthenticationInfo实现； return new SimpleAuthenticationInfo(username, password, getName()); }} ini配置文件指定自定义Realm实现 shiro-realm.ini # 声明一个realmmyRealm1=com.github.gojay001.realm.MyRealm1# 指定securityManager的realms实现securityManager.realms=$myRealm1 通过$name来引入之前的realm定义。 多Realm配置ini配置文件 shiro-multi-realm.ini # 声明一个realm myRealm1=com.github.zhangkaitao.shiro.chapter2.realm.MyRealm1 myRealm2=com.github.zhangkaitao.shiro.chapter2.realm.MyRealm2 # 指定securityManager的realms实现 securityManager.realms=$myRealm1,$myRealm2 securityManager会按照realms指定的顺序进行身份认证。 Shiro默认提供的Realm 以后一般继承AuthorizingRealm（授权）即可；其继承了AuthenticatingRealm（即身份验证），而且也间接继承了CachingRealm（带有缓存实现）。其中主要默认实现如下： org.apache.shiro.realm.text.IniRealm：[users]部分指定用户名/密码及其角色；[roles]部分指定角色即权限信息； org.apache.shiro.realm.text.PropertiesRealm： user.username=password,role1,role2指定用户名/密码及其角色；role.role1=permission1,permission2指定角色及权限信息； org.apache.shiro.realm.jdbc.JdbcRealm：通过sql查询相应的信息，如“select password from users where username = ?”获取用户密码，“select role_name from user_roles where username = ?”获取用户角色；“select permission from roles_permissions where role_name = ?”获取角色对应的权限信息；也可以调用相应的api进行自定义sql； JDBC Realm使用数据库及依赖 更新：alibaba的druid包更新版本。 &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;6.0.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.5&lt;/version&gt;&lt;/dependency&gt; 本文将使用mysql数据库及druid连接池。 数据库下建表users（用户名/密码）、user_roles（用户/角色）、roles_permissions（角色/权限）；具体请参照sql/shiro.sql；并添加一个用户记录，用户名/密码为root/root。 ini配置 shiro-jdbc-realm.ini jdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealm dataSource=com.alibaba.druid.pool.DruidDataSource dataSource.driverClassName=com.mysql.jdbc.Driver dataSource.url=jdbc:mysql://localhost:3306/shiro dataSource.username=root dataSource.password=rootjdbcRealm.dataSource=$dataSource securityManager.realms=$jdbcRealm 变量名=全限定类名 自动创建一个类实例 变量名.属性=值 自动调用相应的setter方法进行赋值 $变量名 引用之前的一个对象实例 测试代码和之前的没什么区别。 Authenticator及AuthenticationStrategy原理Authenticator的职责是验证用户帐号，是Shiro API中身份验证核心的入口点： public AuthenticationInfo authenticate(AuthenticationToken authenticationToken) throws AuthenticationException; 如果验证成功，将返回AuthenticationInfo验证信息，此信息中包含了身份及凭证；如果验证失败将抛出相应的AuthenticationException实现。 SecurityManager接口继承了Authenticator，另外还有一个ModularRealmAuthenticator实现，其委托给多个Realm进行验证，验证规则通过AuthenticationStrategy接口指定，默认提供的实现： FirstSuccessfulStrategy：只要有一个Realm验证成功即可，只返回第一个Realm身份验证成功的认证信息，其他的忽略； AtLeastOneSuccessfulStrategy：只要有一个Realm验证成功即可，和FirstSuccessfulStrategy不同，返回所有Realm身份验证成功的认证信息； AllSuccessfulStrategy：所有Realm验证成功才算成功，且返回所有Realm身份验证成功的认证信息，如果有一个失败就失败了。 ModularRealmAuthenticator默认使用AtLeastOneSuccessfulStrategy策略。 示例 假设有三个realm：myRealm1： 用户名/密码为root/root时成功，且返回身份/凭据为root/root；myRealm2： 用户名/密码为gojay/test时成功，且返回身份/凭据为gojay/test；myRealm3： 用户名/密码为root/root时成功，且返回身份/凭据为root@foxmail.com/root； ini配置文件 shiro-authenticator-all-success.ini [main]#指定securityManager的authenticator实现authenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticatorsecurityManager.authenticator=$authenticator#指定securityManager.authenticator的authenticationStrategyallSuccessfulStrategy=org.apache.shiro.authc.pam.AllSuccessfulStrategysecurityManager.authenticator.authenticationStrategy=$allSuccessfulStrategymyRealm1=com.github.gojay001.realm.MyRealm1myRealm2=com.github.gojay001.realm.MyRealm2myRealm3=com.github.gojay001.realm.MyRealm3securityManager.realms=$myRealm1,$myRealm3 测试代码 com.github.gojay001.test.AuthenticatorTest 首先通用化登录逻辑private void login(String configFile) { //1、获取SecurityManager工厂，此处使用Ini配置文件初始化SecurityManager Factory&lt;org.apache.shiro.mgt.SecurityManager&gt; factory = new IniSecurityManagerFactory(configFile); //2、得到SecurityManager实例 并绑定给SecurityUtils org.apache.shiro.mgt.SecurityManager securityManager = factory.getInstance(); SecurityUtils.setSecurityManager(securityManager); //3、得到Subject及创建用户名/密码身份验证Token（即用户身份/凭证） Subject subject = SecurityUtils.getSubject(); UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;); subject.login(token); } 测试AllSuccessfulStrategy成功@Test public void testAllSuccessfulStrategyWithSuccess() { login(&quot;classpath:shiro-authenticator-all-success.ini&quot;); Subject subject = SecurityUtils.getSubject(); //得到一个身份集合，其包含了Realm验证成功的身份信息 PrincipalCollection principalCollection = subject.getPrincipals(); Assert.assertEquals(2, principalCollection.asList().size()); } 测试AllSuccessfulStrategy失败@Test(expected = UnknownAccountException.class) public void testAllSuccessfulStrategyWithFail() { login(&quot;classpath:shiro-authenticator-all-fail.ini&quot;); Subject subject = SecurityUtils.getSubject(); } shiro-authenticator-all-fail.ini 与 shiro-authenticator-all-success.ini 不同的配置是使用了 securityManager.realms=$myRealm1,$myRealm2 ；即myRealm验证失败。 对于 AtLeastOneSuccessfulStrategy 和 FirstSuccessfulStrategy 的区别：唯一不同点一个是返回所有验证成功的Realm的认证信息；另一个是只返回第一个验证成功的Realm的认证信息.示例代码同上 自定义AuthenticationStrategy实现首先看其API： //在所有Realm验证之前调用 AuthenticationInfo beforeAllAttempts( Collection&lt;? extends Realm&gt; realms, AuthenticationToken token) throws AuthenticationException; //在每个Realm之前调用 AuthenticationInfo beforeAttempt( Realm realm, AuthenticationToken token, AuthenticationInfo aggregate) throws AuthenticationException; //在每个Realm之后调用 AuthenticationInfo afterAttempt( Realm realm, AuthenticationToken token, AuthenticationInfo singleRealmInfo, AuthenticationInfo aggregateInfo, Throwable t) throws AuthenticationException; //在所有Realm之后调用 AuthenticationInfo afterAllAttempts( AuthenticationToken token, AuthenticationInfo aggregate) throws AuthenticationException; 因为每个AuthenticationStrategy实例都是无状态的，所有每次都通过接口将相应的认证信息传入下一次流程；通过如上接口可以进行如合并/返回第一个验证成功的认证信息。自定义实现时一般继承 org.apache.shiro.authc.pam.AbstractAuthenticationStrategy 即可。参考代码同上 到此基本的身份验证就结束了。 总结问题描述Assert过时 Assert in junit.framework has been deprecated解决：将 import junit.framework.Assert; 改为 import org.junit.Assert; SLF4J加载失败 SLF4J: Failed to load class “org.slf4j.impl.StaticLoggerBinder”解决：Maven引入slf4j-nop包： &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt; &lt;version&gt;1.7.6&lt;/version&gt;&lt;/dependency&gt; implements不需要@Overridealibaba的druid版本更新重点用户登录流程 Subject.login(token) SecurityManager Authenticator AuthenticatorStrategy Realm Realm 单Realm 多Realm JDBCRealm AuthenticatorStrategy FirstSuccessfulStrategy AtLeastOneSuccessfulStrategy AllSuccessfulStrategy 自定义Strategy 参考代码：https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter2","link":"/2017/11/22/跟我学Shiro（二）-身份认证/"},{"title":"跟我学Shiro（一）-Shiro简介","text":"简介认识 Apache Shiro是Java的一个安全框架。 对比Spring Security小而简单。 可同时用在JavaSE、JavaEE环境中。 主要完成认证、授权、加密、会话管理、与Web集成、缓存等。 功能 Authentication：身份认证/登录，验证用户是不是拥有相应的身份； Authorization：授权，即权限验证，验证某个已认证的用户是否拥有某个权限； Session Manager：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中； Cryptography：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储； Web Support：Web 支持，可以非常容易的集成到 Web 环境； Concurrency：shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去； Testing：提供测试支持； Run As：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问； Remember Me：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。 Shiro不会去维护用户、维护权限；这些需要自己设计提供，然后通过相应的接口注入给Shiro。 架构从外部看Shiro： Subject：主体；代表了与当前应用交互的用户，如网络爬虫、机器人等；所有 Subject 都绑定到 SecurityManager，与 Subject 的所有交互都会委托给 SecurityManager；可以把 Subject 认为是一个门面，SecurityManager 才是实际的执行者； SecurityManager：安全管理器；即所有与安全有关的操作都会与 SecurityManager 交互，且它管理着所有 Subject；它是 Shiro 的核心，负责与其他组件进行交互，可以把它看成Spring NVC中的 DispatcherServlet 前端控制器； Realm：域；Shiro 从 Realm 获取安全数据（如用户、角色、权限），就是说 SecurityManager 要验证用户身份，那么它需要从 Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从 Realm 得到用户相应的角色/权限进行验证用户是否能进行操作；可以把 Realm 看成 DataSource，即安全数据源。 最简单的一个 Shiro 应用： 应用代码通过 Subject 来进行认证和授权，而 Subject 又委托给 SecurityManager； 我们需要给 Shiro 的 SecurityManager 注入 Realm，从而让 SecurityManager 能得到合法的用户及其权限进行判断。可以看出：Shiro不提供维护用户/权限，而是通过Realm让开发人员自己注入。 从内部看Shiro： Subject：主体；可以看到主体可以是任何可以与应用交互的“用户”； SecurityManager ：安全管理器；所有具体的交互都通过 SecurityManager 进行控制；它管理着所有 Subject，且负责进行认证和授权、及会话、缓存的管理，是Shiro的心脏； Authenticator：认证器；负责主体认证的，这是一个扩展点，如果用户觉得 Shiro 默认的不好，可以自定义实现；其需要认证策略Authentication Strategy，即什么情况下算用户认证通过了； * Authrizer：授权器；用来决定主体是否有权限进行相应的操作，即控制着用户能访问应用中的哪些功能； Realm：可以有 1 个或多个 Realm，可以认为是安全实体数据源，即用于获取安全实体的，由用户提供；Shiro 不知道用户/权限存储在哪及以何种格式存储，所以我们一般在应用中都需要实现自己的 Realm； SessionManager：Session需要有人去管理它的生命周期，这个组件就是SessionManager；Shiro 抽象了一个自己的 Session 来管理主体与应用之间交互的数据；这样的话，比如我们在 Web 环境用，刚开始是一台 Web 服务器；接着又上了台 EJB 服务器；这时想把两台服务器的会话数据放到一个地方，这个时候就可以实现自己的分布式会话（如把数据放到 Memcached 服务器）； SessionDAO：数据访问对象DAO，用于会话的 CRUD，比如我们想把 Session 保存到数据库，那么可以实现自己的 SessionDAO，通过如JDBC写到数据库；比如想把 Session 放到Memcached中，可以实现自己的 Memcached SessionDAO；另外 SessionDAO 中可以使用 Cache 进行缓存，以提高性能； CacheManager：缓存控制器；来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能； Cryptography：密码模块;Shiro 提高了一些常见的加密组件用于如密码加密/解密的。 到此 Shiro 架构及其组件就认识完了。","link":"/2017/11/19/跟我学Shiro（一）-Shiro简介/"},{"title":"Learning JavaWeb Path","text":"基本概念的理解 绝对路径：绝对路径就是你的主页上的文件或目录在硬盘上真正的路径，(URL和物理路径)例如：C:\\xyz\\test.txt；http://www.test.com/index.html； 相对路径：相对与某个基准目录的路径，例如：&quot;/&quot;代表Web应用的根目录，&quot;./&quot;代表当前目录,&quot;../&quot;代表上级目录。 另外关于URI，URL,URN等内容，请参考RFC相关文档标准。 关于JSP/Servlet中的相对路径和绝对路径 服务器端的地址 服务器端的相对地址指的是相对于你的web应用的地址，这个地址是在服务器端解析的（不同于html和javascript中的相对地址，他们是由客户端浏览器解析的）；在jsp和servlet中的相对地址应该是相对于你的web应用，即相对于http://192.168.0.1/webapp/的。用到的地方： forward：servlet中的request.getRequestDispatcher(address);这个address是在服务器端解析的。request.getRequestDispatcher(“/pages/a.jsp”)的绝对路径地址：http://192.168.0.1/webapp/pages/a.jsp； sendRedirect：在jsp中&lt;%response.sendRedirect(&quot;/user/a.jsp&quot;);%&gt;。 客户端的地址 所有的html页面中的相对地址都是相对于服务器根目录http://192.168.0.1/的，而不是根目录下的该Web应用的目录：http://192.168.0.1/webapp/。 HTML中的form表单的action属性的地址应该是相对于服务器根目录http://192.168.0.1/；如果提交到a.jsp为：action＝&quot;/webapp/user/a.jsp&quot;或action=&quot;&lt;%=request.getContextPath()%&gt;&quot;/user/a.jsp； Javascript也是在客户端解析的，所以其相对路径和form表单一样。 因此，一般情况下，在JSP/HTML页面等引用的CSS、Javascript、Action等属性前面最好都加上&lt;%=request.getContextPath()%&gt;,以确保所引用的文件都属于Web应用中的目录。另外，应该尽量避免使用类似&quot;.&quot;,&quot;./&quot;,&quot;../&quot;等类似的相对该文件位置的相对路径，这样当文件移动时，很容易出问题。 站点根目录和css路径问题 当在jsp中引入css时，如果其相对路径相对于当前jsp文件的，而在一个和这个jsp的路径不一样的servlet中forward这个jsp时，就会发现这个css样式根本没有起作用。这是因为在servlet中转发时css的路径就是相对于这个servlet的相对路径，而非jsp的路径了。所以这时候不能在jsp中用这样的路径：&lt;link href=&quot;one.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;或者&lt;link href=&quot;../../one.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;。这个时候要用站点根目录，就是相对于http://192.168.0.1/的目录，以&quot;/&quot;开头。因此上述错误应更正为href=”/test/one.css” 类似的站点根目录的相对目录。 获得JSP/Servlet中当前应用的相对路径和绝对路径 JSP中获得当前应用的相对路径和绝对路径 根目录所对应的绝对路径:request.getRequestURI(); 文件的绝对路径:application.getRealPath(request.getRequestURI()); 当前web应用的绝对路径:application.getRealPath(&quot;/&quot;); 请求文件的上层目录:new File(application.getRealPath(request.getRequestURI())).getParent(); Servlet中获得当前应用的相对路径和绝对路径 根目录所对应的绝对路径:request.getServletPath(); 文件的绝对路径:request.getSession().getServletContext().getRealPath(request.getRequestURI()); 当前web应用的绝对路径:servletConfig.getServletContext().getRealPath(&quot;/&quot;); ServletContext对象获得几种方式：javax.servlet.http.HttpSession.getServletContext();javax.servlet.jsp.PageContext.getServletContext();javax.servlet.ServletConfig.getServletContext(); JAVA的Class中获得相对路径，绝对路径 单独的Java类中获得绝对路径 根据java.io.File的Doc文挡，可知: 默认情况下new File(&quot;/&quot;)代表的目录为：System.getProperty(&quot;user.dir&quot;);程序获得执行类的当前路径: import java.io.File;public class FileTest { public static void main(String[] args) throws Exception { System.out.println(Thread.currentThread().getContextClassLoader().getResource(&quot;&quot;)); System.out.println(FileTest.class.getClassLoader().getResource(&quot;&quot;)); System.out.println(ClassLoader.getSystemResource(&quot;&quot;)); System.out.println(FileTest.class.getResource(&quot;&quot;)); System.out.println(FileTest.class.getResource(&quot;/&quot;));//Class文件所在路径 System.out.println(new File(&quot;/&quot;).getAbsolutePath()); System.out.println(System.getProperty(&quot;user.dir&quot;)); }} 服务器中的Java类获得当前路径Weblogic WebApplication的系统文件根目录是你的weblogic安装所在根目录。例如：如果你的weblogic安装在c:\\bea\\weblogic700.....那么，你的文件根路径就是c:\\所以，有两种方式能够让你访问你的服务器端的文件: 使用绝对路径：比如将你的参数文件放在c:\\yourconfig\\yourconf.properties，直接使用new FileInputStream(&quot;yourconfig/yourconf.properties&quot;); 使用相对路径：相对路径的根目录就是你的webapplication的根路径，即WEB-INF的上一级目录，将你的参数文件放在yourwebapp\\yourconfig\\yourconf.properties，这样使用：new FileInputStream(&quot;./yourconfig/yourconf.properties&quot;); Tomcat 在类中输出System.getProperty(&quot;user.dir&quot;);显示的是%Tomcat_Home%/bin 如何读相对路径哪? 在Java文件中getResource或getResourceAsStream均可。例：getClass().getResourceAsStream(filePath);filePath可以是”/filename”,这里的/代表web发布根路径下WEB-INF/classes。 参考文档：java路径问题","link":"/2017/11/18/Learning-JavaWeb-Path/"},{"title":"Learning JavaWeb Encoding","text":"项目配置 页面乱码页面乱码只需设置相关 字符集编码 即可。 JSP页面：&lt;%@ page pageEncoding=&quot;UTF-8&quot; contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt; pageEncoding :该页面编码格式；charset :页面解码格式； HTML页面：&lt;meta http-equiv=Content-Type content=&quot;text/html;charset=utf-8&quot;&gt; 传值乱码页面 到 controller 传值乱码需要在 web.xml 配置字符编码过滤器。 直接应用 spring 中字符编码过滤器：&lt;!--字符编码--&gt;&lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 分析源码发现作用相当于servlet中： request.setCharacterEncoding(&quot;UTF-8&quot;); response.setCharacterEncoding(&quot;UTF-8&quot;); Spring自带过滤器主要针对POST请求，对GET请求无效。对于GET请求的参数乱码，解决方法是采用数据还原： String userName = request.getParameter(&quot;userName&quot;); userName = new String(userName.getBytes(&quot;iso8859-1&quot;),&quot;UTF-8&quot;); &lt;url-pattern&gt; 中匹配说明：/: 不会匹配到.jsp，但会匹配/login等路径类型的url；/*: 会匹配/login、.jsp、*.html等路径； 根据源码可 自己编写 字符编码过滤器：public class CharacterEncodingFilter implements Filter { private String encoding = null; private FilterConfig filterConfig = null; @Override public void init(FilterConfig config) throws ServletException { this.filterConfig = config; this.encoding = config.getInitParameter(&quot;encoding&quot;); } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws ServletException, IOException { if (encoding != null) { request.setCharacterEncoding(encoding); response.setContentType(&quot;text/html;charset=&quot; + encoding); } chain.doFilter(request, response); }} 存入数据库乱码需要在 数据库配置文件 设置参数。 url=jdbc:mysql://gojay001.mysql.rds.aliyuncs.com:3306/trade?useUnicode=true&amp;characterEncoding=utf8 环境配置 Tomcat配置在tomcat的 conf/server.xml 中配置Get请求默认编码： &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; URIEncoding=&quot;UTF-8&quot; useBodyEncodingForURI=&quot;true&quot; /&gt; 数据库配置安装mysql之后默认的字符编码为 latin1 : 查看: $ show variables like &apos;%char%&apos;; vi /etc/my.cf 修改为下面内容后重启mysql： [mysqld]character_set_server=utf8lower_case_table_names=1datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sockuser=mysqlsymbolic-links=0sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES [mysqld_safe]default-character-set = utf8log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid[client]default-character-set = utf8[mysql.server]default-character-set = utf8[mysql]default-character-set = utf8 其他问题 HTML文件显示乱码：将编码格式保存为UTF-8包含BOM。","link":"/2017/11/15/Learning-JavaWeb-Encoding/"}],"tags":[{"name":"DL","slug":"DL","link":"/tags/DL/"},{"name":"Classification","slug":"Classification","link":"/tags/Classification/"},{"name":"Review","slug":"Review","link":"/tags/Review/"},{"name":"FSS","slug":"FSS","link":"/tags/FSS/"},{"name":"CANet","slug":"CANet","link":"/tags/CANet/"},{"name":"Tracking","slug":"Tracking","link":"/tags/Tracking/"},{"name":"MOT","slug":"MOT","link":"/tags/MOT/"},{"name":"VOT","slug":"VOT","link":"/tags/VOT/"},{"name":"FSL","slug":"FSL","link":"/tags/FSL/"},{"name":"Survey","slug":"Survey","link":"/tags/Survey/"},{"name":"Detection","slug":"Detection","link":"/tags/Detection/"},{"name":"Faster R-CNN","slug":"Faster-R-CNN","link":"/tags/Faster-R-CNN/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"Attention","slug":"Attention","link":"/tags/Attention/"},{"name":"LTM","slug":"LTM","link":"/tags/LTM/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Encoding","slug":"Encoding","link":"/tags/Encoding/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"OSLSM","slug":"OSLSM","link":"/tags/OSLSM/"},{"name":"CRNet","slug":"CRNet","link":"/tags/CRNet/"},{"name":"NIN","slug":"NIN","link":"/tags/NIN/"},{"name":"AOT","slug":"AOT","link":"/tags/AOT/"},{"name":"PANet","slug":"PANet","link":"/tags/PANet/"},{"name":"Path","slug":"Path","link":"/tags/Path/"},{"name":"PGNet","slug":"PGNet","link":"/tags/PGNet/"},{"name":"PPNet","slug":"PPNet","link":"/tags/PPNet/"},{"name":"Detection-3D","slug":"Detection-3D","link":"/tags/Detection-3D/"},{"name":"PV-RCNN","slug":"PV-RCNN","link":"/tags/PV-RCNN/"},{"name":"RN","slug":"RN","link":"/tags/RN/"},{"name":"SG-One","slug":"SG-One","link":"/tags/SG-One/"},{"name":"Toolkit","slug":"Toolkit","link":"/tags/Toolkit/"},{"name":"Overview","slug":"Overview","link":"/tags/Overview/"},{"name":"co-FCN","slug":"co-FCN","link":"/tags/co-FCN/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"Offer","slug":"Offer","link":"/tags/Offer/"},{"name":"数组","slug":"数组","link":"/tags/数组/"},{"name":"字典","slug":"字典","link":"/tags/字典/"},{"name":"二叉搜索树","slug":"二叉搜索树","link":"/tags/二叉搜索树/"},{"name":"字符串","slug":"字符串","link":"/tags/字符串/"},{"name":"链表","slug":"链表","link":"/tags/链表/"},{"name":"递归","slug":"递归","link":"/tags/递归/"},{"name":"栈","slug":"栈","link":"/tags/栈/"},{"name":"二叉树","slug":"二叉树","link":"/tags/二叉树/"},{"name":"HashMap","slug":"HashMap","link":"/tags/HashMap/"},{"name":"队列","slug":"队列","link":"/tags/队列/"},{"name":"动态规划","slug":"动态规划","link":"/tags/动态规划/"},{"name":"二分查找","slug":"二分查找","link":"/tags/二分查找/"},{"name":"DFS","slug":"DFS","link":"/tags/DFS/"},{"name":"回溯","slug":"回溯","link":"/tags/回溯/"},{"name":"BFS","slug":"BFS","link":"/tags/BFS/"},{"name":"贪心","slug":"贪心","link":"/tags/贪心/"},{"name":"分治","slug":"分治","link":"/tags/分治/"},{"name":"位运算","slug":"位运算","link":"/tags/位运算/"},{"name":"有限状态自动机","slug":"有限状态自动机","link":"/tags/有限状态自动机/"},{"name":"指针","slug":"指针","link":"/tags/指针/"},{"name":"树","slug":"树","link":"/tags/树/"},{"name":"堆","slug":"堆","link":"/tags/堆/"},{"name":"topk","slug":"topk","link":"/tags/topk/"},{"name":"排序","slug":"排序","link":"/tags/排序/"},{"name":"数学","slug":"数学","link":"/tags/数学/"},{"name":"滑动窗口","slug":"滑动窗口","link":"/tags/滑动窗口/"},{"name":"Shiro","slug":"Shiro","link":"/tags/Shiro/"},{"name":"Segmentation","slug":"Segmentation","link":"/tags/Segmentation/"},{"name":"Mask R-CNN","slug":"Mask-R-CNN","link":"/tags/Mask-R-CNN/"},{"name":"GoogLeNet","slug":"GoogLeNet","link":"/tags/GoogLeNet/"},{"name":"Inception","slug":"Inception","link":"/tags/Inception/"},{"name":"ResNet","slug":"ResNet","link":"/tags/ResNet/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"}],"categories":[{"name":"DeepLearning","slug":"DeepLearning","link":"/categories/DeepLearning/"},{"name":"Object Tracking","slug":"DeepLearning/Object-Tracking","link":"/categories/DeepLearning/Object-Tracking/"},{"name":"Classification","slug":"DeepLearning/Classification","link":"/categories/DeepLearning/Classification/"},{"name":"Few-Shot Segmentation","slug":"DeepLearning/Few-Shot-Segmentation","link":"/categories/DeepLearning/Few-Shot-Segmentation/"},{"name":"Few-Shot Learning","slug":"DeepLearning/Few-Shot-Learning","link":"/categories/DeepLearning/Few-Shot-Learning/"},{"name":"Object Detection","slug":"DeepLearning/Object-Detection","link":"/categories/DeepLearning/Object-Detection/"},{"name":"Toolkit","slug":"Toolkit","link":"/categories/Toolkit/"},{"name":"Image Generation","slug":"DeepLearning/Image-Generation","link":"/categories/DeepLearning/Image-Generation/"},{"name":"FFT","slug":"DeepLearning/Object-Tracking/FFT","link":"/categories/DeepLearning/Object-Tracking/FFT/"},{"name":"JRMOT","slug":"DeepLearning/Object-Tracking/JRMOT","link":"/categories/DeepLearning/Object-Tracking/JRMOT/"},{"name":"Review","slug":"DeepLearning/Classification/Review","link":"/categories/DeepLearning/Classification/Review/"},{"name":"JavaWeb","slug":"JavaWeb","link":"/categories/JavaWeb/"},{"name":"FairMOT","slug":"DeepLearning/Object-Tracking/FairMOT","link":"/categories/DeepLearning/Object-Tracking/FairMOT/"},{"name":"GlobalTrack","slug":"DeepLearning/Object-Tracking/GlobalTrack","link":"/categories/DeepLearning/Object-Tracking/GlobalTrack/"},{"name":"NIN","slug":"DeepLearning/Classification/NIN","link":"/categories/DeepLearning/Classification/NIN/"},{"name":"PAMCC-AOT","slug":"DeepLearning/Object-Tracking/PAMCC-AOT","link":"/categories/DeepLearning/Object-Tracking/PAMCC-AOT/"},{"name":"CANet","slug":"DeepLearning/Few-Shot-Segmentation/CANet","link":"/categories/DeepLearning/Few-Shot-Segmentation/CANet/"},{"name":"PANet","slug":"DeepLearning/Few-Shot-Segmentation/PANet","link":"/categories/DeepLearning/Few-Shot-Segmentation/PANet/"},{"name":"FSL Survey","slug":"DeepLearning/Few-Shot-Learning/FSL-Survey","link":"/categories/DeepLearning/Few-Shot-Learning/FSL-Survey/"},{"name":"PGNet","slug":"DeepLearning/Few-Shot-Segmentation/PGNet","link":"/categories/DeepLearning/Few-Shot-Segmentation/PGNet/"},{"name":"PPNet","slug":"DeepLearning/Few-Shot-Segmentation/PPNet","link":"/categories/DeepLearning/Few-Shot-Segmentation/PPNet/"},{"name":"3D Object Dedection","slug":"DeepLearning/3D-Object-Dedection","link":"/categories/DeepLearning/3D-Object-Dedection/"},{"name":"Faster R-CNN","slug":"DeepLearning/Object-Detection/Faster-R-CNN","link":"/categories/DeepLearning/Object-Detection/Faster-R-CNN/"},{"name":"RN","slug":"DeepLearning/Few-Shot-Learning/RN","link":"/categories/DeepLearning/Few-Shot-Learning/RN/"},{"name":"SG-One","slug":"DeepLearning/Few-Shot-Segmentation/SG-One","link":"/categories/DeepLearning/Few-Shot-Segmentation/SG-One/"},{"name":"DeepSORT","slug":"DeepLearning/Object-Tracking/DeepSORT","link":"/categories/DeepLearning/Object-Tracking/DeepSORT/"},{"name":"SORT","slug":"DeepLearning/Object-Tracking/SORT","link":"/categories/DeepLearning/Object-Tracking/SORT/"},{"name":"SiamMask","slug":"DeepLearning/Object-Tracking/SiamMask","link":"/categories/DeepLearning/Object-Tracking/SiamMask/"},{"name":"Hexo","slug":"Toolkit/Hexo","link":"/categories/Toolkit/Hexo/"},{"name":"SiamRPN++","slug":"DeepLearning/Object-Tracking/SiamRPN","link":"/categories/DeepLearning/Object-Tracking/SiamRPN/"},{"name":"TSDM","slug":"DeepLearning/Object-Tracking/TSDM","link":"/categories/DeepLearning/Object-Tracking/TSDM/"},{"name":"ImageTransformer","slug":"DeepLearning/Image-Generation/ImageTransformer","link":"/categories/DeepLearning/Image-Generation/ImageTransformer/"},{"name":"Overview","slug":"Toolkit/Overview","link":"/categories/Toolkit/Overview/"},{"name":"Tracklet","slug":"DeepLearning/Object-Tracking/Tracklet","link":"/categories/DeepLearning/Object-Tracking/Tracklet/"},{"name":"co-FCN","slug":"DeepLearning/Few-Shot-Segmentation/co-FCN","link":"/categories/DeepLearning/Few-Shot-Segmentation/co-FCN/"},{"name":"Tracktor","slug":"DeepLearning/Object-Tracking/Tracktor","link":"/categories/DeepLearning/Object-Tracking/Tracktor/"},{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"LTM","slug":"DeepLearning/Few-Shot-Segmentation/LTM","link":"/categories/DeepLearning/Few-Shot-Segmentation/LTM/"},{"name":"Project","slug":"JavaWeb/Project","link":"/categories/JavaWeb/Project/"},{"name":"Interview","slug":"JavaWeb/Interview","link":"/categories/JavaWeb/Interview/"},{"name":"OSLSM","slug":"DeepLearning/Few-Shot-Segmentation/OSLSM","link":"/categories/DeepLearning/Few-Shot-Segmentation/OSLSM/"},{"name":"CRNet","slug":"DeepLearning/Few-Shot-Segmentation/CRNet","link":"/categories/DeepLearning/Few-Shot-Segmentation/CRNet/"},{"name":"PV-RCNN","slug":"DeepLearning/3D-Object-Dedection/PV-RCNN","link":"/categories/DeepLearning/3D-Object-Dedection/PV-RCNN/"},{"name":"Shiro","slug":"JavaWeb/Shiro","link":"/categories/JavaWeb/Shiro/"},{"name":"Object Segmentation","slug":"DeepLearning/Object-Segmentation","link":"/categories/DeepLearning/Object-Segmentation/"},{"name":"剑指Offer","slug":"Algorithm/剑指Offer","link":"/categories/Algorithm/剑指Offer/"},{"name":"Mask R-CNN","slug":"DeepLearning/Object-Segmentation/Mask-R-CNN","link":"/categories/DeepLearning/Object-Segmentation/Mask-R-CNN/"},{"name":"3","slug":"Algorithm/剑指Offer/3","link":"/categories/Algorithm/剑指Offer/3/"},{"name":"4","slug":"Algorithm/剑指Offer/4","link":"/categories/Algorithm/剑指Offer/4/"},{"name":"5","slug":"Algorithm/剑指Offer/5","link":"/categories/Algorithm/剑指Offer/5/"},{"name":"6","slug":"Algorithm/剑指Offer/6","link":"/categories/Algorithm/剑指Offer/6/"},{"name":"7","slug":"Algorithm/剑指Offer/7","link":"/categories/Algorithm/剑指Offer/7/"},{"name":"09","slug":"Algorithm/剑指Offer/09","link":"/categories/Algorithm/剑指Offer/09/"},{"name":"10","slug":"Algorithm/剑指Offer/10","link":"/categories/Algorithm/剑指Offer/10/"},{"name":"11","slug":"Algorithm/剑指Offer/11","link":"/categories/Algorithm/剑指Offer/11/"},{"name":"12","slug":"Algorithm/剑指Offer/12","link":"/categories/Algorithm/剑指Offer/12/"},{"name":"13","slug":"Algorithm/剑指Offer/13","link":"/categories/Algorithm/剑指Offer/13/"},{"name":"14","slug":"Algorithm/剑指Offer/14","link":"/categories/Algorithm/剑指Offer/14/"},{"name":"16","slug":"Algorithm/剑指Offer/16","link":"/categories/Algorithm/剑指Offer/16/"},{"name":"15","slug":"Algorithm/剑指Offer/15","link":"/categories/Algorithm/剑指Offer/15/"},{"name":"18","slug":"Algorithm/剑指Offer/18","link":"/categories/Algorithm/剑指Offer/18/"},{"name":"17","slug":"Algorithm/剑指Offer/17","link":"/categories/Algorithm/剑指Offer/17/"},{"name":"19","slug":"Algorithm/剑指Offer/19","link":"/categories/Algorithm/剑指Offer/19/"},{"name":"20","slug":"Algorithm/剑指Offer/20","link":"/categories/Algorithm/剑指Offer/20/"},{"name":"21","slug":"Algorithm/剑指Offer/21","link":"/categories/Algorithm/剑指Offer/21/"},{"name":"22","slug":"Algorithm/剑指Offer/22","link":"/categories/Algorithm/剑指Offer/22/"},{"name":"25","slug":"Algorithm/剑指Offer/25","link":"/categories/Algorithm/剑指Offer/25/"},{"name":"24","slug":"Algorithm/剑指Offer/24","link":"/categories/Algorithm/剑指Offer/24/"},{"name":"26","slug":"Algorithm/剑指Offer/26","link":"/categories/Algorithm/剑指Offer/26/"},{"name":"29","slug":"Algorithm/剑指Offer/29","link":"/categories/Algorithm/剑指Offer/29/"},{"name":"27","slug":"Algorithm/剑指Offer/27","link":"/categories/Algorithm/剑指Offer/27/"},{"name":"30","slug":"Algorithm/剑指Offer/30","link":"/categories/Algorithm/剑指Offer/30/"},{"name":"31","slug":"Algorithm/剑指Offer/31","link":"/categories/Algorithm/剑指Offer/31/"},{"name":"33","slug":"Algorithm/剑指Offer/33","link":"/categories/Algorithm/剑指Offer/33/"},{"name":"32","slug":"Algorithm/剑指Offer/32","link":"/categories/Algorithm/剑指Offer/32/"},{"name":"34","slug":"Algorithm/剑指Offer/34","link":"/categories/Algorithm/剑指Offer/34/"},{"name":"35","slug":"Algorithm/剑指Offer/35","link":"/categories/Algorithm/剑指Offer/35/"},{"name":"37","slug":"Algorithm/剑指Offer/37","link":"/categories/Algorithm/剑指Offer/37/"},{"name":"36","slug":"Algorithm/剑指Offer/36","link":"/categories/Algorithm/剑指Offer/36/"},{"name":"38","slug":"Algorithm/剑指Offer/38","link":"/categories/Algorithm/剑指Offer/38/"},{"name":"28","slug":"Algorithm/剑指Offer/28","link":"/categories/Algorithm/剑指Offer/28/"},{"name":"39","slug":"Algorithm/剑指Offer/39","link":"/categories/Algorithm/剑指Offer/39/"},{"name":"41","slug":"Algorithm/剑指Offer/41","link":"/categories/Algorithm/剑指Offer/41/"},{"name":"40","slug":"Algorithm/剑指Offer/40","link":"/categories/Algorithm/剑指Offer/40/"},{"name":"42","slug":"Algorithm/剑指Offer/42","link":"/categories/Algorithm/剑指Offer/42/"},{"name":"43","slug":"Algorithm/剑指Offer/43","link":"/categories/Algorithm/剑指Offer/43/"},{"name":"44","slug":"Algorithm/剑指Offer/44","link":"/categories/Algorithm/剑指Offer/44/"},{"name":"46","slug":"Algorithm/剑指Offer/46","link":"/categories/Algorithm/剑指Offer/46/"},{"name":"47","slug":"Algorithm/剑指Offer/47","link":"/categories/Algorithm/剑指Offer/47/"},{"name":"48","slug":"Algorithm/剑指Offer/48","link":"/categories/Algorithm/剑指Offer/48/"},{"name":"49","slug":"Algorithm/剑指Offer/49","link":"/categories/Algorithm/剑指Offer/49/"},{"name":"45","slug":"Algorithm/剑指Offer/45","link":"/categories/Algorithm/剑指Offer/45/"},{"name":"GoogLeNet","slug":"DeepLearning/Classification/GoogLeNet","link":"/categories/DeepLearning/Classification/GoogLeNet/"},{"name":"ResNet","slug":"DeepLearning/Classification/ResNet","link":"/categories/DeepLearning/Classification/ResNet/"},{"name":"JVM","slug":"JavaWeb/JVM","link":"/categories/JavaWeb/JVM/"},{"name":"Overview","slug":"DeepLearning/Overview","link":"/categories/DeepLearning/Overview/"}]}