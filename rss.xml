<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gojay&#39;s Records</title>
  
  
  <link href="/rss.xml" rel="self"/>
  
  <link href="https://gojay.top/"/>
  <updated>2020-08-29T07:17:59.581Z</updated>
  <id>https://gojay.top/</id>
  
  <author>
    <name>Gojay</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>剑指Offer-10-I.斐波那契数列</title>
    <link href="https://gojay.top/2020/08/27/%E5%89%91%E6%8C%87Offer-10-I-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/"/>
    <id>https://gojay.top/2020/08/27/剑指Offer-10-I-斐波那契数列/</id>
    <published>2020-08-27T07:01:45.000Z</published>
    <updated>2020-08-29T07:17:59.581Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>写一个函数，输入 <code>n</code> ，求斐波那契（Fibonacci）数列的第 <code>n</code> 项。斐波那契数列的定义如下：  </p><blockquote><p>F(0) = 0,   F(1) = 1<br>F(N) = F(N - 1) + F(N - 2), 其中 N &gt; 1.  </p></blockquote><p><strong>斐波那契数列</strong>由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。<br>答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。</p><a id="more"></a><h2 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h2><blockquote><p><strong>输入</strong>：n = 2<br><strong>输出</strong>：1</p></blockquote><h2 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h2><blockquote><p><strong>输入</strong>：n = 5<br><strong>输出</strong>：5</p></blockquote><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h2><blockquote><p>把f(n) <code>拆解</code> 为f(n-1)和f(n-2)递归计算，以f(1)和f(0)为终止条件。<br>大量重复递归计算，会直接超时。<br><strong>时间复杂度</strong>：O($2^n$)<br><strong>空间复杂度</strong>：O(n)</p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fib</span><span class="hljs-params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> n &lt;= <span class="hljs-number">0</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> n</span><br><span class="line">        <span class="hljs-keyword">return</span> self.fib(n<span class="hljs-number">-1</span>) + self.fib(n<span class="hljs-number">-2</span>)</span><br></pre></td></tr></table></figure><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><ul><li><strong>状态定义</strong>：dp为一维数组，dp[i]为斐波那契数列的第i个值。  </li><li><strong>转移方程</strong>：dp[i] = dp[i-1] + dp[i-2]。  </li><li><strong>初始状态</strong>：dp[0] = 0, dp[1] = 1。</li></ul><blockquote><p><strong>时间复杂度</strong>：O(n)，循环n次，每次O(1)。<br><strong>空间复杂度</strong>：O(n)，dp[n]占用O(n)。</p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fib</span><span class="hljs-params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> n &lt;= <span class="hljs-number">0</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> n</span><br><span class="line">        dp = []</span><br><span class="line">        dp.append(<span class="hljs-number">0</span>)</span><br><span class="line">        dp.append(<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>, n+<span class="hljs-number">1</span>):</span><br><span class="line">            dp_tmp = (dp[i<span class="hljs-number">-1</span>] + dp[i<span class="hljs-number">-2</span>]) % <span class="hljs-number">1000000007</span></span><br><span class="line">            dp.append(dp_tmp)</span><br><span class="line">        <span class="hljs-keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure><h2 id="动态规划（空间优化）"><a href="#动态规划（空间优化）" class="headerlink" title="动态规划（空间优化）"></a>动态规划（空间优化）</h2><blockquote><p>利用 <code>辅助变量使a, b</code> 交替前进，节省了dp[]列表空间。<br><strong>时间复杂度</strong>：O(n)，循环n次，每次O(1)。<br><strong>空间复杂度</strong>：O(1)，变量占用O(1)。</p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fib</span><span class="hljs-params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        a, b = <span class="hljs-number">0</span>, <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n):</span><br><span class="line">            a, b = b, a + b</span><br><span class="line">        <span class="hljs-keyword">return</span> a % <span class="hljs-number">1000000007</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;p&gt;写一个函数，输入 &lt;code&gt;n&lt;/code&gt; ，求斐波那契（Fibonacci）数列的第 &lt;code&gt;n&lt;/code&gt; 项。斐波那契数列的定义如下：  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;F(0) = 0,   F(1) = 1&lt;br&gt;F(N) = F(N - 1) + F(N - 2), 其中 N &amp;gt; 1.  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;斐波那契数列&lt;/strong&gt;由 0 和 1 开始，之后的斐波那契数就是由之前的两数相加而得出。&lt;br&gt;答案需要取模 1e9+7（1000000007），如计算初始结果为：1000000008，请返回 1。&lt;/p&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="10" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/10/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="递归" scheme="https://gojay.top/tags/%E9%80%92%E5%BD%92/"/>
    
      <category term="动态规划" scheme="https://gojay.top/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer-09-用两个栈实现队列</title>
    <link href="https://gojay.top/2020/08/26/%E5%89%91%E6%8C%87Offer-09-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"/>
    <id>https://gojay.top/2020/08/26/剑指Offer-09-用两个栈实现队列/</id>
    <published>2020-08-26T06:59:35.000Z</published>
    <updated>2020-08-26T08:00:05.864Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p><strong>用两个栈实现一个队列</strong>。队列的声明如下，请实现它的两个函数 <code>appendTail</code> 和 <code>deleteHead</code> ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，<code>deleteHead</code> 操作返回 <code>-1</code> ).</p></blockquote><a id="more"></a><h2 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h2><blockquote><p><strong>输入</strong>：<br>[“CQueue”,”appendTail”,”deleteHead”,”deleteHead”]<br>[[],[3],[],[]]<br><strong>输出</strong>：[null,null,3,-1]</p></blockquote><h2 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h2><blockquote><p><strong>输入</strong>：<br>[“CQueue”,”deleteHead”,”appendTail”,”appendTail”,”deleteHead”,”deleteHead”]<br>[[],[],[5],[2],[],[]]<br><strong>输出</strong>：[null,-1,null,null,5,2]</p></blockquote><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="双栈分别正序和逆序"><a href="#双栈分别正序和逆序" class="headerlink" title="双栈分别正序和逆序"></a>双栈分别正序和逆序</h2><blockquote><p><strong>栈</strong>：后入先出；<strong>队列</strong>：先入先出。<br><strong>栈1</strong>：正序（后入先出）；<strong>栈2</strong>：栈1的逆序。<br><strong>实现逆序</strong>：循环执行将栈1的元素出栈，并将其入栈至栈2，直到栈1为空。即 <code>stack2.append(stack1.pop())</code> 。<br><strong>appendTail()</strong>：直接将元素 <code>入栈至栈1</code> 。<br><strong>deleteHead()</strong>：1. 栈2不为空， <code>return stack2.pop()</code> ；2. 栈1为空， <code>return -1</code> ；3. 将栈1逆序输出到栈2， <code>return stack2.pop()</code> 。<br><strong>时间复杂度</strong>： <code>appendTail()</code> 为O(1)； <code>deleteTail()</code> 为O(n)，先遍历输出逆序。<br><strong>空间复杂度</strong>：O(n)，栈1和栈2共保存n个元素。</p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CQueue</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        self.stack1, self.stack2 = [], []</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">appendTail</span><span class="hljs-params">(self, value: int)</span> -&gt; <span class="hljs-keyword">None</span>:</span></span><br><span class="line">        self.stack1.append(value)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">deleteHead</span><span class="hljs-params">(self)</span> -&gt; int:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> self.stack2:</span><br><span class="line">            <span class="hljs-keyword">return</span> self.stack2.pop()</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.stack1:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span></span><br><span class="line">        <span class="hljs-keyword">while</span> self.stack1:</span><br><span class="line">            self.stack2.append(self.stack1.pop())</span><br><span class="line">        <span class="hljs-keyword">return</span> self.stack2.pop()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Your CQueue object will be instantiated and called as such:</span></span><br><span class="line"><span class="hljs-comment"># obj = CQueue()</span></span><br><span class="line"><span class="hljs-comment"># obj.appendTail(value)</span></span><br><span class="line"><span class="hljs-comment"># param_2 = obj.deleteHead()</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;用两个栈实现一个队列&lt;/strong&gt;。队列的声明如下，请实现它的两个函数 &lt;code&gt;appendTail&lt;/code&gt; 和 &lt;code&gt;deleteHead&lt;/code&gt; ，分别完成在队列尾部插入整数和在队列头部删除整数的功能。(若队列中没有元素，&lt;code&gt;deleteHead&lt;/code&gt; 操作返回 &lt;code&gt;-1&lt;/code&gt; ).&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="09" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/09/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="栈" scheme="https://gojay.top/tags/%E6%A0%88/"/>
    
      <category term="队列" scheme="https://gojay.top/tags/%E9%98%9F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer-07-重建二叉树</title>
    <link href="https://gojay.top/2020/08/25/%E5%89%91%E6%8C%87Offer-07-%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    <id>https://gojay.top/2020/08/25/剑指Offer-07-重建二叉树/</id>
    <published>2020-08-25T08:05:59.000Z</published>
    <updated>2020-08-26T06:52:16.073Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>输入某二叉树的 <code>前序遍历和中序遍历</code> 的结果，请 <code>重建该二叉树</code> 。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。</p></blockquote><a id="more"></a><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><ul><li><p><strong>输入</strong>： </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">前序遍历 preorder = [3,9,20,15,7]  </span><br><span class="line">中序遍历 inorder = [9,3,15,20,7]</span><br></pre></td></tr></table></figure></li><li><p><strong>输出</strong>：返回如下二叉树</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">  3  </span><br><span class="line"> / \  </span><br><span class="line">9  20  </span><br><span class="line">  /  \  </span><br><span class="line"> 15   7</span><br></pre></td></tr></table></figure></li></ul><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="原地递归"><a href="#原地递归" class="headerlink" title="原地递归"></a>原地递归</h2><blockquote><p><strong>前序遍历</strong>：根节点、左子树、右子树；<strong>中序遍历</strong>：左子树、根节点、右子树。<br>已知前序遍历和中序遍历重建二叉树：</p><ul><li><strong>递推</strong>：前序遍历的第一个节点为 <code>根节点</code> ，找到其在中序遍历中的对应位置，在根节点左边为 <code>左子树</code> ，在根节点右边为 <code>右子树</code> 。将得到的左右子树作为新序列， <code>递归</code> 划分其左右子树。  </li><li><strong>终止</strong>：前序或中序为空，即到达叶子节点，return None。  </li></ul><p><strong>时间复杂度</strong>：O($n^2$)，以根节点遍历序列，每次递归左右子树。<br><strong>空间复杂度</strong>：O(n)，递归使用O(n)空间。</p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="hljs-comment"># class TreeNode:</span></span><br><span class="line"><span class="hljs-comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="hljs-comment">#         self.val = x</span></span><br><span class="line"><span class="hljs-comment">#         self.left = None</span></span><br><span class="line"><span class="hljs-comment">#         self.right = None</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">buildTree</span><span class="hljs-params">(self, preorder: List[int], inorder: List[int])</span> -&gt; TreeNode:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> preorder <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> inorder:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span></span><br><span class="line">        loc = inorder.index(preorder[<span class="hljs-number">0</span>])</span><br><span class="line">        root = TreeNode(preorder[<span class="hljs-number">0</span>])</span><br><span class="line">        root.left = self.buildTree(preorder[<span class="hljs-number">1</span> : loc+<span class="hljs-number">1</span>], inorder[ : loc])</span><br><span class="line">        root.right = self.buildTree(preorder[loc+<span class="hljs-number">1</span> : ], inorder[loc+<span class="hljs-number">1</span> : ])</span><br><span class="line">        <span class="hljs-keyword">return</span> root</span><br></pre></td></tr></table></figure><h2 id="哈希表递归"><a href="#哈希表递归" class="headerlink" title="哈希表递归"></a>哈希表递归</h2><p>(<a href="https://leetcode-cn.com/problems/zhong-jian-er-cha-shu-lcof/solution/mian-shi-ti-07-zhong-jian-er-cha-shu-di-gui-fa-qin/" target="_blank" rel="noopener">引用Krahets</a>)</p><blockquote><p>使用哈希表预存储 <code>中序遍历的值与索引</code> 的映射关系，每次搜索的时间复杂度为O(1)。<br><strong>方法参数</strong>： <code>pre_root</code> ：前序根节点索引； <code>in_left</code> ：中序左边界； <code>in_right</code> ：中序右边界。<br><strong>子树根节点索引</strong>：左子树根节点为 <code>前序根节点+1</code> ；右子树根节点为 <code>前序根节点+1+左子树长度(中序根节点索引-左边界)</code> 。<br><strong>时间复杂度</strong>：O(n)，初始化HashMap遍历inorder，占用O(n)；每层递归中的节点建立、搜索操作占用O(1)。<br><strong>空间复杂度</strong>：O(n)，HashMap使用O(n)额外空间；递归操作中系统需使用O(n)额外空间。  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">buildTree</span><span class="hljs-params">(self, preorder: List[int], inorder: List[int])</span> -&gt; TreeNode:</span></span><br><span class="line">        self.dic, self.preorder = &#123;&#125;, preorder</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(inorder)):</span><br><span class="line">            self.dic[inorder[i]] = i</span><br><span class="line">        <span class="hljs-keyword">return</span> self.recur(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, len(inorder)<span class="hljs-number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">recur</span><span class="hljs-params">(self, pre_root, in_left, in_right)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> in_left &gt; in_right: </span><br><span class="line">            <span class="hljs-keyword">return</span></span><br><span class="line">        root = TreeNode(self.preorder[pre_root])</span><br><span class="line">        loc = self.dic[self.preorder[pre_root]]</span><br><span class="line">        root.left = self.recur(pre_root+<span class="hljs-number">1</span>, in_left, loc<span class="hljs-number">-1</span>)</span><br><span class="line">        root.right = self.recur(pre_root+<span class="hljs-number">1</span>+loc-in_left, loc+<span class="hljs-number">1</span>, in_right)</span><br><span class="line">        <span class="hljs-keyword">return</span> root</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;输入某二叉树的 &lt;code&gt;前序遍历和中序遍历&lt;/code&gt; 的结果，请 &lt;code&gt;重建该二叉树&lt;/code&gt; 。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="7" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/7/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="递归" scheme="https://gojay.top/tags/%E9%80%92%E5%BD%92/"/>
    
      <category term="二叉树" scheme="https://gojay.top/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
    
      <category term="HashMap" scheme="https://gojay.top/tags/HashMap/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer-06-从尾到头打印链表</title>
    <link href="https://gojay.top/2020/08/24/%E5%89%91%E6%8C%87Offer-06-%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/"/>
    <id>https://gojay.top/2020/08/24/剑指Offer-06-从尾到头打印链表/</id>
    <published>2020-08-24T07:09:34.000Z</published>
    <updated>2020-08-25T08:07:33.196Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>输入一个链表的头节点， <code>从尾到头反过来返回每个节点的值</code> （用数组返回）。</p></blockquote><a id="more"></a><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><blockquote><p><strong>输入</strong>：head = [1,3,2]<br><strong>输出</strong>：[2,3,1]  </p></blockquote><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="递归解法"><a href="#递归解法" class="headerlink" title="递归解法"></a>递归解法</h2><blockquote><p>先走到链表末端，回溯时依次将节点元素加入列表。<br><strong>递推</strong>：每次传入 <code>head.next</code> ，以 <code>head == None</code> 为终止条件，此时返回空列表 <code>[]</code> 。<br><strong>回溯</strong>：每次返回 <code>当前list + 当前节点元素[head.val]</code> 。<br><strong>时间复杂度</strong>：O(n)，遍历链表，递归n次。<br><strong>空间复杂度</strong>：O(n)，递归使用O(n)空间。  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="hljs-comment"># class ListNode:</span></span><br><span class="line"><span class="hljs-comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="hljs-comment">#         self.val = x</span></span><br><span class="line"><span class="hljs-comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reversePrint</span><span class="hljs-params">(self, head: ListNode)</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> head <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            <span class="hljs-keyword">return</span> []</span><br><span class="line">        <span class="hljs-keyword">return</span> self.reversePrint(head.next) + [head.val]</span><br></pre></td></tr></table></figure><h2 id="辅助栈"><a href="#辅助栈" class="headerlink" title="辅助栈"></a>辅助栈</h2><blockquote><p>使用Python列表中的reverse方法： <code>list.reverse()</code> 或 <code>list[::-1]</code> ，反向列表中元素。<br>遍历链表，依次将节点元素 <code>push入栈</code> （使用append方法实现）；然后将节点元素 <code>pop出栈</code> （反向）。<br><strong>时间复杂度</strong>：O(n)<br><strong>空间复杂度</strong>：O(n)  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reversePrint</span><span class="hljs-params">(self, head: ListNode)</span> -&gt; List[int]:</span></span><br><span class="line">        stack = []</span><br><span class="line">        <span class="hljs-keyword">while</span> head:</span><br><span class="line">            stack.append(head.val)</span><br><span class="line">            head = head.next</span><br><span class="line">        <span class="hljs-keyword">return</span> stack[::<span class="hljs-number">-1</span>]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;输入一个链表的头节点， &lt;code&gt;从尾到头反过来返回每个节点的值&lt;/code&gt; （用数组返回）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="6" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/6/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="递归" scheme="https://gojay.top/tags/%E9%80%92%E5%BD%92/"/>
    
      <category term="栈" scheme="https://gojay.top/tags/%E6%A0%88/"/>
    
      <category term="链表" scheme="https://gojay.top/tags/%E9%93%BE%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer-05-替换空格</title>
    <link href="https://gojay.top/2020/08/24/%E5%89%91%E6%8C%87Offer-05-%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/"/>
    <id>https://gojay.top/2020/08/24/剑指Offer-05-替换空格/</id>
    <published>2020-08-24T06:35:47.000Z</published>
    <updated>2020-08-24T07:42:56.007Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>请实现一个函数，把 <code>字符串 s 中的每个空格替换成&quot;%20&quot;</code> 。</p></blockquote><a id="more"></a><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><blockquote><p><strong>输入</strong>：s = “We are happy.”<br><strong>输出</strong>：”We%20are%20happy.”  </p></blockquote><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="新建字符串"><a href="#新建字符串" class="headerlink" title="新建字符串"></a>新建字符串</h2><blockquote><p>在Python语言中， <code>字符串被设计成不可变类型</code> ，即无法直接修改字符串的某一位字符，需要 <code>新建一个字符串</code> 实现。<br>初始化一个list； <code>遍历字符串s中每个字符c</code> ，若c为空格，则在list中添加”%20”；若c不为空格，则在list中添加字符c。<br><strong>时间复杂度</strong>：O(n)，遍历O(n)，每轮添加O(1)。<br><strong>空间复杂度</strong>：O(n)，新建list使用线性大小空间。  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">replaceSpace</span><span class="hljs-params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> s:</span><br><span class="line">            <span class="hljs-keyword">if</span> c == <span class="hljs-string">' '</span>: </span><br><span class="line">                res.append(<span class="hljs-string">"%20"</span>)</span><br><span class="line">            <span class="hljs-keyword">else</span>: </span><br><span class="line">                res.append(c)</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-string">""</span>.join(res)</span><br></pre></td></tr></table></figure><h2 id="Python特性"><a href="#Python特性" class="headerlink" title="Python特性"></a>Python特性</h2><blockquote><p>使用Python字符串中的replace方法： <code>str.replace(old, new[, max])</code> ，将字符串中的 str1 替换成 str2,如果 max 指定，则替换不超过 max 次。<br><strong>时间复杂度</strong>：O(n)<br><strong>空间复杂度</strong>：O(n)  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">replaceSpace</span><span class="hljs-params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> s.replace(<span class="hljs-string">" "</span>, <span class="hljs-string">"%20"</span>);</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;请实现一个函数，把 &lt;code&gt;字符串 s 中的每个空格替换成&amp;quot;%20&amp;quot;&lt;/code&gt; 。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="5" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/5/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="字符串" scheme="https://gojay.top/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer-04-二维数组中的查找</title>
    <link href="https://gojay.top/2020/08/21/%E5%89%91%E6%8C%87Offer-04-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/"/>
    <id>https://gojay.top/2020/08/21/剑指Offer-04-二维数组中的查找/</id>
    <published>2020-08-21T08:02:19.000Z</published>
    <updated>2020-08-24T07:42:41.463Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，<code>输入这样的一个二维数组和一个整数，判断数组中是否含有该整数</code>。</p></blockquote><a id="more"></a><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><blockquote><p>现有矩阵 matrix 如下：  </p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">  [1,   4,  7, 11, 15],</span><br><span class="line">  [2,   5,  8, 12, 19],</span><br><span class="line">  [3,   6,  9, 16, 22],</span><br><span class="line">  [10, 13, 14, 17, 24],</span><br><span class="line">  [18, 21, 23, 26, 30]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><blockquote><p>给定 target = <code>5</code>，返回 <code>true</code>。<br>给定 target = <code>20</code>，返回 <code>false</code>。</p></blockquote><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="暴力解法"><a href="#暴力解法" class="headerlink" title="暴力解法"></a>暴力解法</h2><blockquote><p><code>按行依次搜索</code>target，如果当前元素大于target，则进行下一行搜索；若当前元素等于target，则return True；未找到return False。<br><strong>时间复杂度</strong>：O(nm)，最好情况O(1)<br><strong>空间复杂度</strong>：O(1)  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findNumberIn2DArray</span><span class="hljs-params">(self, matrix: List[List[int]], target: int)</span> -&gt; bool:</span></span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(matrix)):</span><br><span class="line">            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(len(matrix[i])):</span><br><span class="line">                <span class="hljs-keyword">if</span> matrix[i][j] == target:</span><br><span class="line">                    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span></span><br><span class="line">                <span class="hljs-keyword">elif</span> matrix[i][j] &gt; target:</span><br><span class="line">                    <span class="hljs-keyword">continue</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span></span><br></pre></td></tr></table></figure><h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><blockquote><p>按行依次搜索target，<code>每次从中间元素进行匹配</code>，如果该行未找到，则进入下一行搜索；若找到则return True；未找到return False。<br><strong>时间复杂度</strong>：O(nlogm)<br><strong>空间复杂度</strong>：O(1)  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findNumberIn2DArray</span><span class="hljs-params">(self, matrix: List[List[int]], target: int)</span> -&gt; bool:</span></span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(matrix)):</span><br><span class="line">            left, right = <span class="hljs-number">0</span>, len(matrix[i])<span class="hljs-number">-1</span></span><br><span class="line">            <span class="hljs-keyword">while</span> left &lt;= right:</span><br><span class="line">                mid = (left+right) // <span class="hljs-number">2</span></span><br><span class="line">                <span class="hljs-keyword">if</span> target &lt; matrix[i][mid]:</span><br><span class="line">                    right = mid - <span class="hljs-number">1</span></span><br><span class="line">                <span class="hljs-keyword">elif</span> target &gt; matrix[i][mid]:</span><br><span class="line">                    left = mid + <span class="hljs-number">1</span></span><br><span class="line">                <span class="hljs-keyword">else</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span></span><br></pre></td></tr></table></figure><h2 id="二叉搜索树"><a href="#二叉搜索树" class="headerlink" title="二叉搜索树"></a>二叉搜索树</h2><p><img src="https://i.loli.net/2020/08/23/3RJrhp4V5SdFyEa.png" alt="剑指Offer-04.png"></p><blockquote><p><code>从右上角开始搜索，将矩阵转化为二叉搜索树</code>,如上图所示。右上角元素作为根节点，如果target小于当前节点，则向左下方搜索，即向左边列搜索；如果target大于当前节点，则向右下方搜索，即向下一行搜索；否则为找到target，return True。若行索引或列索引越界，即矩阵中无target值，return False。<br><code>每轮迭代相当于将矩阵删除一行（列）</code>，得到新矩阵进行迭代。<br><strong>时间复杂度</strong>：O(n+m)<br><strong>空间复杂度</strong>：O(1)</p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findNumberIn2DArray</span><span class="hljs-params">(self, matrix: List[List[int]], target: int)</span> -&gt; bool:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> matrix: </span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span></span><br><span class="line">        i, j = <span class="hljs-number">0</span>, len(matrix[<span class="hljs-number">0</span>])<span class="hljs-number">-1</span></span><br><span class="line">        <span class="hljs-keyword">while</span> i &lt;= len(matrix)<span class="hljs-number">-1</span> <span class="hljs-keyword">and</span> j &gt;= <span class="hljs-number">0</span>:</span><br><span class="line">            <span class="hljs-keyword">if</span> target &lt; matrix[i][j]: </span><br><span class="line">                j -= <span class="hljs-number">1</span></span><br><span class="line">            <span class="hljs-keyword">elif</span> target &gt; matrix[i][j]: </span><br><span class="line">                i += <span class="hljs-number">1</span></span><br><span class="line">            <span class="hljs-keyword">else</span>: </span><br><span class="line">                <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在一个 n * m 的二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，&lt;code&gt;输入这样的一个二维数组和一个整数，判断数组中是否含有该整数&lt;/code&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="4" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/4/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="数组" scheme="https://gojay.top/tags/%E6%95%B0%E7%BB%84/"/>
    
      <category term="二叉搜索树" scheme="https://gojay.top/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>剑指Offer-03-数组中重复的数字</title>
    <link href="https://gojay.top/2020/08/21/%E5%89%91%E6%8C%87Offer-03-%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/"/>
    <id>https://gojay.top/2020/08/21/剑指Offer-03-数组中重复的数字/</id>
    <published>2020-08-21T01:46:07.000Z</published>
    <updated>2020-08-24T07:42:30.757Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><hr><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><blockquote><p>找出 <code>数组中重复的数字</code> 。<br>在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。</p></blockquote><a id="more"></a><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><blockquote><p><strong>输入</strong>：<br>[2, 3, 1, 0, 2, 5, 3].<br><strong>输出</strong>：2 或 3 </p></blockquote><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><hr><h2 id="排序后查看相邻元素"><a href="#排序后查看相邻元素" class="headerlink" title="排序后查看相邻元素"></a>排序后查看相邻元素</h2><blockquote><p>先 <code>排序，然后查看相邻元素是否相同</code> ；若相同则元素重复，return。该方法运行时间慢。<br><strong>时间复杂度</strong>：O(nlogn)<br><strong>空间复杂度</strong>：O(1)  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findRepeatNumber</span><span class="hljs-params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="hljs-keyword">if</span> nums[i] == nums[i+<span class="hljs-number">1</span>]:</span><br><span class="line">                <span class="hljs-keyword">return</span> nums[i]</span><br></pre></td></tr></table></figure><h2 id="辅助空间"><a href="#辅助空间" class="headerlink" title="辅助空间"></a>辅助空间</h2><blockquote><p>开辟一个新的列表或字典， <code>将nums数值作为新列表的索引并赋值为1</code> ；若该索引已经复制则元素重复，return。<br><strong>时间复杂度</strong>：O(n)<br><strong>空间复杂度</strong>：O(n)  </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findRepeatNumber</span><span class="hljs-params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        repeatDict = &#123;&#125;</span><br><span class="line">        <span class="hljs-keyword">for</span> num <span class="hljs-keyword">in</span> nums:</span><br><span class="line">            <span class="hljs-keyword">if</span> num <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> repeatDict:</span><br><span class="line">                repeatDict[num] = <span class="hljs-number">1</span></span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                <span class="hljs-keyword">return</span> num</span><br></pre></td></tr></table></figure><h2 id="原地交换"><a href="#原地交换" class="headerlink" title="原地交换"></a>原地交换</h2><blockquote><p>与辅助空间思路类似， <code>让索引i位置存放数值i</code> 。如果索引位置i的元素不是i，将把i元素放在对应位置，即nums[nums[i]]与nums[i]交换。若交换时发现索引nums[i]的元素与索引相同，则元素重复，return。<br><strong>时间复杂度</strong>：O(n)<br><strong>空间复杂度</strong>：O(1) </p></blockquote><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">findRepeatNumber</span><span class="hljs-params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="hljs-keyword">while</span> i != nums[i]:</span><br><span class="line">                <span class="hljs-keyword">if</span> nums[i] == nums[nums[i]]:</span><br><span class="line">                    <span class="hljs-keyword">return</span> nums[i]</span><br><span class="line">                nums[nums[i]], nums[i] = nums[i], nums[nums[i]]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;找出 &lt;code&gt;数组中重复的数字&lt;/code&gt; 。&lt;br&gt;在一个长度为 n 的数组 nums 里的所有数字都在 0～n-1 的范围内。数组中某些数字是重复的，但不知道有几个数字重复了，也不知道每个数字重复了几次。请找出数组中任意一个重复的数字。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="https://gojay.top/categories/Algorithm/"/>
    
      <category term="剑指Offer" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/"/>
    
      <category term="3" scheme="https://gojay.top/categories/Algorithm/%E5%89%91%E6%8C%87Offer/3/"/>
    
    
      <category term="Algorithm" scheme="https://gojay.top/tags/Algorithm/"/>
    
      <category term="Offer" scheme="https://gojay.top/tags/Offer/"/>
    
      <category term="数组" scheme="https://gojay.top/tags/%E6%95%B0%E7%BB%84/"/>
    
      <category term="字典" scheme="https://gojay.top/tags/%E5%AD%97%E5%85%B8/"/>
    
  </entry>
  
  <entry>
    <title>Mask R-CNN</title>
    <link href="https://gojay.top/2020/08/17/Mask-R-CNN/"/>
    <id>https://gojay.top/2020/08/17/Mask-R-CNN/</id>
    <published>2020-08-17T13:37:19.000Z</published>
    <updated>2020-08-19T15:05:10.355Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Mask R-CNN</strong><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" target="_blank" rel="noopener">[1]</a> is a framework for object <strong>instance segmentation</strong>, which adds a branch for <code>predicting an object mask</code> in parallel with the existing branch for bounding box recognition of <code>Faster R-CNN</code>. There are some details of reading and implementing it.  </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Mask R-CNN</a>(ICCV 2017 paper)<br><strong>Code</strong>: <a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">Pytorch</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=7cf81d53-02f7-9c65-0040-d1a695798524&documentId=c8be9f0f-e376-399b-96d7-5470dfd95a2a" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/08/19/CYqtbVd9SJoDkwa.png" alt="Mask R-CNN_Abstract.png">  </p><blockquote><ol><li>It is a framework for object <strong>instance segmentation</strong>.  </li><li>It extends <code>Faster R-CNN</code> by adding a branch for <strong>predicting an object mask</strong> in parallel with the existing branch for bounding box recognition.  </li><li>It challenges instance <code>segmentation</code>, bounding-box object <code>detection</code>, and person <code>keypoint</code> detection.  </li><li>It serves as a solid <strong>baseline</strong> in instance-level recognition.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/08/19/ZON9UKSFLe2RVaz.png" alt="Mask R-CNN_PD.png"></p><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/08/19/WwRoj1sS4Y5fuaC.png" alt="Mask R-CNN_PS.png"></p><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/08/19/JBnDjVRAO6Hl5si.png" alt="Mask R-CNN_framework.png"></p><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h3><p><img src="https://i.loli.net/2020/08/19/4NZEos2dGHUVpIc.png" alt="Mask R-CNN_Loss.png"></p><h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p><img src="https://i.loli.net/2020/08/19/ADjVwQFTWKcbgS8.png" alt="Mask R-CNN_mask.png"><br><img src="https://i.loli.net/2020/08/19/4jErPFaW9RAI3UM.png" alt="Mask R-CNN_Head.png"></p><h3 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h3><p><img src="https://i.loli.net/2020/08/19/78pRm3IBwaEcY2h.png" alt="Mask R-CNN_RoIAlign.png"><br><img src="https://i.loli.net/2020/08/19/kDOiJQuMpRHLs3o.png" alt="Mask R-CNN_RoIAlign-details.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/08/19/eGxm6bV48LyXEdj.png" alt="Mask R-CNN_results.png"><br><img src="https://i.loli.net/2020/08/19/cg5apbFo93rZQDS.png" alt="Mask R-CNN_Ablations.png"><br><img src="https://i.loli.net/2020/08/19/kRViK42PfIChMsu.png" alt="Mask R-CNN_AP.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in detectron2<a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">[2]</a>.  </p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details of <strong>Mask R-CNN and its extends</strong> like RoIAlign, bilinear interpolation and etc. can be found in <a href="https://zhuanlan.zhihu.com/p/37998710" target="_blank" rel="noopener">[3]</a>.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969.<br>[2] detectron2. <a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">https://github.com/facebookresearch/detectron2</a><br>[3] stone. “The amazing Mask R-CNN.” <a href="https://zhuanlan.zhihu.com/p/37998710" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37998710</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Mask R-CNN&lt;/strong&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a framework for object &lt;strong&gt;instance segmentation&lt;/strong&gt;, which adds a branch for &lt;code&gt;predicting an object mask&lt;/code&gt; in parallel with the existing branch for bounding box recognition of &lt;code&gt;Faster R-CNN&lt;/code&gt;. There are some details of reading and implementing it.  &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Segmentation" scheme="https://gojay.top/categories/DeepLearning/Object-Segmentation/"/>
    
      <category term="Mask R-CNN" scheme="https://gojay.top/categories/DeepLearning/Object-Segmentation/Mask-R-CNN/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Segmentation" scheme="https://gojay.top/tags/Segmentation/"/>
    
      <category term="Mask R-CNN" scheme="https://gojay.top/tags/Mask-R-CNN/"/>
    
  </entry>
  
  <entry>
    <title>LTM</title>
    <link href="https://gojay.top/2020/07/29/LTM/"/>
    <id>https://gojay.top/2020/07/29/LTM/</id>
    <published>2020-07-29T02:59:27.000Z</published>
    <updated>2020-08-03T05:01:06.773Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>LTM</strong>(Local Transformation Module)<a href="https://arxiv.org/abs/1910.05886" target="_blank" rel="noopener">[1]</a> focus on the relationship of the <code>local features</code>. It uses linear transformation of the <code>relationship matrix</code> in a high-dimensional metric embedding space to accomplish the transformation. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1910.05886" target="_blank" rel="noopener">A New Local Transformation Module for Few-Shot Segmentation</a>(ICMM 2020 paper)<br><strong>Code</strong>: [Code]<br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=7e97e94c-5aa2-0441-4595-bb8d91bb7ab8&documentId=fee65a92-ec26-3b52-bd92-018e98345b05" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/07/29/UKipYnbFdhNjW4y.png" alt="LTM_Abstract.png"></p><blockquote><ol><li>The key step of <strong>few-shot segmentation</strong> is to establish the <code>transformation</code> module.  </li><li>The <strong>existing methods</strong> form transformation model based on <code>global cues</code>, which however ignores the local cues.  </li><li>This paper proposes a new transformation module based on <strong>local cues</strong>,<br><code>relationship matrix with cosine distance</code> to enhance the generalization, <code>generalized inverse matrix</code> to handle the challenging mapping problem.  </li><li>It outperforms the <strong>state-of-the-art</strong> method on the <code>PASCAL VOC 2012</code> dataset.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/07/29/X3QRyhvDMTIPisc.png" alt="LTM_PD.png"></p><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/07/29/cpxNvzQIy6eME5f.png" alt="LTM_PS.png"></p><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/07/29/PRdjEUhO5NTrk9B.png" alt="LTM_Pipeline.png"></p><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2020/07/29/RtAkOYDB72mJ54P.png" alt="LTM_Transformation.png"><br><img src="https://i.loli.net/2020/07/29/DKkthF678NarubT.png" alt="LTM_Upsample.png"></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><blockquote><p>$F^\prime_{s}(i,j)=F_{s}(i,j) \times {G_{s}(i,j)}$<br>$\hat{F^\prime_{q}}(i,j)=\hat{F_{q}}(i,j) \times {A(i,j)}$<br>$R_{ij}=\frac{\langle E_{si},E_{qj}\rangle}{||E_{si}|| ||E_{qj}||}$<br>$R_{truth}=G_{q} \cdot G_{s}$<br>$R=A \cdot G_{s}$<br>$A=R \cdot [\left( G_{s} \right)^T \left( G_{s} \left( G_{s} \right)^T \right)^{-1}]$<br>$\hat{A}=\frac{A-\min{(A)}}{\max{(A)}-\min{(A)}}$  </p></blockquote><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><blockquote><p>$L_{m}=\sum_{i}\sum_{j}-(Y(i,j)\log(M(i,j))+(1-Y(i,j))\log(1-M(i,j)))$<br>$L_{a}=\sum_{i}\sum_{j}-(Y(i,j)\log(M_{a}(i,j))+(1-Y(i,j))\log(1-M_{a}(i,j)))$<br>$L_{r}=||R-R_{truth}||_2^2$  </p></blockquote><blockquote><p>$L=\lambda_{m}L_{m}+\lambda_{a}L_{a}+\lambda{r}L_{r}$</p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/07/29/zPGwoqd91kFCb2R.png" alt="LTM_Results.png"><br><img src="https://i.loli.net/2020/07/29/DnKgQzRyUIrF7st.png" alt="LTM_Setting.png"><br><img src="https://i.loli.net/2020/07/29/Zui9QbzweGYWSIl.png" alt="LTM_1-shot.png"><br><img src="https://i.loli.net/2020/07/29/Ig7CtXsNZVMQKS4.png" alt="LTM_5-shot.png"><br><img src="https://i.loli.net/2020/07/29/7TkbGpmjMVOq5HC.png" alt="LTM_FB-IoU.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p>[Updating]</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Yang Y, Meng F, Li H, et al. A new local transformation module for few-shot segmentation[C]//International Conference on Multimedia Modeling. Springer, Cham, 2020: 76-87.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LTM&lt;/strong&gt;(Local Transformation Module)&lt;a href=&quot;https://arxiv.org/abs/1910.05886&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; focus on the relationship of the &lt;code&gt;local features&lt;/code&gt;. It uses linear transformation of the &lt;code&gt;relationship matrix&lt;/code&gt; in a high-dimensional metric embedding space to accomplish the transformation. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Few-Shot Segmentation" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Segmentation/"/>
    
      <category term="LTM" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Segmentation/LTM/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="FSS" scheme="https://gojay.top/tags/FSS/"/>
    
      <category term="LTM" scheme="https://gojay.top/tags/LTM/"/>
    
  </entry>
  
  <entry>
    <title>PGNet</title>
    <link href="https://gojay.top/2020/07/28/PGNet/"/>
    <id>https://gojay.top/2020/07/28/PGNet/</id>
    <published>2020-07-28T07:39:02.000Z</published>
    <updated>2020-07-29T01:47:50.202Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>PGNet</strong>(Pyramid Graph Networks)<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf" target="_blank" rel="noopener">[1]</a> modeled structured segmentation data with <code>graphs</code> and further proposed a <code>pyramid-like</code> structure that models different sizes of image regions as graph nodes. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Pyramid Graph Networks with Connection Attentions for Region-Based One-Shot Semantic Segmentation</a>(ICCV 2019 paper)<br><strong>Code</strong>: [Code]<br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=672b8f91-8a7c-c4d5-293e-66aa74eea9e8&documentId=363d5efa-69a1-3590-ba6f-be219cd095b6" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/07/29/zMuxchgAdr9UGTj.png" alt="PGNet_Abstract.png"></p><blockquote><ol><li><strong>One-shot image segmentation</strong> yields a many-to-many message passing problem with only <code>one training image</code> available.  </li><li><strong>Previous methods</strong> described as one-to-many problem by squeezing support data to a <code>global descriptor</code>.  </li><li>In this work, they model structured segmentation data with <strong>graphs</strong> and apply <code>attentive graph reasoning</code>, <code>graph attention mechanism</code> could establish the element-to-element correspondence, <code>pyramid-like structure</code> is able to  capture correspondence at different semantic levels.  </li><li>It leads to new <strong>state-of-the-art</strong> performance on 1-shot and 5-shot segmentation benchmarks of the <code>PASCAL VOC 2012</code> dataset.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/07/29/sxjrE43viX5B1No.png" alt="PGNet_PD.png"></p><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/07/29/c6reyUIoqORD9K1.png" alt="PGNet_PS.png"><br><img src="https://i.loli.net/2020/07/29/YAroctS45LE9eVD.png" alt="PGNet_episode.png"></p><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/07/29/G53zlNkaSJCFrwv.png" alt="PGNet_Illustration.png"><br><img src="https://i.loli.net/2020/07/29/HiRqwF68s7EpMfk.png" alt="PGNet_Network.png"> </p><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2020/07/29/W93LIonVJFAw5Z2.png" alt="PGNet_GAU.png"><br><img src="https://i.loli.net/2020/07/29/FCrWSKxBiVmqGdI.png" alt="PGNet_correlation.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/07/29/nYi1b57XQkdoUR3.png" alt="PGNet_Results.png"><br><img src="https://i.loli.net/2020/07/29/XNGbSHPeOdUWsou.png" alt="PGNet_Comparison.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p>[Updating]</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Zhang C, Lin G, Liu F, et al. Pyramid graph networks with connection attentions for region-based one-shot semantic segmentation[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 9587-9595.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;PGNet&lt;/strong&gt;(Pyramid Graph Networks)&lt;a href=&quot;https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; modeled structured segmentation data with &lt;code&gt;graphs&lt;/code&gt; and further proposed a &lt;code&gt;pyramid-like&lt;/code&gt; structure that models different sizes of image regions as graph nodes. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Few-Shot Segmentation" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Segmentation/"/>
    
      <category term="PGNet" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Segmentation/PGNet/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="FSS" scheme="https://gojay.top/tags/FSS/"/>
    
      <category term="PGNet" scheme="https://gojay.top/tags/PGNet/"/>
    
  </entry>
  
  <entry>
    <title>CRNet</title>
    <link href="https://gojay.top/2020/07/10/CRNet/"/>
    <id>https://gojay.top/2020/07/10/CRNet/</id>
    <published>2020-07-10T07:56:24.000Z</published>
    <updated>2020-07-16T07:42:10.276Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>CRNet</strong>(Cross-Reference Networks)<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CRNet_Cross-Reference_Networks_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[1]</a> make predictions for both the support image and the query image. It can better find the <code>co-occurrent objects</code> in the two images, thus helping the few-shot segmentation task. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CRNet_Cross-Reference_Networks_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">CRNet: Cross-Reference Networks for Few-Shot Segmentation</a>(CVPR 2020 paper)<br><strong>Code</strong>: [Code]<br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=161a13d8-6526-470d-7889-dd7905e96320&documentId=b44fb606-2cd2-3955-bfa5-1430f6db76d8" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/07/13/IcG7NiFghYf3MBx.png" alt="CRNet_Abstract.png"></p><blockquote><ol><li><strong>Image segmentation</strong> algorithms are based on <code>deep convolutional neural networks</code> in recent years.  </li><li><strong>Few-shot segmentation</strong> aims to learn a segmentation model that can be gener- alized to novel classes with only a <code>few training images</code>.  </li><li>In this work, they propose a cross-reference network (<strong>CRNet</strong>) including <code>cross-reference module</code> for finding the co-occurrent objects and <code>mask refinement</code> module for refining predictions.  </li><li>It achieves <strong>state-of-the-art</strong> performance on the <code>PASCAL VOC 2012</code> dataset.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/07/13/uZmHLzYG8TpRCVg.png" alt="CRNet_PD.png"></p><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/07/13/ZfexKDjHP9QmRJ3.png" alt="CRNet_PS.png"></p><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/07/13/VY4yIAuv2ieTN1g.png" alt="CRNet_Comparison.png"><br><img src="https://i.loli.net/2020/07/13/AWUHx93gQ8NmXlT.png" alt="CRNet_Network.png"> </p><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2020/07/13/JaSxqzYCcLiyArM.png" alt="CRNet_cross-reference.png"><br><img src="https://i.loli.net/2020/07/13/eOvZuxcfjEwMCah.png" alt="CRNet_condition.png"><br><img src="https://i.loli.net/2020/07/13/CJV9j5ctwrinuAg.png" alt="CRNet_refinement.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/07/13/wymXlRYZdpJfGcz.png" alt="CRNet_examples.png"><br><img src="https://i.loli.net/2020/07/13/tB7KTQGSkpudmNg.png" alt="CRNet_Comparison-1-shot.png"><br><img src="https://i.loli.net/2020/07/13/ExJOW6Gk7Us2PbS.png" alt="CRNet_Comparison-5-shot.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p>[Updating]</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Liu W, Zhang C, Lin G, et al. CRNet: Cross-Reference Networks for Few-Shot Segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 4165-4173.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;CRNet&lt;/strong&gt;(Cross-Reference Networks)&lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CRNet_Cross-Reference_Networks_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; make predictions for both the support image and the query image. It can better find the &lt;code&gt;co-occurrent objects&lt;/code&gt; in the two images, thus helping the few-shot segmentation task. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Few-Shot Segmentation" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Segmentation/"/>
    
      <category term="CRNet" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Segmentation/CRNet/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="FSS" scheme="https://gojay.top/tags/FSS/"/>
    
      <category term="CRNet" scheme="https://gojay.top/tags/CRNet/"/>
    
  </entry>
  
  <entry>
    <title>FSL-Survey-2019</title>
    <link href="https://gojay.top/2020/07/07/FSL-Survey-2019/"/>
    <id>https://gojay.top/2020/07/07/FSL-Survey-2019/</id>
    <published>2020-07-07T07:23:44.000Z</published>
    <updated>2020-07-09T07:42:04.223Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>FSL-Survey</strong><a href="https://arxiv.org/abs/1904.05046" target="_blank" rel="noopener">[1]</a> is a <strong>survey on Few-Shot Learning</strong>(FSL), which cotains 166 paper to review Few-Shot Learning. They categorize FSL methods from three perspectives: <code>data</code>, <code>model</code> and <code>algorithm</code>. There are some details of reading it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&note">Paper &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-note"><a href="#Paper-amp-note" class="headerlink" title="Paper &amp; note"></a>Paper &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1904.05046" target="_blank" rel="noopener">Generalizing from a Few Examples: A Survey on Few-Shot Learning</a>(CSUR 2019 paper)<br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=327bb16e-b716-d631-7d51-1a80bda5efb4&documentId=7dee4617-ab8b-3b47-9f5c-4245a1693997" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/07/09/EAjulYM8wtJLoBH.png" alt="FSL_Abstract.png"></p><blockquote><ol><li>Starting from a <code>formal definition</code> of FSL, then point out that the <code>core issue</code> in FSL.  </li><li><strong>Data</strong>: which uses prior knowledge to <code>augment the supervised experience</code>.  </li><li><strong>model</strong>: which uses prior knowledge to <code>reduce the size of the hypothesis space</code>.  </li><li><strong>algorithm</strong>: which uses prior knowledge to <code>alter the search for the best hypothesis</code> in the given hypothesis space.  </li><li><code>Promising directions</code> are also proposed to provide insights for future research.</li></ol></blockquote><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p><img src="https://i.loli.net/2020/07/09/QdI3gBFulmcSP1y.png" alt="FSL_MLdefinition.png"><br><img src="https://i.loli.net/2020/07/09/1iGFHbPKAJj3hUD.png" alt="FSL_FSLdefinition.png"><br><img src="https://i.loli.net/2020/07/09/bZu4cqMBxmQtLzE.png" alt="FSL_Minimization.png"><br><img src="https://i.loli.net/2020/07/09/XYrSviFTUd8bkIL.png" alt="FSL_Problem.png"></p><h2 id="Taxonomy"><a href="#Taxonomy" class="headerlink" title="Taxonomy"></a>Taxonomy</h2><p><img src="https://i.loli.net/2020/07/09/WSLoZFvi98NRMaY.png" alt="FSL_Taxonomy.png"><br><img src="https://i.loli.net/2020/07/09/HAF7cUVnYhPB1lE.png" alt="FSL_methods.png"></p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p><img src="https://i.loli.net/2020/07/09/tRFjy7m8vYWh4wa.png" alt="FSL_data.png"><br><img src="https://i.loli.net/2020/07/09/PQmErDOdFsVNiK9.png" alt="FSL_DataAugmentation.png"></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><img src="https://i.loli.net/2020/07/09/qIXFzS6BKVbZg9w.png" alt="FSL_model.png"></p><h3 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h3><p><img src="https://i.loli.net/2020/07/09/hlbiYKeX2AQTRtG.png" alt="FSL_ParameterTyping.png"><br><img src="https://i.loli.net/2020/07/09/CmYvdOSZJijrquI.png" alt="FSL_ParameterSharing.png"></p><h3 id="Embedding-Learning"><a href="#Embedding-Learning" class="headerlink" title="Embedding Learning"></a>Embedding Learning</h3><p><img src="https://i.loli.net/2020/07/09/z29XpluoK1YHAJF.png" alt="FSL_EmbeddingLearning.png"><br><img src="https://i.loli.net/2020/07/09/BNhpZciAQOnSDyK.png" alt="FSL_task-invariant.png"><br><img src="https://i.loli.net/2020/07/09/VwPC8pSNW5Oi4G3.png" alt="FSL_hybrid.png"></p><h3 id="Learning-with-External-Memory"><a href="#Learning-with-External-Memory" class="headerlink" title="Learning with External Memory"></a>Learning with External Memory</h3><p><img src="https://i.loli.net/2020/07/09/Niel1OZzVvyDaEf.png" alt="FSL_memory.png"><br><img src="https://i.loli.net/2020/07/09/X74PBbhluHUzidk.png" alt="FSL_MemoryMethods.png"></p><h3 id="Generative-Modeling"><a href="#Generative-Modeling" class="headerlink" title="Generative Modeling"></a>Generative Modeling</h3><p><img src="https://i.loli.net/2020/07/09/nPKWEXNlZAh7r4L.png" alt="FSL_generative.png"></p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p><img src="https://i.loli.net/2020/07/09/ef2QC6LGgTyVnAw.png" alt="FSL_algorithm.png"></p><h3 id="Refining-Existing-Parameters"><a href="#Refining-Existing-Parameters" class="headerlink" title="Refining Existing Parameters"></a>Refining Existing Parameters</h3><p><img src="https://i.loli.net/2020/07/09/SOl4cJosVaBrbuP.png" alt="FSL_fine-tune.png"><br><img src="https://i.loli.net/2020/07/09/iE5CvtDh7eybcKA.png" alt="FSL_aggregate.png"><br><img src="https://i.loli.net/2020/07/09/qL1nZwDyuCHiX9t.png" alt="FSL_NewParameters.png"></p><h3 id="Refining-Meta-Learned-Parameter"><a href="#Refining-Meta-Learned-Parameter" class="headerlink" title="Refining Meta-Learned Parameter"></a>Refining Meta-Learned Parameter</h3><p><img src="https://i.loli.net/2020/07/09/urRwAjUkHgOXiDI.png" alt="FSL_Meta-Learned.png"></p><h3 id="Learning-the-Optimizer"><a href="#Learning-the-Optimizer" class="headerlink" title="Learning the Optimizer"></a>Learning the Optimizer</h3><p><img src="https://i.loli.net/2020/07/09/QRkw2VajdZxFtmG.png" alt="FSL_optimizer.png"></p><h2 id="Meta-learning"><a href="#Meta-learning" class="headerlink" title="Meta-learning"></a>Meta-learning</h2><p><img src="https://i.loli.net/2020/07/09/C3IEkxsnytqduOP.png" alt="FSL_meta-learning.png"></p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>Offical Online link can be found in <strong>FewShotPapers</strong><a href="https://github.com/tata1661/FewShotPapers" target="_blank" rel="noopener">[2]</a>.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Wang Y, Yao Q, Kwok J T, et al. Generalizing from a few examples: A survey on few-shot learning[J]. ACM Computing Surveys (CSUR), 2019.<br>[2] FewShotPapers. <a href="https://github.com/tata1661/FewShotPapers" target="_blank" rel="noopener">https://github.com/tata1661/FewShotPapers</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;FSL-Survey&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.05046&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a &lt;strong&gt;survey on Few-Shot Learning&lt;/strong&gt;(FSL), which cotains 166 paper to review Few-Shot Learning. They categorize FSL methods from three perspectives: &lt;code&gt;data&lt;/code&gt;, &lt;code&gt;model&lt;/code&gt; and &lt;code&gt;algorithm&lt;/code&gt;. There are some details of reading it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Few-Shot Learning" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Learning/"/>
    
      <category term="FSL Survey" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Learning/FSL-Survey/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="FSL" scheme="https://gojay.top/tags/FSL/"/>
    
      <category term="Survey" scheme="https://gojay.top/tags/Survey/"/>
    
  </entry>
  
  <entry>
    <title>PV-RCNN</title>
    <link href="https://gojay.top/2020/06/23/PV-RCNN/"/>
    <id>https://gojay.top/2020/06/23/PV-RCNN/</id>
    <published>2020-06-23T10:43:53.000Z</published>
    <updated>2020-07-03T07:34:48.479Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>PV-RCNN</strong><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">[1]</a> is a <strong>3D Object Detection</strong> framework to integrate <code>3D voxel CNN</code> and <code>PointNet-based set abstraction</code> to learn more discriminative point cloud features. The most contributions in this papar is  <strong>two-stage strategy</strong> including the <code>voxel-to-keypoint</code> 3D scene encoding and the <code>keypoint-to-grid</code> RoI feature abstraction. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</a>(CVPR 2020 paper)<br><strong>Code</strong>: <a href="https://github.com/sshaoshuai/PV-RCNN" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=27532eab-6324-9637-0ff2-686fb058a2c4&documentId=3e0406f7-8d48-3933-a9dd-55aa2f55a808" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/06/23/BEe5DYuwqiZc61C.png" alt="PV-RCNN_Abstract.png"></p><blockquote><ol><li>They present PointVoxel-RCNN(<strong>PV-RCNN</strong>) for accurate 3D object detection from point clouds.  </li><li>It summarizes the 3D scene with a <strong>3D voxel CNN</strong> into a small set of <code>keypoints</code> via a novel <strong>voxel set abstraction</strong>(VSA) module.  </li><li><strong>RoI-grid pooling</strong> is proposed to abstract proposal-specific features from the keypoints to the RoI-<code>grid points</code>, the RoI-grid feature points encode much richer context information.  </li><li>It surpasses <strong>state-of-the-art</strong> 3D detection.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/06/23/heG8Q6FBamEtwiU.png" alt="PV-RCNN_PD.png"></p><blockquote><ol><li>The <code>grid-based methods</code> generally transform the irregular point clouds to regular representations such as 3D voxels, they are <strong>more computationally efficient</strong>.  </li><li>The <code>point-based methods</code> directly extract discriminative features from raw point clouds for 3D detection, they could achieve <strong>larger receptive field</strong>.  </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/06/23/8FvWG5xBNH9hpYe.png" alt="PV-RCNN_PS.png"></p><blockquote><ol><li>They <strong>integrated</strong> these two types. The <code>voxel-based operation</code> efficiently encodes <strong>multi-scale feature</strong> representations, <code>PointNet-based set abstraction operation</code> preserves <strong>accurate location information</strong> with flexible receptive field.  </li><li>The voxel CNN with <code>3D sparse convolution</code> is adopted for <strong>voxel-wise feature</strong> learning and <strong>accurate proposal</strong> generation.  </li><li>A small set of <code>keypoints</code> are selected by the furtherest point sampling (<strong>FPS</strong>) to <strong>summarize the overall 3D information</strong> from the voxel-wise features.  </li><li><code>PointNet-based set abstraction</code> for summarizing <strong>multi-scale</strong> point cloud information.  </li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/06/23/l2r9zKOSiCLd4ue.png" alt="PV-RCNN_framework.png"><br><img src="https://i.loli.net/2020/06/23/hwrMFAQfmxVJyba.png" alt="PV-RCNN_overall.png"></p><blockquote><ol><li><strong>3D Sparse Convolution</strong>: Input the <code>raw point clouds</code> to learn <code>multi-scale semantic features</code> and generate <code>3D object proposals</code>.  </li><li><strong>Voxel Set Abstraction</strong>: the learned <code>voxel-wise feature</code> volumes at multiple neural layers are summarized into a small set of <code>key points</code>.  </li><li><strong>RoI-grid Pooling</strong>: the <code>keypoint</code> features are aggregated to the RoI-<code>grid points</code>.  </li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><h3 id="Predicted-Keypoint-Weighting"><a href="#Predicted-Keypoint-Weighting" class="headerlink" title="Predicted Keypoint Weighting"></a>Predicted Keypoint Weighting</h3><p><img src="https://i.loli.net/2020/06/23/fHRlJBraFbs4eN7.png" alt="PV-RCNN_PKW.png"></p><h3 id="RoI-grid-Pooling"><a href="#RoI-grid-Pooling" class="headerlink" title="RoI-grid Pooling"></a>RoI-grid Pooling</h3><p><img src="https://i.loli.net/2020/06/23/5EaycV76sG9TxwR.png" alt="PV-RCNN_RoI-grid.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/06/23/kJ7PbGZzawoWLAR.png" alt="PV-RCNN_KITTI.png"><br><img src="https://i.loli.net/2020/06/23/9UJAYQT6ezLKjFR.png" alt="PV-RCNN_val.png"><br><img src="https://i.loli.net/2020/06/23/fmj27FpLaIohAUP.png" alt="PV-RCNN_WaymoOpen.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><ol><li>The <strong>complete code</strong> can be found in PV-RCNN<a href="https://github.com/sshaoshuai/PV-RCNN" target="_blank" rel="noopener">[2]</a>.  </li><li>Another implementation can be found in vision3d<a href="https://github.com/jhultman/vision3d" target="_blank" rel="noopener">[3]</a>.  </li></ol></blockquote><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><ol><li>Provide more <code>accurate detections</code> by point cloud features.</li><li>Integrate it to <code>multiple object tracking</code> framework.</li></ol></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Shi S, Guo C, Jiang L, et al. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10529-10538.<br>[2] PV-RCNN. <a href="https://github.com/sshaoshuai/PV-RCNN" target="_blank" rel="noopener">https://github.com/sshaoshuai/PV-RCNN</a><br>[3] vision3d. <a href="https://github.com/jhultman/vision3d" target="_blank" rel="noopener">https://github.com/jhultman/vision3d</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;PV-RCNN&lt;/strong&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a &lt;strong&gt;3D Object Detection&lt;/strong&gt; framework to integrate &lt;code&gt;3D voxel CNN&lt;/code&gt; and &lt;code&gt;PointNet-based set abstraction&lt;/code&gt; to learn more discriminative point cloud features. The most contributions in this papar is  &lt;strong&gt;two-stage strategy&lt;/strong&gt; including the &lt;code&gt;voxel-to-keypoint&lt;/code&gt; 3D scene encoding and the &lt;code&gt;keypoint-to-grid&lt;/code&gt; RoI feature abstraction. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="3D Object Dedection" scheme="https://gojay.top/categories/DeepLearning/3D-Object-Dedection/"/>
    
      <category term="PV-RCNN" scheme="https://gojay.top/categories/DeepLearning/3D-Object-Dedection/PV-RCNN/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Detection-3D" scheme="https://gojay.top/tags/Detection-3D/"/>
    
      <category term="PV-RCNN" scheme="https://gojay.top/tags/PV-RCNN/"/>
    
  </entry>
  
  <entry>
    <title>DeepSORT</title>
    <link href="https://gojay.top/2020/06/20/DeepSORT/"/>
    <id>https://gojay.top/2020/06/20/DeepSORT/</id>
    <published>2020-06-20T02:35:15.000Z</published>
    <updated>2020-06-21T02:56:28.022Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>DeepSORT</strong><a href="https://arxiv.org/abs/1703.07402" target="_blank" rel="noopener">[1]</a> integrates <code>appearance information</code> to improve the performance of <code>SORT</code>, learned a deep association metric. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1703.07402" target="_blank" rel="noopener">Simple Online and Realtime Tracking with a Deep Association Metric</a>(ICIP 2017 paper)<br><strong>Code</strong>: <a href="https://github.com/nwojke/deep_sort" target="_blank" rel="noopener">PyTorch</a>, <a href="https://github.com/Qidian213/deep_sort_yolov3" target="_blank" rel="noopener">TensorFlow</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=4bdd5275-2587-dcc1-6dcd-ea8a59676550&documentId=de716d88-7206-37da-8903-6f3616998bac" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/06/20/aDNpkhEtdTiszx5.png" alt="DeepSORT_Abstract.png"></p><blockquote><ol><li><strong>SORT</strong><a href="https://gojay.top/2020/06/14/SORT/">[2]</a> is a pragmatic approach to multiple object tracking.  </li><li>In this paper, <strong>appearance information</strong> was integrated to improve the performance of SORT for tackling the <code>long-term occlusions</code>.  </li><li>They place <strong>offline pre-traning</strong> with a learned <code>deep association metirc</code> on person re-id dataset, while establish <code>measurement-to-track associations</code> using nearest neighbor queries during <strong>online application</strong>.  </li><li>It reduces the number of <strong>identity switches</strong> by 45%.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><blockquote><ol><li>Traditional methods are not applicable in <strong>online scenarios</strong> and the performance of these methods comes at <code>increased computational</code> and <code>implementation complexity</code>.  </li><li><strong>SORT</strong> returns a relatively high number of <code>identity switches</code>.  </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/06/20/wukElyi38fd6jaV.png" alt="DeepSORT_PS.png"></p><blockquote><p>They overcome this issue by replacing the association metric with a more informed metric that <strong>combines motion and appearance information</strong>.</p></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><h3 id="State-Estimation"><a href="#State-Estimation" class="headerlink" title="State Estimation"></a>State Estimation</h3><p><img src="https://i.loli.net/2020/06/20/KlhfnoV56zywWjB.png" alt="DeepSORT_Estimation.png"></p><h3 id="Assignment-Problem"><a href="#Assignment-Problem" class="headerlink" title="Assignment Problem"></a>Assignment Problem</h3><blockquote><ol><li><strong>Motion</strong> information: <code>Mahalanobis</code> distance $d^{(1)}(i,j)=(d_{j}-y_{i})^{T}S_{i}^{-1}(d_{j}-y_{i})$.  </li><li><strong>Appearance</strong> information: smallest <code>cosine</code> distance $d^{(2)}(i,j)=min \lbrace 1-r_{j}^{T}r_{k}^{(i)} |r_{k}^{(i)}\in{R_{i}} \rbrace$.  </li><li>In <strong>combination</strong> for occlusions: weighted sum $c_{i,j}=\lambda d^{(1)}(i,j)+(1-\lambda)d^{(2)}(i,j)$.  </li></ol></blockquote><h3 id="Matching-Cascade"><a href="#Matching-Cascade" class="headerlink" title="Matching Cascade"></a>Matching Cascade</h3><p><img src="https://i.loli.net/2020/06/20/VrXym6S3MjaEYfB.png" alt="DeepSORT_Matching.png"></p><h3 id="Deep-Appearance-Descriptor"><a href="#Deep-Appearance-Descriptor" class="headerlink" title="Deep Appearance Descriptor"></a>Deep Appearance Descriptor</h3><p><img src="https://i.loli.net/2020/06/20/5TcFVHg2aozqsSZ.png" alt="DeepSORT_Architecture.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/06/20/LCgbwueJB1aOGnM.png" alt="DeepSORT_output.png"><br><img src="https://i.loli.net/2020/06/20/yvYxZ9j6JNiz3so.png" alt="DeepSORT_Results.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><ol><li>The <strong>complete code</strong> can be found in deep_sort<a href="https://github.com/nwojke/deep_sort" target="_blank" rel="noopener">[3]</a>.  </li><li>Another <code>tensorflow</code> implementation can be found in deep_sort_yolov3<a href="https://github.com/Qidian213/deep_sort_yolov3" target="_blank" rel="noopener">[4]</a>.</li></ol></blockquote><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details about the <code>whole algorithm</code> and its <code>implementation</code> can be found in <a href="https://zhuanlan.zhihu.com/p/133678626" target="_blank" rel="noopener">[5]</a>.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Wojke N, Bewley A, Paulus D. Simple online and realtime tracking with a deep association metric[C]//2017 IEEE international conference on image processing (ICIP). IEEE, 2017: 3645-3649.<br>[2] Gojay. “SORT.” <a href="https://gojay.top/2020/06/14/SORT/">https://gojay.top/2020/06/14/SORT/</a><br>[3] deep_sort. <a href="https://github.com/nwojke/deep_sort" target="_blank" rel="noopener">https://github.com/nwojke/deep_sort</a><br>[4] deep_sort_yolov3. <a href="https://github.com/Qidian213/deep_sort_yolov3" target="_blank" rel="noopener">https://github.com/Qidian213/deep_sort_yolov3</a><br>[5] pprp. “Anasis for Deep SORT.” <a href="https://zhuanlan.zhihu.com/p/133678626" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/133678626</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;DeepSORT&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.07402&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; integrates &lt;code&gt;appearance information&lt;/code&gt; to improve the performance of &lt;code&gt;SORT&lt;/code&gt;, learned a deep association metric. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="DeepSORT" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/DeepSORT/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>Toolkit for DL</title>
    <link href="https://gojay.top/2020/06/18/Toolkit-for-DL/"/>
    <id>https://gojay.top/2020/06/18/Toolkit-for-DL/</id>
    <published>2020-06-18T06:45:10.000Z</published>
    <updated>2020-06-18T07:01:40.584Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>There are the overall of toolkit for <strong>Deep Learning</strong>.<br><a href="https://github.com/Gojay001/toolkit-DeepLearning" target="_blank" rel="noopener">https://github.com/Gojay001/toolkit-DeepLearning</a></p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">    |- DataProcess/</span><br><span class="line">        |- data_aug.py</span><br><span class="line">        |- data_config.py</span><br><span class="line">        |- data_loader.py</span><br><span class="line">        |- divide_data.py</span><br><span class="line">        |- img2video.py</span><br><span class="line">        |- img_resize.py</span><br><span class="line">        |- video_extract.py</span><br><span class="line">    |- utils/</span><br><span class="line">        |- bbox_iou.py</span><br><span class="line">        |- nms</span><br><span class="line">            |- nms_cpu.py</span><br><span class="line">            |- ...</span><br><span class="line">    |- test.py</span><br><span class="line">    |- train.py</span><br></pre></td></tr></table></figure><h2 id="DataProcess"><a href="#DataProcess" class="headerlink" title="DataProcess"></a>DataProcess</h2><blockquote><p>DataProcess/<code>data_aug.py</code>: augment data with transforms to aug file.<br>DataProcess/<code>data_config.py</code>: config (hyper-)parameters.<br>DataProcess/<code>data_loader.py</code>: load data to tensor of DataSet type.<br>DataProcess/<code>divide_data.py</code>: divide data to train and valid files.<br>DataProcess/<code>img2video.py</code>: transform images set to video using opencv.<br>DataProcess/<code>img_resize.py</code>: resize images to specific size using opencv.<br>DataProcess/<code>video_extract.py</code>: extract each frame of video to images file.</p></blockquote><h2 id="utils"><a href="#utils" class="headerlink" title="utils"></a>utils</h2><blockquote><p>utils/<code>bbox_iou.py</code>: calculate iou between two bounding-box.<br>utils/nms/<code>nms_cpu.py</code>: remove useless bounding-box by nms(Non-maximum suppression).</p></blockquote><p>(Updating…)</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;There are the overall of toolkit for &lt;strong&gt;Deep Learning&lt;/strong&gt;.&lt;br&gt;&lt;a href=&quot;https://github.com/Gojay001/toolkit-DeepLearning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Gojay001/toolkit-DeepLearning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Toolkit" scheme="https://gojay.top/categories/Toolkit/"/>
    
      <category term="Overview" scheme="https://gojay.top/categories/Toolkit/Overview/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Toolkit" scheme="https://gojay.top/tags/Toolkit/"/>
    
      <category term="Overview" scheme="https://gojay.top/tags/Overview/"/>
    
  </entry>
  
  <entry>
    <title>SORT</title>
    <link href="https://gojay.top/2020/06/14/SORT/"/>
    <id>https://gojay.top/2020/06/14/SORT/</id>
    <published>2020-06-14T04:47:54.000Z</published>
    <updated>2020-06-21T02:52:58.254Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>SORT</strong><a href="https://arxiv.org/abs/1602.00763" target="_blank" rel="noopener">[1]</a> is pragmatic approach for <strong>online and realtime</strong> applications. It achieves SOTA with using <code>Kalman filter</code> and <code>Hungarian algorithm</code>. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1602.00763" target="_blank" rel="noopener">Simple Online and Realtime Tracking</a>(ICIP 2016 paper)<br><strong>Code</strong>: <a href="https://github.com/abewley/sort" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=13481f42-a600-524f-08b2-c16d48b76b2c&documentId=a3d58906-82b5-3454-b0db-173e8bedc4e0" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/06/15/u7SPa1cqrkLZOiy.png" alt="SORT_Abstract.png"></p><blockquote><ol><li>THis paper explores a pragmatic approach to multiple object tracking where the main focus is to <code>associate objects</code>.  </li><li>To this end, <code>detection quality</code> is identified as a key factor influencing tracking performance.  </li><li>It only use the <code>Kalman filter</code> and <code>Hungarian algorithm</code> for the tracking components.  </li><li>It achieves an accuracy comparable to <code>state-of-the-art</code> online trackers.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><blockquote><ol><li>Traditionally methods delay making difficult decisions while there is <code>high uncertainty</code> over the object assignments.  </li><li>Recent developments still delay the <code>decision making</code> which makes them unsuitable for online tracking.  </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/06/15/BiferdJYLpGm96F.png" alt="SORT_Simple.png"><br><img src="https://i.loli.net/2020/06/15/av8bwNkBlYFPgzf.png" alt="SORT_PS.png"></p><blockquote><ol><li><strong>Occam’s Razor</strong>: Only the <code>bounding box</code> position and size are used for both motion estimation and data association.  </li><li><strong>Detection</strong>: CNN based, like Faster R-CNN<a href="https://gojay.top/2019/10/19/Faster-R-CNN/">[2]</a>.  </li><li><strong>Motion estimation</strong>: Kalman filter<a href="https://zhuanlan.zhihu.com/p/39912633" target="_blank" rel="noopener">[3]</a>.  </li><li><strong>Data association</strong>: Hungarian algorithm<a href="https://zhuanlan.zhihu.com/p/62981901" target="_blank" rel="noopener">[4]</a>.</li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><h3 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h3><p><img src="https://i.loli.net/2020/06/15/TDyNnBFciJKQm1z.png" alt="SORT_Comparison.png"></p><blockquote><p>To this end, <strong>detection quality</strong> is identified as a <code>key factor</code> influencing tracking performance, where changing the detector can improve tracking by up to 18.9%. </p></blockquote><h3 id="Estimation-Model"><a href="#Estimation-Model" class="headerlink" title="Estimation Model"></a>Estimation Model</h3><p><img src="https://i.loli.net/2020/06/15/LVa2uc4lARBEjS6.png" alt="SORT_Estimation.png"></p><blockquote><ol><li>It used to <code>propagate</code> a target’s identity into the next frame.  </li><li>It uses <code>Kalman filter</code> with a linear constant velocity model.  </li><li>The state of each target is modelled as: $x=[u,v,s,r,\dot{u},\dot{v},\dot{s}]^T$.  </li></ol></blockquote><h3 id="Data-Association"><a href="#Data-Association" class="headerlink" title="Data Association"></a>Data Association</h3><p><img src="https://i.loli.net/2020/06/15/iFpybWmC3dcqseY.png" alt="SORT_Association.png"></p><blockquote><ol><li>It used to <code>assign detections</code> to existing targets.  </li><li>The assignment cost matrix is then computed as the intersection-over-union <code>(IOU) distance</code>.  </li><li>The assignment is solved optimally using the <code>Hungarian algorithm</code>.</li></ol></blockquote><h3 id="Creation-and-Deletion-of-Track-Identities"><a href="#Creation-and-Deletion-of-Track-Identities" class="headerlink" title="Creation and Deletion of Track Identities"></a>Creation and Deletion of Track Identities</h3><p><img src="https://i.loli.net/2020/06/15/1oW98fgSUrXFsKk.png" alt="SORT_Management.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/06/15/ifJtZF2CWT6LHme.png" alt="SORT_Performance.png"><br><img src="https://i.loli.net/2020/06/15/vcXpkKJVNtWoaP4.png" alt="SORT_MOT.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in SORT<a href="https://github.com/abewley/sort" target="_blank" rel="noopener">[5]</a>.  </p></blockquote><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><ol><li>Allowing for new methods to focus on object <code>re-identification</code> to handle long term occlusion.  </li><li>Future work will investigate a tightly coupled <code>detection</code> and tracking framework.</li></ol></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Bewley, Alex, et al. “Simple online and realtime tracking.” 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016.<br>[2] Gojay. “Faster R-CNN.” <a href="https://gojay.top/2019/10/19/Faster-R-CNN/">https://gojay.top/2019/10/19/Faster-R-CNN/</a><br>[3] Bzarg, Bot. “How a Kalman filter works in pictures.” <a href="https://zhuanlan.zhihu.com/p/39912633" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/39912633</a><br>[4] ZihaoZhao. “Hungarian algorithm and Kuhn-Munkres algorithm.” <a href="https://zhuanlan.zhihu.com/p/62981901" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62981901</a><br>[5] SORT. <a href="https://github.com/abewley/sort" target="_blank" rel="noopener">https://github.com/abewley/sort</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;SORT&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.00763&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is pragmatic approach for &lt;strong&gt;online and realtime&lt;/strong&gt; applications. It achieves SOTA with using &lt;code&gt;Kalman filter&lt;/code&gt; and &lt;code&gt;Hungarian algorithm&lt;/code&gt;. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="SORT" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/SORT/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>FairMOT</title>
    <link href="https://gojay.top/2020/05/25/FairMOT/"/>
    <id>https://gojay.top/2020/05/25/FairMOT/</id>
    <published>2020-05-25T11:31:14.000Z</published>
    <updated>2020-06-17T12:26:30.893Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>FairMOT</strong><a href="https://arxiv.org/abs/2004.01888" target="_blank" rel="noopener">[1]</a> is a <strong>one-shot tracker</strong> to fuse object detection and re-identification in a single network. The most contributions in this papar are <code>anchor-free</code> Re-ID feture extraction, multi-layer <code>feature aggregation</code> and <code>lower-dimensional</code> re-ID fetures. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2004.01888" target="_blank" rel="noopener">A Simple Baseline for Multi-Object Tracking</a>(arXiv 2020 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/FairMOT" target="_blank" rel="noopener">Pytorch</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=3596929f-22de-4f63-7b5a-574ca3f75d1c&documentId=2f8fe432-d0b0-3dd8-aca8-2128175c156a" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/05/28/LIsCEvDGgup65e2.png" alt="FairMOT_Abstract.png"></p><blockquote><ol><li>There has been remarkable progress on <strong>multi-object tracking</strong> with object detection and re-identification.  </li><li>Little attention has been focused on accomplishing the two tasks in a <strong>single network</strong>.  </li><li>In this work, they study the essential <strong>reasons</strong> behind the failure, and accordingly present a simple baseline to addresses the problem.  </li><li>It outperforms the <strong>state-of-the-art</strong> on the public datasets.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><blockquote><ol><li><strong>Two steps</strong>: First the <code>detection</code> model localizes the bounding boxes of objects, then the <code>association</code> model extracts Re-ID features and links it to tracks. However, those methods cannot perform inference at video rate because the two networks <strong>do not share features</strong>.  </li><li><strong>One-shot</strong>: Those methods <code>jointly</code> detect objects and learn Re-ID features. However, the <strong>accuracy</strong> and <strong>ID switches</strong> get worse a lot.  </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/05/28/8bFG29dRVMkZI1t.png" alt="FairMOT_anchor-free.png"></p><blockquote><ol><li><strong>Anchor-Free</strong>: the anchor-based methods usually operate on a <code>coarse grid</code>. So there is a high chance that the features extracted at the anchor are <code>not aligned with the object center</code>.  </li><li><strong>Multi-Layer Feature Aggregation</strong>: it helps <code>reduce identity switches</code> by aggregating low-level and high-level features.  </li><li><strong>Lower-dimensional features</strong>: It helps reduce the risk of <code>over-fitting</code> to small data, and improves the tracking <code>robustness</code>.</li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/05/28/PzwV5syQnaN36LO.png" alt="FairMOT_Overview.png"></p><blockquote><ol><li><strong>Multi-Layer Feature Aggregation</strong>: It follows Deep Layer Aggregation (<code>DLA</code>) to fuse features from multiple layers in order to deal with objects of different scales.  </li><li><strong>Anchor-free object detection</strong>: It estimates the <code>object centers</code> on high-resolution feature map.  </li><li><strong>pixel-wise Re-identification</strong>: It learn <code>low-dimensional</code> Re-ID features to reduce the computation time and improve the robustness.  </li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><h3 id="Object-Dection-Branch"><a href="#Object-Dection-Branch" class="headerlink" title="Object Dection Branch"></a>Object Dection Branch</h3><blockquote><ol><li><strong>Heatmap Head</strong>: This head is responsible for estimating the locations of the <code>object centers</code>.  </li><li><strong>Center Offset Head</strong>: This head is responsible for localizing the objects <code>more precisely</code>.  </li><li><strong>Box Size Head</strong>: This head is responsible for estimating the height and width of the target <code>bounding box</code> at each anchor location.  </li></ol></blockquote><h3 id="Identify-Embedding-Branch"><a href="#Identify-Embedding-Branch" class="headerlink" title="Identify Embedding Branch"></a>Identify Embedding Branch</h3><blockquote><ol><li>The goal of the identity embedding branch is to generate features that can <code>distinguish different objects</code>.  </li><li>The resulting featuresis $E\in{R^{128\times{W}\times{H}}}$, the distance between different objects should be larger.</li></ol></blockquote><h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><blockquote><ol><li><strong>Heatmap Loss</strong>: The loss function is defined as pixel-wise logistic regression with <code>focal loss</code>.  </li><li><strong>Offset and Size Loss</strong>: They we enforce <code>l1 losses</code> for the two heads.  </li><li><strong>Identity Enbedding Loss</strong>: They treat object identity embedding as a classification task, then compute the <code>softmax loss</code>.</li></ol></blockquote><h3 id="Online-Tracking"><a href="#Online-Tracking" class="headerlink" title="Online Tracking"></a>Online Tracking</h3><p><img src="https://i.loli.net/2020/05/28/QlBpdnh8v14YTaM.png" alt="FairMOT_Tracking.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/05/28/bsJxNwjY1cKHRuh.png" alt="FairMOT_one-shot.png"><br><img src="https://i.loli.net/2020/05/28/VWYjHe5bt9kLI4y.png" alt="FairMOT_private.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in <a href="https://github.com/Gojay001/FairMOT" target="_blank" rel="noopener">here</a> with citing FairMOT<a href="https://github.com/ifzhang/FairMOT" target="_blank" rel="noopener">[2]</a>.<br>[Updating]</p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><ol><li>This method achieves the SOTA under the <code>private detector</code> on MOT Challenge, but it still exists in experiments.  </li><li>It mostly improved detectional performance, when using it in actual enviroments, the <code>IDS</code> increase a lot than previous methods.  </li><li>Considering how to improve the IDS is important in real world, maybe we can improve the association module based on <code>depth information</code>.  </li></ol></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Zhan Y, Wang C, Wang X, et al. A Simple Baseline for Multi-Object Tracking[J]. arXiv preprint arXiv:2004.01888, 2020.<br>[2] FairMOT. <a href="https://github.com/ifzhang/FairMOT" target="_blank" rel="noopener">https://github.com/ifzhang/FairMOT</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;FairMOT&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.01888&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a &lt;strong&gt;one-shot tracker&lt;/strong&gt; to fuse object detection and re-identification in a single network. The most contributions in this papar are &lt;code&gt;anchor-free&lt;/code&gt; Re-ID feture extraction, multi-layer &lt;code&gt;feature aggregation&lt;/code&gt; and &lt;code&gt;lower-dimensional&lt;/code&gt; re-ID fetures. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="FairMOT" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/FairMOT/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>TSDM</title>
    <link href="https://gojay.top/2020/05/23/TSDM/"/>
    <id>https://gojay.top/2020/05/23/TSDM/</id>
    <published>2020-05-23T03:09:01.000Z</published>
    <updated>2020-06-17T12:26:30.904Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>TSDM</strong><a href="https://arxiv.org/abs/2005.04063" target="_blank" rel="noopener">[1]</a> is a <strong>RGB-D tracker</strong> which use depth information to pretreatment and fuse information to pro-processing. It is composed of a <code>Mask-generator</code>(M-g), <code>SiamRPN++</code> and a <code>Depth-refiner</code>(D-r). There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2005.04063" target="_blank" rel="noopener">TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator</a>(arXiv 2020 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/TSDM" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=7721e686-ad20-cd43-2c85-efb222828ee4&documentId=3ceab671-243c-3258-8acc-3c4dc894fdc6" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/05/23/CTjkxo4luMcvIRb.png" alt="TSDM_Abstract.png"></p><blockquote><ol><li><strong>Depth information</strong> provides informative cues for foreground-background separation and target bounding box regression.  </li><li>Few trackers have used depth information to play the important role aforementioned due to the <strong>lack of a suitable model</strong>.  </li><li>In this paper, a <strong>RGB-D tracker</strong> named TSDM is proposed, The <code>M-g</code> generates the background masks, and updates them as the target 3D position changes. The <code>D-r</code> optimizes the target bounding box estimated by <code>SiamRPN++</code>, based on the spatial depth distribution difference between the target and the surrounding background.  </li><li>It outperforms the <strong>state-of-the-art</strong> on the PTB and VOT.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/05/23/l7k2CnYdKFuXHDi.png" alt="TSDM_PD.png"></p><blockquote><ol><li>The main obstacle is that the tracker requires <strong>constant information</strong> (such as color), but the target <code>depth distribution may change</code> a lot when the target moves.  </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/05/23/sJCzbWlnBiQ8XdT.png" alt="TSDM_PS1.png"><br><img src="https://i.loli.net/2020/05/23/kYjv1QwrRKTobnO.png" alt="TSDM_PS2.png"></p><blockquote><ol><li><strong>Depth mudules</strong>: <code>M-g</code> and <code>D-r</code> can overcome the obstacle above and make use of depth information effectively.</li><li><strong>Data augmentation</strong>: it helps <code>retrain SiamRPN++</code> to work better with the M-g module.  </li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/05/23/FZU1hn3WsTva87o.png" alt="TSDM_Pipeline.png"></p><blockquote><ol><li><strong>Mask-generator</strong>: Input $X_d$ and $\overline{Dt_{i-1}}$ into M-g to get $M$ and $M_c$, then use $F_m(\cdot)$ to get $X_m$.  </li><li><strong>SiamRPN++</strong>: Input $Z$ and $X_m$ into the core, then outputs the target bounding box $B_s$ ($W,H,C_x,C_y$).  </li><li><strong>Depth-refiner</strong>: Cut out $R_c$ and $R_d$ from $X_c$ and $X_d$ by $B_s$ respectively. Then input $R_c$ and $R_d$ into D-r to get the refined target bounding box $B_d$ ($w,h,xr,yb$).  </li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><h3 id="Mask-generator"><a href="#Mask-generator" class="headerlink" title="Mask-generator"></a>Mask-generator</h3><blockquote><ol><li>M-g generates two <strong>background mask</strong> images, $M$ is a 2-value image for clearing out the background of $X_c$, and $M_c$ is a color image for coloring the background of $X_c$.  </li><li><strong>$M_c$ color selection</strong>: $M_c$ enhances the target background difference to make the target template matching easier.</li><li><strong>M-g stop-restart strategy</strong>: M-g should automatically stop to avoid masking the real target when a transient tracking drift happens.  </li><li><strong>M-g simulated data augmentation</strong>: it used to generate enough training samples ($X_m$) to retrain the SiamRPN++.  </li></ol></blockquote><h3 id="SiamRPN"><a href="#SiamRPN" class="headerlink" title="SiamRPN++"></a>SiamRPN++</h3><blockquote><ul><li>It takes an image pair ($Z,X$) as input and outputs the target bounding box in the current frame, as: $f(Z,X)=\phi(Z)\ast\phi(X)$.  </li><li>More details of SiamRPN++ can be found in previous blog [SiamRPN++]<a href="https://gojay.top/2020/05/09/SiamRPN++/">[2]</a>.</li></ul></blockquote><h3 id="Depth-refiner"><a href="#Depth-refiner" class="headerlink" title="Depth-refiner"></a>Depth-refiner</h3><blockquote><ol><li>The bounding box estimated by the core contains the whole target, D-r improve the tracker performance just by <code>cutting out no-target area</code>.  </li><li><strong>Information Fusion Network</strong>: It uses <code>depth</code> information to optmize the target state, and <code>color</code> information to overcomes the slight color-depth mismatch. The full architecture is as follows:    </li></ol></blockquote><p><img src="https://i.loli.net/2020/05/23/KsZ3E49h7rqwbHO.png" alt="TSDM_Architecture.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/05/23/vsPDpfiEXBTNzCH.png" alt="TSDM_Result.png"><br><img src="https://i.loli.net/2020/05/23/4khIEPgyb97e5Kp.png" alt="TSDM_VOT.png"><br><img src="https://i.loli.net/2020/05/23/N2DJ1WKnzqisE3u.png" alt="TSDM_PTB.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in <a href="https://github.com/Gojay001/TSDM" target="_blank" rel="noopener">here</a> with citing TSDM<a href="https://github.com/lql-team/TSDM" target="_blank" rel="noopener">[3]</a>.<br>[Updating]</p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>How to use depth information on MOT tasks, detection or re-ID.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] ZHAO, Pengyao, et al. TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator. arXiv preprint arXiv:2005.04063, 2020.<br>[2] Gojay. “SiamRPN++.” <a href="https://gojay.top/2020/05/09/SiamRPN++/">https://gojay.top/2020/05/09/SiamRPN++/</a><br>[3] TSDM. <a href="https://github.com/lql-team/TSDM" target="_blank" rel="noopener">https://github.com/lql-team/TSDM</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TSDM&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.04063&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a &lt;strong&gt;RGB-D tracker&lt;/strong&gt; which use depth information to pretreatment and fuse information to pro-processing. It is composed of a &lt;code&gt;Mask-generator&lt;/code&gt;(M-g), &lt;code&gt;SiamRPN++&lt;/code&gt; and a &lt;code&gt;Depth-refiner&lt;/code&gt;(D-r). There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="TSDM" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/TSDM/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="VOT" scheme="https://gojay.top/tags/VOT/"/>
    
  </entry>
  
  <entry>
    <title>Image Transformer</title>
    <link href="https://gojay.top/2020/05/15/Image-Transformer/"/>
    <id>https://gojay.top/2020/05/15/Image-Transformer/</id>
    <published>2020-05-15T08:15:09.000Z</published>
    <updated>2020-06-17T12:26:30.897Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Image Transformer</strong><a href="https://arxiv.org/abs/1802.05751" target="_blank" rel="noopener">[1]</a> is a sequence modeling formulation of image generation generalized by <code>Transformer</code>, which restricting the <strong>self-attention mechanism</strong> to attend to local neighborhoods, while maintaining <code>large receptive field</code>. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1802.05751" target="_blank" rel="noopener">Image Transformer</a>(2018 arXiv paper)<br><strong>Code</strong>: [Code]<br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=33fbf9ab-c19c-f9e1-01a8-eb0c466a248c&documentId=29df54a6-2fd3-3d8a-8172-398e996de1f5" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/05/15/aolzun27SwJmFGZ.png" alt="ImageTrans_Abstract.png"></p><blockquote><ol><li><strong>Image generation</strong> has been successfully cast as an autoregressive sequence generation or transformation problem.  </li><li>In this work, they generalize the <code>Transformer</code> to a sequence modeling formulation of image generation.  </li><li>By restricting the <strong>self-attention</strong> mechanism to attend to local neighborhoods while maintaining <code>large receptive field</code>.  </li><li>outperform the current <strong>state of the art</strong> in image generation and super-resolution.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/05/15/Z2ok45xiMILATOD.png" alt="ImageTrans_PD.png"></p><blockquote><ol><li>Training <strong>RNNs</strong>(recurrent neural networks) to sequentially predict each pixel of even a small image is <code>computationally</code> very challenging. Thus, <code>parallelizable</code> models that use <strong>CNNs</strong>(convolutional neural networks) such as the PixelCNN have recently received much more attention, and have now surpassed the PixelRNN in quality.  </li><li>One disadvantage of <strong>CNNs</strong> compared to RNNs is their typically fairly <code>limited receptive field</code>. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers.  </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/05/15/XdJgzIWvAaT2cQr.png" alt="ImageTrans_PS.png"></p><blockquote><ol><li><strong>self-attention</strong> can achieve a better <code>balance in the trade-off</code> between the virtually unlimited receptive field of the necessarily sequential <code>PixelRNN</code> and the limited receptive field of the much more parallelizable <code>PixelCNN</code> and its various extensions.<br>We.</li><li>Image Transformer which is a model based entirely on a self-attention mechanism allows us to use significantly <strong>larger receptive fields</strong> than the PixelCNN.  </li><li><strong>Increasing the size</strong> of the receptive field plays a significant role in experiments improvement.  </li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p><img src="https://i.loli.net/2020/05/15/PxnZoR4Q3dawgTJ.png" alt="ImageTrans_SelfAttention.png"></p><blockquote><ol><li>Each self-attention layer computes a <code>d-dimensional representation</code> for each position.  </li><li>it first compares the position’s current representation to other positions’ representations, obtaining an <code>attention distribution</code> over the other positions.  </li><li>This distribution is then used to <code>weight the contribution</code> of the other positions’ representations to the next representation for the position. </li></ol></blockquote><h3 id="Local-Self-Attention"><a href="#Local-Self-Attention" class="headerlink" title="Local Self-Attention"></a>Local Self-Attention</h3><p><img src="https://i.loli.net/2020/05/15/BKF6Dw1yWQLzhIH.png" alt="ImageTrans_LocalAttention.png"></p><blockquote><ol><li>Inspired by CNNs, they address this by adopting a notion of <strong>locality</strong>, restricting the positions in the <code>memory matrix M</code> to a local neighborhood around the query position.  </li><li>They partition the image into <strong>query blocks</strong> and associate each of these with a larger <code>memory block</code>.  </li><li>The model attends to the <strong>same memory matrix</strong>, the self-attention is then computed for all query blocks in parallel.  </li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2020/05/15/u2wic7FlBJG31LQ.png" alt="ImageTrans_details.png"></p><blockquote><ol><li><strong>Recomputing</strong> the representation $q’$ of a single channel of one pixel $q$ by attending to a memory of previously generated pixels $m_1,m_2,…$.  </li><li>After performing <strong>local self-attention</strong> we apply a two-layer position- wise <strong>feed-forward neural network</strong> with the <code>same parameters</code> for all positions in a given layer.  </li><li>Self-attention and the feed-forward networks are followed by <code>dropout</code> and bypassed by a <code>residual connection</code> with subsequent <code>layer normalization</code>.</li></ol></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/05/15/6a3DBNMGqXIJ2bt.png" alt="ImageTrans_Experiments.png"><br><img src="https://i.loli.net/2020/05/15/szybrG49R6w7Cd3.png" alt="ImageTrans_results.png"><br><img src="https://i.loli.net/2020/05/15/cYl3pPF2v4AbW8B.png" alt="ImageTrans_Comparison.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2020/05/15/5RjOb8AcJiDPhYr.png" alt="ImageTrans_future1.png"><br><img src="https://i.loli.net/2020/05/15/i2a3zUuM7bFNjBx.png" alt="ImageTrans_future2.png"></p><blockquote><ol><li>We further hope to have provided additional evidence that even in the light of <strong>GANs</strong>(generative adversarial networks), <strong>likelihood-based</strong> models of images is very much a promising area for further research.  </li><li>We would like to explore a broader variety of <strong>conditioning information</strong> including free-form text, and tasks combining modalities such as language-driven editing of images.  </li><li>Fundamentally, we aim to move beyond still images <strong>to video</strong> and towards applications in model-based <code>reinforcement learning</code>.</li></ol></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><p>[1] Parmar, Niki, et al. “Image transformer.” arXiv preprint arXiv:1802.05751 (2018).</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Image Transformer&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.05751&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a sequence modeling formulation of image generation generalized by &lt;code&gt;Transformer&lt;/code&gt;, which restricting the &lt;strong&gt;self-attention mechanism&lt;/strong&gt; to attend to local neighborhoods, while maintaining &lt;code&gt;large receptive field&lt;/code&gt;. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Image Generation" scheme="https://gojay.top/categories/DeepLearning/Image-Generation/"/>
    
      <category term="ImageTransformer" scheme="https://gojay.top/categories/DeepLearning/Image-Generation/ImageTransformer/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Attention" scheme="https://gojay.top/tags/Attention/"/>
    
  </entry>
  
  <entry>
    <title>SiamRPN++</title>
    <link href="https://gojay.top/2020/05/09/SiamRPN++/"/>
    <id>https://gojay.top/2020/05/09/SiamRPN++/</id>
    <published>2020-05-09T08:44:57.000Z</published>
    <updated>2020-06-17T12:26:30.903Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>SiamRPN++</strong><a href="https://arxiv.org/abs/1812.11703" target="_blank" rel="noopener">[1]</a> is a novel Siamese network based tracker to adopt <strong>deep networks</strong> that broke strict <code>translation invariance</code>. It performs <code>layer-wise</code> and <code>depth-wise</code> aggregations to successfully trained a <code>ResNet-driven</code> Siamese tracker. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1812.11703" target="_blank" rel="noopener">SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks</a>(CVPR 2019 paper)<br><strong>Code</strong>: <a href="https://github.com/STVIR/pysot" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://www.mendeley.com/viewer/?fileId=4ba3094e-f89b-cd13-6eda-a9d9cf3f08b0&documentId=b76fbf38-6c7f-31d6-8357-522ce419fa0b" target="_blank" rel="noopener">Mendeley</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/05/09/dKiPGpSx8LZt1CE.png" alt="SiamRPN___Abstract.png"></p><blockquote><ol><li>Siamese trackers formulate tracking as convolutional feature cross-correlation that still have an <code>accuracy gap</code> to take advantage of features from <strong>deep networks</strong>.  </li><li>This paper proved the core reason comes from the <code>lack ofstrict translation invariance</code>, and break this restriction through a simple yet effective <strong>spatial aware sampling strategy</strong>.  </li><li>They further proposed a new model architecture to perform <strong>layer-wise and depth- wise aggregations</strong>.  </li><li>It obtains currently the <strong>best results</strong> on five large tracking benchmarks.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/05/09/RQm2BqgKuEZC49o.png" alt="SiamRPN___PD.png"></p><blockquote><ol><li><code>Padding</code> in deep networks will destroy the strict <strong>translation invariance</strong>.  </li><li><code>RPN</code> requires <strong>asymmetrical</strong> features for classification and regression. </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/05/09/JdM6ON2mR9jxvI3.png" alt="SiamRPN___PS.png"></p><blockquote><ol><li><strong>Sampling strategy</strong>: break the spatial <code>invariance</code> restriction.</li><li><strong>Layer-wise</strong> feature aggregation: predict the similarity map from features learned at <code>multiple levels</code>.  </li><li><strong>Depth-wise</strong> separable correlation: produce multiple similarity maps associated with different semantic meanings to <code>reduces the parameter number</code>.  </li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/05/09/DnFoO9fEZIcYiuB.png" alt="SiamRPN___CU.png"></p><blockquote><ol><li><strong>Hypothesis</strong>: the violation of strict translation invariance will lead to a <code>spatial bias</code>.  </li><li><strong>Experiments</strong>: targets are placed in the center with <code>different shift ranges</code> in sepreate training experiments.  </li><li><strong>Results</strong>: a strong center bias is learned, increasing shift ranges could <code>learn more area</code> to alleviate it. </li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2020/05/09/IiRTPfqLa27oStE.png" alt="SiamRPN___framework.png"></p><h3 id="Layer-wise-Aggregation"><a href="#Layer-wise-Aggregation" class="headerlink" title="Layer-wise Aggregation"></a>Layer-wise Aggregation</h3><p><img src="https://i.loli.net/2020/05/09/NHGbl5iJcjFohxs.png" alt="SiamRPN___layer-wise.png"></p><blockquote><ol><li>They explore <strong>multi-level features</strong> both low level and semantic information that extracted from the last three residual block, refering these outputs as $F_3(z)$, $F_4(z)$, and $F_5(z)$.  </li><li>The output sizes of the three RPN modules have the same spatial resolution, <strong>weighted sum</strong> is adopted directly on the RPN output.</li></ol></blockquote><h3 id="Depth-wise-Cross-Correlation"><a href="#Depth-wise-Cross-Correlation" class="headerlink" title="Depth-wise Cross Correlation"></a>Depth-wise Cross Correlation</h3><p><img src="https://i.loli.net/2020/05/09/mUdgQ1nJy4uRqOt.png" alt="SiamRPN___XCorr.png"><br><img src="https://i.loli.net/2020/05/09/dUDWuGs3Xotikjv.png" alt="SiamRPN___depth-wise.png"></p><blockquote><ol><li>A <strong>conv-bn block</strong> is adopted to make two feature maps with the same number of channels do the <code>correlation operation</code>.  </li><li>Another <strong>conv-bn-relu block</strong> is appended to <code>fuse different channel</code> outputs.</li></ol></blockquote><p><img src="https://i.loli.net/2020/05/09/OMvzBE2nfeVrhLP.png" alt="SiamRPN___phenomena.png"><br>Furthermore, an interesting phenomena is that the objects in the same category have high response on same channels, while responses of the rest channels are suppressed. It can be comprehended as <code>each channel represents some semantic information</code>.  </p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/05/09/QziTsWuJ4m5HY9n.png" alt="SiamRPN___results.png"><br><img src="https://i.loli.net/2020/05/09/QzWjHtSND98s3FJ.png" alt="SiamRPN___more.png"> </p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in [pysot]<a href="https://github.com/STVIR/pysot" target="_blank" rel="noopener">[2]</a>.</p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details of <strong>SiamRPN++</strong> and the like can be found in <a href="https://zhuanlan.zhihu.com/p/66757733" target="_blank" rel="noopener">[3]</a>.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] LI, Bo, et al. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019. p. 4282-4291.<br>[2] pysot. <a href="https://github.com/STVIR/pysot" target="_blank" rel="noopener">https://github.com/STVIR/pysot</a>.<br>[3] Erer Huang. “Overview of Siamese Network Methods.” <a href="https://zhuanlan.zhihu.com/p/66757733" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/66757733</a>.  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;SiamRPN++&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.11703&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a novel Siamese network based tracker to adopt &lt;strong&gt;deep networks&lt;/strong&gt; that broke strict &lt;code&gt;translation invariance&lt;/code&gt;. It performs &lt;code&gt;layer-wise&lt;/code&gt; and &lt;code&gt;depth-wise&lt;/code&gt; aggregations to successfully trained a &lt;code&gt;ResNet-driven&lt;/code&gt; Siamese tracker. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="SiamRPN++" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/SiamRPN/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="VOT" scheme="https://gojay.top/tags/VOT/"/>
    
  </entry>
  
  <entry>
    <title>Overview</title>
    <link href="https://gojay.top/2020/04/01/Overview/"/>
    <id>https://gojay.top/2020/04/01/Overview/</id>
    <published>2020-04-01T02:32:41.000Z</published>
    <updated>2020-08-19T14:09:55.053Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>There are the overall of papers about <strong>Deep Learning</strong>.<br><a href="https://github.com/Gojay001/DeepLearning-pwcn" target="_blank" rel="noopener">https://github.com/Gojay001/DeepLearning-pwcn</a></p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><a href="#Image-Classification">Classification</a><ul><li>LeNet-5</li><li>AlexNet</li><li>NIN(Network In Network)</li><li>VGG</li><li>GoogLeNet(Inception-v1)</li><li>ResNet</li><li>Inception-v4</li><li>DenseNet</li><li>ShuffleNet</li><li>MobileNetV3</li></ul></li><li><a href="#Object-Detection">Detection</a><ul><li>One-stage<ul><li>SSD</li><li>YOLO</li><li>YOLOv2</li><li>YOLOv3</li><li>YOLOv4</li></ul></li><li>Two-stage<ul><li>R-CNN</li><li>Fast R-CNN</li><li>Faster R-CNN</li><li>FPN</li></ul></li></ul></li><li><a href="#Object-Segmentation">Segmentation</a><ul><li>Mask R-CNN</li></ul></li><li><a href="#Object-Tracking">Tracking</a><ul><li>MOT<ul><li>SORT</li><li>DeepSORT</li><li>Tracktor</li><li>Flow-Fuse Tracker</li><li>JRMOT</li><li>Tracklet</li><li>FairMOT</li></ul></li><li>VOT<ul><li>DepthTrack</li><li>BinocularTrack</li><li>SiamRPN++</li><li>SiamMask</li><li>GlobalTrack</li><li>PAMCC-AOT</li><li>TSDM</li></ul></li></ul></li><li><a href="#Few-Shot-Segmentation">FSS</a><ul><li>OSLSM</li><li>SG-One(Similarity Guidance)</li><li>CENet(Combinatorial Embedding Network)</li><li>PANet(Prototype Alignment)</li><li>PGNet(Pyramid Graph Network)</li><li>AMP(Adaptive Masked Proxies)</li><li>CRNet(Cross-Reference Network)</li><li>FGN(Fully Guided Network)</li><li>DoG-BConvLSTM</li><li>LTM(Local Transformation Module)</li></ul></li><li><a href="#3D-Object-Detection">Detection-3D</a><ul><li>PV-RCNN</li></ul></li><li><a href="#Few-Shot-Learning">FSL</a><ul><li>RN(Relation Network)</li></ul></li><li><a href="#Generative-Adversarial-Network">GAN</a><ul><li>BeautyGAN</li></ul></li><li><a href="#Image-Generation">Image Generation</a><ul><li>ImageTransformer</li></ul></li><li><a href="#Survey">Survey</a><ul><li>3D-Detection-Survey-2019</li><li>FSL-Survey-2019</li><li>MOT-Survey-2020</li></ul></li></ul><h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left">LeNet-5</td><td align="center"><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a></td><td align="center">IEEE(1998)</td><td align="center">[code]</td></tr><tr><td align="left">AlexNet</td><td align="center"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></td><td align="center">NIPS(2012)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2019/08/31/NIN-Network-In-Network/">NIN</a></td><td align="center"><a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network In Network</a></td><td align="center">arXiv(2013)</td><td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/NIN/Code" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left">VGG</td><td align="center"><a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></td><td align="center">ICLR(2015)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2019/09/05/GoogLeNet/">GoogLeNet</a></td><td align="center"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Going deeper with convolutions</a></td><td align="center">CVPR(2015)</td><td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/GoogLeNet/Code" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2019/09/08/ResNet/">ResNet</a></td><td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></td><td align="center">CVPR(2016)</td><td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left">Inception-v4</td><td align="center"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14806/14311" target="_blank" rel="noopener">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></td><td align="center">AAAI(2017)</td><td align="center">[code]</td></tr><tr><td align="left">DenseNet</td><td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a></td><td align="center">CVPR(2017)</td><td align="center">[code]</td></tr><tr><td align="left">ShuffleNet</td><td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></td><td align="center">CVPR(2018)</td><td align="center">[code]</td></tr><tr><td align="left">MobileNetV3</td><td align="center"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Searching for MobileNetV3</a></td><td align="center">ICCV(2019)</td><td align="center">[code]</td></tr></tbody></table><blockquote><p>More information can be found in <a href="https://github.com/weiaicunzai/awesome-image-classification" target="_blank" rel="noopener">Awesome - Image Classification</a>.</p></blockquote><h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left">R-CNN</td><td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noopener">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></td><td align="center">CVPR(2014)</td><td align="center">[code]</td></tr><tr><td align="left">Fast R-CNN</td><td align="center"><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Fast R-CNN</a></td><td align="center">ICCV(2015)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2019/10/19/Faster-R-CNN/">Faster R-CNN</a></td><td align="center"><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td><td align="center">NIPS(2015)</td><td align="center"><a href="https://github.com/Gojay001/faster-rcnn.pytorch" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left">SSD</td><td align="center"><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></td><td align="center">ECCV(2016)</td><td align="center"><a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">Caffe</a></td></tr><tr><td align="left">YOLO</td><td align="center"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></td><td align="center">CVPR(2016)</td><td align="center">[code]</td></tr><tr><td align="left">YOLOv2</td><td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a></td><td align="center">CVPR(2017)</td><td align="center">[code]</td></tr><tr><td align="left">FPN</td><td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Feature Pyramid Networks for Object Detection</a></td><td align="center">CVPR(2017)</td><td align="center">[code]</td></tr><tr><td align="left">YOLOv3</td><td align="center"><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">YOLOv3: An Incremental Improvement</a></td><td align="center">arXiv(2018)</td><td align="center"><a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">Offical</a></td></tr><tr><td align="left">YOLOv4</td><td align="center"><a href="https://arxiv.org/abs/2004.10934" target="_blank" rel="noopener">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td><td align="center">arXiv(2020)</td><td align="center"><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">Offical</a></td></tr></tbody></table><blockquote><p>More information can be found in <a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">awesome-object-detection</a>.</p></blockquote><h2 id="Object-Segmentation"><a href="#Object-Segmentation" class="headerlink" title="Object Segmentation"></a>Object Segmentation</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left"><a href="https://gojay.top/2020/08/17/Mask-R-CNN/">Mask R-CNN</a></td><td align="center"><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Mask R-CNN</a></td><td align="center">ICCV(2017)</td><td align="center"><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">PyTorch</a></td></tr></tbody></table><h2 id="Object-Tracking"><a href="#Object-Tracking" class="headerlink" title="Object Tracking"></a>Object Tracking</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left"><a href="https://gojay.top/2020/06/14/SORT/">SORT</a></td><td align="center"><a href="https://arxiv.org/abs/1602.00763" target="_blank" rel="noopener">Simple Online and Realtime Tracking</a></td><td align="center">ICIP(2016)</td><td align="center"><a href="https://github.com/abewley/sort" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left">DepthTrack</td><td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/Binocular%20camera/DepthTrack.pdf" target="_blank" rel="noopener">Real-time depth-based tracking using a binocular camera</a></td><td align="center">WCICA(2016)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2020/06/20/DeepSORT/">DeepSORT</a></td><td align="center"><a href="https://arxiv.org/abs/1703.07402" target="_blank" rel="noopener">Simple Online and Realtime Tracking with a Deep Association Metric</a></td><td align="center">ICIP(2017)</td><td align="center"><a href="https://github.com/nwojke/deep_sort" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left">BinocularTrack</td><td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/blob/master/Tracking/Binocular%20camera/BinocularTrack.pdf" target="_blank" rel="noopener">Research on Target Tracking Algorithm Based on Parallel Binocular Camera</a></td><td align="center">ITAIC(2019)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2020/05/09/SiamRPN++/">SiamRPN++</a></td><td align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_SiamRPN_Evolution_of_Siamese_Visual_Tracking_With_Very_Deep_Networks_CVPR_2019_paper.pdf" target="_blank" rel="noopener">SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks</a></td><td align="center">CVPR(2019)</td><td align="center"><a href="https://github.com/STVIR/pysot" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2019/11/26/SiamMask/">SiamMask</a></td><td align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a></td><td align="center">CVPR(2019)</td><td align="center"><a href="https://github.com/Gojay001/SiamMask" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2019/11/09/Tracktor/">Tracktor</a></td><td align="center"><a href="https://arxiv.org/abs/1903.05625" target="_blank" rel="noopener">Tracking without bells and whistles</a></td><td align="center">ICCV(2019)</td><td align="center"><a href="https://github.com/Gojay001/tracking_wo_bnw" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2020/01/04/GlobalTrack/">GlobalTrack</a></td><td align="center"><a href="https://arxiv.org/abs/1912.08531" target="_blank" rel="noopener">GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</a></td><td align="center">AAAI(2020)</td><td align="center"><a href="https://github.com/huanglianghua/GlobalTrack" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2020/02/25/PAMCC-AOT/">PAMCC-AOT</a></td><td align="center"><a href="https://arxiv.org/abs/2001.05161" target="_blank" rel="noopener">Pose-Assisted Multi-Camera Collaboration for Active Object Tracking</a></td><td align="center">AAAI(2020)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2020/03/05/FFT-Flow-Fuse-Tracker/">FFT</a></td><td align="center"><a href="https://arxiv.org/abs/2001.11180" target="_blank" rel="noopener">Multiple Object Tracking by Flowing and Fusing</a></td><td align="center">arXiv(2020)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2020/02/28/JRMOT/">JRMOT</a></td><td align="center"><a href="https://arxiv.org/abs/2002.08397" target="_blank" rel="noopener">JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</a></td><td align="center">arXiv(2020)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2020/03/26/Tracklet/">Tracklet</a></td><td align="center"><a href="https://arxiv.org/abs/2003.02795" target="_blank" rel="noopener">Multi-object Tracking via End-to-end Tracklet Searching and Ranking</a></td><td align="center">arXiv(2020)</td><td align="center">[code]</td></tr><tr><td align="left"><a href="https://gojay.top/2020/05/23/TSDM/">TSDM</a></td><td align="center"><a href="https://arxiv.org/abs/2005.04063" target="_blank" rel="noopener">TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator</a></td><td align="center">arXiv(2020)</td><td align="center"><a href="https://github.com/Gojay001/TSDM" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2020/05/25/FairMOT/">FairMOT</a></td><td align="center"><a href="https://arxiv.org/abs/2004.01888" target="_blank" rel="noopener">A Simple Baseline for Multi-Object Tracking</a></td><td align="center">arXiv(2020)</td><td align="center"><a href="https://github.com/Gojay001/FairMOT" target="_blank" rel="noopener">PyTorch</a></td></tr></tbody></table><h2 id="Few-Shot-Segmentation"><a href="#Few-Shot-Segmentation" class="headerlink" title="Few-Shot Segmentation"></a>Few-Shot Segmentation</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left">OSLSM</td><td align="center"><a href="https://arxiv.org/abs/1709.03410" target="_blank" rel="noopener">One-Shot Learning for Semantic Segmentation</a></td><td align="center">arXiv(2017)</td><td align="center"><a href="https://github.com/lzzcd001/OSLSM" target="_blank" rel="noopener">Caffe</a></td></tr><tr><td align="left">SG-One</td><td align="center"><a href="https://arxiv.org/abs/1810.09091" target="_blank" rel="noopener">SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation</a></td><td align="center">arXiv(2018) / ITC(2020)</td><td align="center"><a href="https://github.com/xiaomengyc/SG-One" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left">CENet</td><td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Learning Combinatorial Embedding Networks for Deep Graph Matching</a></td><td align="center">ICCV(2019)</td><td align="center"><a href="https://github.com/Thinklab-SJTU/PCA-GM" target="_blank" rel="noopener">Pytorch</a></td></tr><tr><td align="left">PANet</td><td align="center"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf" target="_blank" rel="noopener">PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</a></td><td align="center">ICCV(2019)</td><td align="center"><a href="https://github.com/kaixin96/PANet" target="_blank" rel="noopener">PyTorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2020/07/28/PGNet/">PGNet</a></td><td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Pyramid Graph Networks with Connection Attentions for Region-Based One-Shot Semantic Segmentation</a></td><td align="center">ICCV(2019)</td><td align="center">[code]</td></tr><tr><td align="left">AMP</td><td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">AMP: Adaptive Masked Proxies for Few-Shot Segmentation</a></td><td align="center">ICCV(2019)</td><td align="center"><a href="https://github.com/MSiam/AdaptiveMaskedProxies" target="_blank" rel="noopener">Pytorch</a></td></tr><tr><td align="left"><a href="https://gojay.top/2020/07/10/CRNet/">CRNet</a></td><td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CRNet_Cross-Reference_Networks_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">CRNet: Cross-Reference Networks for Few-Shot Segmentation</a></td><td align="center">CVPR(2020)</td><td align="center">[code]</td></tr><tr><td align="left">FGN</td><td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_FGN_Fully_Guided_Network_for_Few-Shot_Instance_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">FGN: Fully Guided Network for Few-Shot Instance Segmentation</a></td><td align="center">CVPR(2020)</td><td align="center">[code]</td></tr><tr><td align="left">DoG-BConvLSTM</td><td align="center"><a href="https://arxiv.org/abs/2003.04052" target="_blank" rel="noopener">On the Texture Bias for Few-Shot CNN Segmentation</a></td><td align="center">arXiv(2020)</td><td align="center"><a href="https://github.com/rezazad68/fewshot-segmentation" target="_blank" rel="noopener">TensorFlow</a></td></tr><tr><td align="left"><a href="https://gojay.top/2020/07/29/LTM/">LTM</a></td><td align="center"><a href="https://arxiv.org/abs/1910.05886" target="_blank" rel="noopener">A New Local Transformation Module for Few-Shot Segmentation</a></td><td align="center">ICMM(2020)</td><td align="center">[code]</td></tr></tbody></table><blockquote><p>More information can be found in <a href="https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers" target="_blank" rel="noopener">Few-Shot-Semantic-Segmentation-Papers</a>.</p></blockquote><h2 id="3D-Object-Detection"><a href="#3D-Object-Detection" class="headerlink" title="3D Object Detection"></a>3D Object Detection</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left"><a href="https://gojay.top/2020/06/23/PV-RCNN/">PV-RCNN</a></td><td align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</a></td><td align="center">CVPR(2020)</td><td align="center"><a href="https://github.com/sshaoshuai/PV-RCNN" target="_blank" rel="noopener">PyTorch</a></td></tr></tbody></table><h2 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left"><a href="https://gojay.top/2019/08/21/RN-Realation-Network/">RN</a></td><td align="center"><a href="https://arxiv.org/abs/1711.06025" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning</a></td><td align="center">CVPR(2018)</td><td align="center"><a href="https://github.com/Gojay001/LearningToCompare_FSL" target="_blank" rel="noopener">PyTorch</a></td></tr></tbody></table><h2 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left">BeautyGAN</td><td align="center"><a href="http://colalab.org/media/paper/BeautyGAN-camera-ready.pdf" target="_blank" rel="noopener">Beautygan: Instance-level facial makeup transfer with deep generative adversarial network</a></td><td align="center">ACM(2018)</td><td align="center"><a href="http://liusi-group.com/projects/BeautyGAN" target="_blank" rel="noopener">TensorFlow</a></td></tr></tbody></table><h2 id="Image-Generation"><a href="#Image-Generation" class="headerlink" title="Image Generation"></a>Image Generation</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th><th align="center">Code</th></tr></thead><tbody><tr><td align="left"><a href="https://gojay.top/2020/05/15/Image-Transformer/">ImageTransformer</a></td><td align="center"><a href="https://arxiv.org/abs/1802.05751" target="_blank" rel="noopener">Image Transformer</a></td><td align="center">arXiv(2018)</td><td align="center">[code]</td></tr></tbody></table><h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><table><thead><tr><th align="left">Title</th><th align="center">Paper</th><th align="center">Conf</th></tr></thead><tbody><tr><td align="left">3D-Detection-Survey-2019</td><td align="center"><a href="http://wrap.warwick.ac.uk/114314/1/WRAP-survey-3D-object-detection-methods-autonomous-driving-applications-Arnold-2019.pdf" target="_blank" rel="noopener">A Survey on 3D Object Detection Methods for Autonomous Driving Applications</a></td><td align="center">ITS(2019)</td></tr><tr><td align="left"><a href="https://gojay.top/2020/07/07/FSL-Survey-2019/">FSL-Survey-2019</a></td><td align="center"><a href="https://arxiv.org/abs/1904.05046" target="_blank" rel="noopener">Generalizing from a Few Examples: A Survey on Few-Shot Learning</a></td><td align="center">CSUR(2019)</td></tr><tr><td align="left">MOT-Survey-2020</td><td align="center"><a href="https://arxiv.org/abs/1907.12740" target="_blank" rel="noopener">Deep Learning in Video Multi-Object Tracking: A Survey</a></td><td align="center">Neurocomputing(2020)</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;There are the overall of papers about &lt;strong&gt;Deep Learning&lt;/strong&gt;.&lt;br&gt;&lt;a href=&quot;https://github.com/Gojay001/DeepLearning-pwcn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Gojay001/DeepLearning-pwcn&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Overview" scheme="https://gojay.top/categories/DeepLearning/Overview/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Overview" scheme="https://gojay.top/tags/Overview/"/>
    
  </entry>
  
  <entry>
    <title>Tracklet</title>
    <link href="https://gojay.top/2020/03/26/Tracklet/"/>
    <id>https://gojay.top/2020/03/26/Tracklet/</id>
    <published>2020-03-26T08:41:56.000Z</published>
    <updated>2020-06-17T12:26:30.904Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Tracklet</strong><a href="https://arxiv.org/abs/2003.02795" target="_blank" rel="noopener">[1]</a> is a novel method for optimizing <code>tracklet consistency</code>, which directly takes the <code>prediction errors</code> into account. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2003.02795" target="_blank" rel="noopener">Multi-object Tracking via End-to-end Tracklet Searching and Ranking</a>(arXiv 2020 paper)<br><strong>Code</strong>: [Pytorch][Updating]<br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/Tracklet/Note" target="_blank" rel="noopener">Tracklet</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/03/27/nxgNL3iSzwIy6Fr.png" alt="Tracklet_Abstract.png"></p><blockquote><ol><li>Recent work use <strong>sequence model</strong> to calculate the <code>similarity score</code> between the detections and the previous tracklets, but the forced exposure to ground-truth in the training stage leads to the <code>training-inference discrepancy</code> problem.  </li><li>This paper directly takes the <code>prediction errors</code> into account to optimize tracklet consistency.  </li><li>It havs achieved <code>state-of-the-art</code> in MOT15-17 challenge benchmarks using public detection and online settings.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/03/27/zJ7hZow1cfXHgjI.png" alt="Tracklet_PD1.png"><br><img src="https://i.loli.net/2020/03/27/xyDN9hWI5TbrZAF.png" alt="Tracklet_PD2.png"></p><blockquote><ol><li><strong>pairwise-detection matching</strong> based on affinity model: It has limited capability to associate <code>long-term</code> consistent trajectories.  </li><li>affinity model on <strong>sequence model</strong>: tracklet representative feature for <code>matching</code> can somewhat be ill-posed and ideal assumption brings up a potential <code>vulnerability</code>. </li></ol></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/03/27/FWGkirCulfpbhgv.png" alt="Tracklet_PS.png"></p><blockquote><ol><li>They propose a <strong>global score</strong> to measure the inner <code>appearance consistency</code> of tracklet.</li><li>It optimizes the <strong>whole tracklet</strong> with a <code>margin loss</code>.  </li><li>a novel algorithm has been established to <code>simulate the prediction data distribution on training</code> by introducing realistic discombobulated candidates to model.  </li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/03/27/3ZmrPFzwNvLkKUI.png" alt="Tracklet_CU.png"></p><blockquote><ol><li><strong>Tracklet-level</strong> based tracking: It constructs an affinity model on the <code>tracklet level</code> and then uses it to associate the tracklet with detection or connect short tracklets.  </li><li><strong>Pair-wise association</strong> methods: They establish an affinity model on the <code>isolated detections</code>, and then generate tracking results from the bottom up.<br>The <strong>common concern</strong> of these two types of methods is to <code>guarantee the consistency of the entire associated trajectories</code>. </li></ol></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2020/03/27/219VSogiUlnLwqM.png" alt="Tracklet_Network.png"></p><blockquote><ol><li>Training procedure: It follows a “searching-learning-ranking-pruning” pipeline.  </li><li>Scoring Network: The <code>appearance feature</code> of each detection are extracted with CNN(<strong>ResNet-50</strong>), and the <code>appearance embedding</code> of tracklet are obtained through encoder(<strong>LSTM</strong>).  </li><li>It trained by online hypothesis tracklet searching with <code>margin loss</code> and <code>rank loss</code>, details as follow.</li></ol></blockquote><p><img src="https://i.loli.net/2020/03/27/IxdSb8aVH3DBsOG.png" alt="Tracklet_CC.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/03/27/qAfh2vIuDRWTtlX.png" alt="Tracklet_Results.png">  </p><blockquote><p>They report the quantitative results on the three datasets in <strong>MOT Challenge Benchmark</strong>.</p></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p><img src="https://i.loli.net/2020/03/27/CgTOnXJujhDeIsV.png" alt="Tracklet_SBTO.png"></p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details of <strong>Tracklet optimization</strong> and the like can be found in <a href="https://blog.csdn.net/qq_36449741/article/details/104815321?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">[2]</a>.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><p>[1] Hu, T., Huang, L., &amp; Shen, H. (2020). Multi-object Tracking via End-to-end Tracklet Searching and Ranking. arXiv preprint arXiv:2003.02795.<br>[2] Change_ZH. “Tracklet: MOT Scoring Network.” <a href="https://blog.csdn.net/qq_36449741/article/details/104815321?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">https://blog.csdn.net/qq_36449741/article/details/104815321?depth_1-utm_source=distribute.pc_relevant.none-task&amp;utm_source=distribute.pc_relevant.none-task</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tracklet&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.02795&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a novel method for optimizing &lt;code&gt;tracklet consistency&lt;/code&gt;, which directly takes the &lt;code&gt;prediction errors&lt;/code&gt; into account. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="Tracklet" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/Tracklet/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>FFT(Flow-Fuse Tracker)</title>
    <link href="https://gojay.top/2020/03/05/FFT-Flow-Fuse-Tracker/"/>
    <id>https://gojay.top/2020/03/05/FFT-Flow-Fuse-Tracker/</id>
    <published>2020-03-05T03:13:26.000Z</published>
    <updated>2020-06-17T12:26:30.893Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>FFT(Flow-Fuse Tracker)</strong><a href="https://arxiv.org/abs/2001.11180" target="_blank" rel="noopener">[1]</a> is an end-to-end DNN tracking approach, that jointly learns both target <code>motions</code> and <code>associations</code> for <strong>MOT</strong>(multiple object tracking). There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/2001.11180" target="_blank" rel="noopener">Multiple Object Tracking by Flowing and Fusing</a>(arXiv 2020 paper)<br><strong>Code</strong>: [Pytorch][Updating]<br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/FFT/Note" target="_blank" rel="noopener">FFT</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/03/05/1OesrJHjDAGtz9P.png" alt="FFT_Abstract.png"></p><blockquote><ol><li><strong>Previous</strong>: estimating target-wise motions and conducting pair-wise Re-Identification(Re-ID).  </li><li><strong>This paper</strong>: target <code>flowing</code> and target <code>fusing</code>.  </li><li><strong>Achievment</strong>: SOTA on 2DMOT15, MOT16 and MOT17.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/03/05/8OtgljCNLzT9o76.png" alt="FTT_PD.png"></p><blockquote><p>Reccent approaches: First produce target <code>motion and appearance features</code> respectively, then conduct target <code>association</code> between frames.<br>It is very difficult for <strong>computating</strong> both features.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/03/05/VRa6PuLqOtsTZK3.png" alt="FFT_PS.png"></p><blockquote><p>It includes two techniques: <strong>target flowing</strong> and <strong>target fusing</strong>.<br>For target flowing: <code>FlowTracker</code> extract target-wise motions from pixel-level optical flows.<br>For target fusing: <code>FuseTracker</code> refines and fuse targets.  </p></blockquote><p><img src="https://i.loli.net/2020/03/05/23n594VCaT8ODXA.png" alt="FFT_Step.png"></p><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/03/05/vLTWRQGalhquezm.png" alt="FFT_Overview.png"><br><img src="https://i.loli.net/2020/03/05/3hD9UVx1i7taR5I.png" alt="FFT_FlowTracker.png"><br><img src="https://i.loli.net/2020/03/05/lJauE7OYwsFZf8X.png" alt="FFT_FuseTracker.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/03/05/9IFQE2sliwM1JBc.png" alt="FFT_AblationStudy.png"><br><img src="https://i.loli.net/2020/03/05/WubM9GzesxKoaJl.png" alt="FFT_MOT15.png"><br><img src="https://i.loli.net/2020/03/05/shPekuRUY1v2qpa.png" alt="FFT_MOT.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p><img src="https://i.loli.net/2020/03/05/Siv6hm3QZLpftJO.png" alt="FFT_Algorithm.png"></p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2020/03/05/FXkIu4oRUAyMWSN.png" alt="FFT_Improvement.png"></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Zhang, Jimuyang, et al. “Multiple Object Tracking by Flowing and Fusing.” arXiv preprint arXiv:2001.11180 (2020).  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;FFT(Flow-Fuse Tracker)&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.11180&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is an end-to-end DNN tracking approach, that jointly learns both target &lt;code&gt;motions&lt;/code&gt; and &lt;code&gt;associations&lt;/code&gt; for &lt;strong&gt;MOT&lt;/strong&gt;(multiple object tracking). There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="FFT" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/FFT/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>JRMOT</title>
    <link href="https://gojay.top/2020/02/28/JRMOT/"/>
    <id>https://gojay.top/2020/02/28/JRMOT/</id>
    <published>2020-02-28T09:17:03.000Z</published>
    <updated>2020-06-17T12:26:30.897Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>JRMOT</strong><a href="https://arxiv.org/pdf/2002.08397.pdf" target="_blank" rel="noopener">[1]</a> is a novel <strong>3D MOT</strong> system that integrates information from <code>2D RGB images</code> and <code>3D point clouds</code> into a real-time performing framework. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/2002.08397.pdf" target="_blank" rel="noopener">JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</a>(arXiv 2020 paper)<br><strong>Code</strong>: [Pytorch][Updating]<br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/JRMOT/Note" target="_blank" rel="noopener">JRMOT</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/03/07/dmaLBQJ4GUCwT68.png" alt="JRMOT_Abstract.png"></p><blockquote><ol><li><strong>JRMOT</strong> is a novel 3D MOT system that integrates information from 2D RGB images and 3D point clouds into a real-time performing framework.  </li><li>They also released the <strong>JRDB dataset</strong>, which is a novel large scale 2D+3D dataset.  </li><li>It demonstrates <strong>state-of-the-art</strong> performance against competing methods on the popular <code>2D tracking</code> KITTI benchmark and serves as a competitive <code>3D tracking</code> baseline for our dataset and benchmark.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/03/07/dghLEX3SUctofVY.png" alt="JRMOT_PD.png"></p><blockquote><p><strong>MOT</strong>: the agent needs to perceive the motion of the multiple dynamic objects and other agents.<br><strong>Reccent approaches:</strong>: perceive <code>2D motion</code> from RGB video streams. </p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/03/07/HfmgSq3XZUKW9Rw.png" alt="JRMOT_PS.png"></p><blockquote><ol><li>It integrates 2D detection from <code>Mask R-CNN</code> and 3D information from <code>F-PointNet</code>.</li><li>It fuses the 3D shape descriptor with a 2D RGB descriptor through <code>Aligned-ReID</code>.  </li><li>It uses optimal joint probabilistic <code>data association</code> step.  </li><li>A novel multi-modal <code>recursive Kalman filter</code> was proposed.</li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/03/07/MFN7LyRxT2ptcVr.png" alt="JRMOT_Overview.png"></p><blockquote><p>It shows all components of the JRMOT, and workflow as below.<br><img src="https://i.loli.net/2020/03/07/GkX5gKVAhBJSYzs.png" alt="JRMOT_Workflow.png"></p></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/03/07/7sZvzGifPRN35Qt.png" alt="JRMOT_Car.png"><br><img src="https://i.loli.net/2020/03/07/4Dk8QM7rW2xwqCX.png" alt="JRMOT_Pedestrian.png">  </p><blockquote><p>It results on online KITTI Car and Pedestrian Tracking, and gets SOTA performance.  </p></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2020/03/07/JcwWAfkHONEIUne.png" alt="JRMOT_dataset.png"></p><blockquote><p>It shows data collection platform and sample visualization of the dataset.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><ol><li>Shenoi A, Patel M, Gwak J Y, et al. JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset[J]. arXiv preprint arXiv:2002.08397, 2020.</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;JRMOT&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.08397.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a novel &lt;strong&gt;3D MOT&lt;/strong&gt; system that integrates information from &lt;code&gt;2D RGB images&lt;/code&gt; and &lt;code&gt;3D point clouds&lt;/code&gt; into a real-time performing framework. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="JRMOT" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/JRMOT/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>PAMCC-AOT</title>
    <link href="https://gojay.top/2020/02/25/PAMCC-AOT/"/>
    <id>https://gojay.top/2020/02/25/PAMCC-AOT/</id>
    <published>2020-02-25T09:23:40.000Z</published>
    <updated>2020-06-17T12:26:30.901Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Pose-Assisted Multi-Camera Collaboration System</strong><a href="https://arxiv.org/pdf/2001.05161.pdf" target="_blank" rel="noopener">[1]</a> is a novel method, which enables a camera to cooperate with the others by sharing camera poses for <strong>AOT</strong>(active object tracking). There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/2001.05161.pdf" target="_blank" rel="noopener">Pose-Assisted Multi-Camera Collaboration for Active Object Tracking</a>(AAAI 2020 paper)<br><strong>Code</strong>: [Pytorch][Updating]<br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/PAMCC-AOT/Note" target="_blank" rel="noopener">PAMCC-AOT</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/03/05/boPwgRD79B1cmJk.png" alt="PAMCC_Abstract.png"></p><blockquote><ol><li><strong>PAMCC-AOT</strong> is proposed to solve complex scenarios problems.</li><li>The <code>vision-based controller</code> tracks targets based on observed images.  </li><li>The <code>pose-based controller</code> moves the camera in accordance to the poses of the other cameras.  </li><li>At each step, the <code>switcher</code> decides which action to take from the two controllers according to the visibility of the target.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/03/05/sUzY42L75oXkWn6.png" alt="PAMCC_PD.png"></p><blockquote><p><strong>AOT</strong>: a tracker is able to control its motion so as to follow a target autonomously.<br><strong>Problems</strong>: high complexity of environments and limitation of camera mobility. </p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/03/05/uLdrSojp5OI71RN.png" alt="PAMCC_PS.png"></p><blockquote><ol><li>It extend the independent AOT to the <code>CMC-AOT</code>.  </li><li>They proposed <code>PAMCC-AOT</code> sharing camera poses.  </li><li>They provided a set of <code>3D environments</code>.</li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/03/05/LCe9slH3XKqUmZO.png" alt="PAMCC_Overview.png"><br><img src="https://i.loli.net/2020/03/05/tey8owNKmrjbsW4.png" alt="PAMCC_Network.png"></p><blockquote><p>In the system, each camera is equipped with <code>two controllers</code> and <code>a switcher</code>.  </p><ol><li><strong>Vision-based Controller</strong>: it serves as an image processor and guides the camera to execute policy based on image observation.  </li><li><strong>Pose-based Controller</strong>: it helps the camera who receives an imperfect observation to execute policy based on the supplementary pose information provided by other cameras.  </li><li><strong>Switcher</strong>: it makes the camera switch between the vision-based controller and pose-based controller properly.</li></ol></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/03/05/xTiPhZcnHMRDW3l.png" alt="PAMCC_Results.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><p>[Updating]</p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2020/03/05/CQniYkFgSTRpL84.png" alt="PAMCC_Conclusion.png"></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Li, Jing, et al. “Pose-Assisted Multi-Camera Collaboration for Active Object Tracking.” arXiv preprint arXiv:2001.05161 (2020).  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Pose-Assisted Multi-Camera Collaboration System&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2001.05161.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a novel method, which enables a camera to cooperate with the others by sharing camera poses for &lt;strong&gt;AOT&lt;/strong&gt;(active object tracking). There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="PAMCC-AOT" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/PAMCC-AOT/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="AOT" scheme="https://gojay.top/tags/AOT/"/>
    
  </entry>
  
  <entry>
    <title>GlobalTrack</title>
    <link href="https://gojay.top/2020/01/04/GlobalTrack/"/>
    <id>https://gojay.top/2020/01/04/GlobalTrack/</id>
    <published>2020-01-04T00:15:20.000Z</published>
    <updated>2020-06-17T12:26:30.895Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>GlobalTrack</strong><a href="https://arxiv.org/pdf/1912.08531.pdf" target="_blank" rel="noopener">[1]</a> is a pure global tracker for <strong>long-term tracking</strong>, without temporal consistency assumption making cumulative errors. There are some details of reading and implementing it.</p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1912.08531.pdf" target="_blank" rel="noopener">GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</a>(AAAI 2020 paper)<br><strong>Code</strong>: <a href="https://github.com/huanglianghua/GlobalTrack" target="_blank" rel="noopener">Pytorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/GlobalTrack/Note" target="_blank" rel="noopener">GlobalTrack</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/01/09/9xEH8ec2YSyNQOb.png" alt="GlobalTrack_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly proposed a method called <strong>GlobalTrack</strong>, which is a pure global instance search based tracker that makes <code>no assumption</code> on the temporal consistency.   </p><blockquote><ol><li>It is developed based on two-stage object detector Faster R-CNN, with two submodules <code>QG-RPN</code> and <code>QG-RCNN</code>.  </li><li>it is able to perform <code>full-image</code> and <code>multi-scale</code> search of arbitrary instances with only a <strong>single query</strong> as the guide.  </li><li>They further propose a <code>cross-query</code> loss to improve the robustness of this approach against distractors. </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/01/09/FieyOYchuaRSqNK.png" alt="GlobalTrack_PD.png"> </p><blockquote><p>It shows the difficults of <code>long-term tracking</code> and the problem of <code>existing trackers</code>.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/01/09/7CuMdwjcrWtbKi3.png" alt="GlobalTrack_PS.png">  </p><blockquote><p>It shows the <strong>methods</strong> for solving long-term tracking problem.  </p></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/01/11/Vx6BRCIos7PmLzq.png" alt="GlobalTrack_Architecture.png"><br><img src="https://i.loli.net/2020/01/09/O2rsA5CqplZG3kX.png" alt="GlobalTrack_CU.png">  </p><blockquote><p>It describes the overall architecture of GlobalTrack with <code>QG-RPN</code> and <code>QG-RCNN</code>.</p></blockquote><h2 id="Details-of-implementation"><a href="#Details-of-implementation" class="headerlink" title="Details of implementation"></a>Details of implementation</h2><p><img src="https://i.loli.net/2020/01/09/xrBYwAnS3i8FzVL.png" alt="GlobalTrack_Implementation.png"> </p><blockquote><ol><li><strong>Offline Training</strong>: it samples <code>frame pairs</code> from training videos.  </li><li><strong>Online Tracking</strong>: it contains <code>initialization</code>, <code>tracking</code> and <code>results</code>.  </li><li><strong>Cross-query Loss</strong>: it choose <code>top-1 prediction</code> as result. </li></ol></blockquote><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="https://i.loli.net/2020/01/11/Bb6iJfIWFo8kEq2.png" alt="GlobalTrack_Arch.png"></p><blockquote><ol><li><strong>Query-Guide RPN</strong>: it generating query-specific proposals.  </li><li><strong>Query-Guide RCNN</strong>: it consists of feature modulation and traditional RCNN.  </li><li><strong>Tracking Results</strong>: it takes top-1 prediction as results.  </li></ol></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>They compared this approach GlobalTrack with state-of-the-art trackers on four large-scale tracking benchmarks as follows.<br><img src="https://i.loli.net/2020/01/11/jnrF62a1SDPTKXt.png" alt="GlobalTrack_Ex1.png"><br><img src="https://i.loli.net/2020/01/11/bpnHEVCB2fG8qvy.png" alt="GlobalTrack_Ex2.png"><br><img src="https://i.loli.net/2020/01/11/wP1H5Jg68XBfCoq.png" alt="GlobalTrack_Ex3.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in [GlobalTrack]<a href="https://github.com/huanglianghua/GlobalTrack" target="_blank" rel="noopener">[2]</a>. </p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2020/01/09/uz2587VQdgOPfsy.png" alt="GlobalTrack_Improvement.png"></p><blockquote><p>some free ideas that orienting <strong>future work</strong>. </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Huang, Lianghua, Xin Zhao, and Kaiqi Huang. “GlobalTrack: A Simple and Strong Baseline for Long-term Tracking.” arXiv preprint arXiv:1912.08531 (2019).<br>[2] GlobalTrack. <a href="https://github.com/huanglianghua/GlobalTrack" target="_blank" rel="noopener">https://github.com/huanglianghua/GlobalTrack</a>  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;GlobalTrack&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.08531.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is a pure global tracker for &lt;strong&gt;long-term tracking&lt;/strong&gt;, without temporal consistency assumption making cumulative errors. There are some details of reading and implementing it.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="GlobalTrack" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/GlobalTrack/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="VOT" scheme="https://gojay.top/tags/VOT/"/>
    
  </entry>
  
  <entry>
    <title>SiamMask</title>
    <link href="https://gojay.top/2019/11/26/SiamMask/"/>
    <id>https://gojay.top/2019/11/26/SiamMask/</id>
    <published>2019-11-26T02:31:16.000Z</published>
    <updated>2020-06-17T12:26:30.903Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>SiamMask</strong><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf" target="_blank" rel="noopener">[1]</a> is used to <strong>detect and segment objects</strong> from videos in each frame, <strong>initializing</strong> a single bounding box and <strong>outputing</strong> binary segmentation mask and rotated objects boxes. There are some details of reading and implementing it. </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a>(CVPR 2019 paper)<br><strong>Code</strong>: <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/SiamMask/Note" target="_blank" rel="noopener">SiamMask</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2019/11/27/GoOd5IRwKBQHZhT.png" alt="SiamMask_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly dubbed a method called <strong>SiamMask</strong>, which foucused on <strong>VOT</strong>(visual object tracking) and semi-supervised <strong>VOS</strong>(video object segmentation). It improved the <code>offline training</code> by augmenting loss with a binary segmentatin task.   </p><blockquote><ol><li>It solely relies on a single bounding box <strong>initialisation</strong> and <strong>produces</strong> class-agnostic object mask and rotated bounding boxes.  </li><li>It yield a solid evidence that SiamMask is a new <strong>state of the art</strong> among <strong>real-time</strong> trackers.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2019/11/27/rKdO8AiWwNCIzLH.png" alt="SiamMask_PD.png"> </p><blockquote><p>It shows the <strong>task</strong> of SiamMask focused on and the <strong>needs</strong> for tacking this problem.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2019/11/27/CaObXSwIFGtfEi6.png" alt="SiamMask_PS.png">  </p><blockquote><p>It shows improments on <strong>Initialisation</strong> and <strong>outputs</strong> for accuracy.  </p></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/01/03/xIu8fhOiVRpWDlc.png" alt="SiamMask_Schematic.png"><br><img src="https://i.loli.net/2019/11/27/YoH1sdPFzbQnc3N.png" alt="SiamMask_CU.png">  </p><blockquote><p>It describes the whole <strong>architecture</strong> of SiamMask with three brach and two branch, which adds <strong>mask branch</strong> to original siamese network.</p></blockquote><h2 id="Details-of-implementation"><a href="#Details-of-implementation" class="headerlink" title="Details of implementation"></a>Details of implementation</h2><p><img src="https://i.loli.net/2019/11/27/rinPNk4UtOSJ7Zf.png" alt="SiamMask_Implementation.png">   </p><blockquote><ol><li><strong>network architecture</strong>: it consists of <code>backbone</code>, <code>head</code> and <code>mask refinement module</code>.  </li><li><strong>training</strong>: it divides three parts to training respectively, including <code>FC</code>, <code>RPN</code> and <code>segmentation</code>.  </li><li><strong>inference</strong>: it evaluated once per frame with <code>max scores</code>.  </li></ol></blockquote><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>More details can be found in paper.</p><blockquote><ol><li><strong>backbone</strong>: it remains the first 4-th stage of <code>ResNet</code>, with adding <code>adjust layer</code> and depth-wise <code>cross-correlated</code>.<br><img src="https://i.loli.net/2020/01/03/xu1Um6jrt7NDByk.png" alt="SiamMask_Backbone.png"></li><li><strong>head</strong>: The <code>conv5</code> block in both variants contains a normalisation layer and ReLU non-linearity while <code>conv6</code> only consists of a 1×1 convolutional layer.<br><img src="https://i.loli.net/2020/01/03/qw1PkeFBaDiAfxK.png" alt="SiamMask_Head.png"></li><li><strong>refinement</strong>: It merges low and high resolution features using multi- ple refinement modules made of <code>upsampling layers</code> and <code>skip connections</code>.<br><img src="https://i.loli.net/2020/01/03/RvQENIfnAW4yFba.png" alt="SiamMask_Refinement.png"><br><img src="https://i.loli.net/2020/01/03/JuihvjsC4rFbnAo.png" alt="SiamMask_Example.png"></li></ol></blockquote><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>Ablation study shows the contributions for VOT.<br><img src="https://i.loli.net/2020/01/03/oRuWZHq9GcIrCEf.png" alt="SiamMask_VOT2016.png"><br>More experienment results shows below.<br><img src="https://i.loli.net/2020/01/03/qH3TDVCeyZGBj65.png" alt="SiamMask_Results.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in [SiamMask]<a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">[2]</a>. </p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2020/01/02/lupaxeJ4N7L5gXq.png" alt="SiamMask_Improvement.png"></p><blockquote><p>some free ideas that orienting <strong>future work</strong>.<br>More details of <strong>Understanding</strong> this work from author can be found in <a href="https://zhuanlan.zhihu.com/p/58154634" target="_blank" rel="noopener">[3]</a>. </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Wang, Qiang, et al. “Fast online object tracking and segmentation: A unifying approach.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br>[2] SiamMask. <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">https://github.com/foolwood/SiamMask</a><br>[3] Qiang Wang. “Thinking about SiamMask.” <a href="https://zhuanlan.zhihu.com/p/58154634" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58154634</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;SiamMask&lt;/strong&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is used to &lt;strong&gt;detect and segment objects&lt;/strong&gt; from videos in each frame, &lt;strong&gt;initializing&lt;/strong&gt; a single bounding box and &lt;strong&gt;outputing&lt;/strong&gt; binary segmentation mask and rotated objects boxes. There are some details of reading and implementing it. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="SiamMask" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/SiamMask/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="VOT" scheme="https://gojay.top/tags/VOT/"/>
    
  </entry>
  
  <entry>
    <title>Tracktor</title>
    <link href="https://gojay.top/2019/11/09/Tracktor/"/>
    <id>https://gojay.top/2019/11/09/Tracktor/</id>
    <published>2019-11-09T00:21:49.000Z</published>
    <updated>2020-06-17T12:26:30.905Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Tracktor</strong><a href="https://arxiv.org/abs/1903.05625" target="_blank" rel="noopener">[1]</a> is used to <strong>detect objects</strong> from videos in each frame, while <strong>forming tracks</strong> by linking corresponding detections across time. There are some details of reading and implementing it.  </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: [<a href="https://arxiv.org/pdf/1903.05625.pdf" target="_blank" rel="noopener">Tracking without bells and whistles</a>(ICCV 2019 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/tracking_wo_bnw" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/tracktor/Note" target="_blank" rel="noopener">tracking_wo_bnw</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2019/11/09/g1R4zUjKkCfbieo.png" alt="tracking_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly converted a detector into a <strong>Tracktor</strong>, which exploit the <strong>bounding box regression</strong> of an object detector to predict the position of an object in the next frame.  </p><blockquote><ol><li>It extended a straightforward <strong>re-identification</strong> and <strong>camera motion compensation</strong> to improving <code>identity preservation</code> across frames.  </li><li>It achieved <strong>state-of-the-art</strong> on tackling most of the <code>easy tracking scenarios</code>. Besides, it also got ideal effect in tackling <code>complex tracking scenarios</code>. Therefore, it point out promising future research <strong>directions</strong>.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2019/11/09/7C6jrXBRAn1dveg.png" alt="tracking_PD.png"> </p><blockquote><p>It shows the <strong>problem</strong> of multi-object tracking and <strong>exsiting solution</strong> for tacking this problem.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2019/11/09/XvVkJMzjNSDU2cf.png" alt="tracking_PS.png">  </p><blockquote><p>It intrudued a <strong>Tracktor</strong>, converted detector into a tracktor by exploit the <code>bounding box regression</code>, and then extended <code>Siamese network</code> and <code>motion model</code>.  </p></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2019/11/09/g92n4ZytoFVqMQJ.png" alt="tracking_CU.png"><br><img src="https://i.loli.net/2019/11/09/tHsJ9qa1IuE7SwP.png" alt="tracking_flow.png">  </p><blockquote><p>It describes the whole flow of Tracktor, including <strong>detector</strong> and <strong>two processing steps</strong>, that is <code>initialing new</code> tracks and <code>killing old</code> tracks. Then it explains <strong>each symbol</strong> of this process and how to deal with them. </p></blockquote><h2 id="Details-of-implementation"><a href="#Details-of-implementation" class="headerlink" title="Details of implementation"></a>Details of implementation</h2><p><img src="https://i.loli.net/2019/11/09/oYwuheT9bKmrtFH.png" alt="tracking_Doi.png">   </p><h3 id="tracking-multi-object"><a href="#tracking-multi-object" class="headerlink" title="tracking multi-object"></a>tracking multi-object</h3><blockquote><ol><li><strong>tracking step</strong>: it includes <code>detecting</code> object and <code>form tracks</code> by linking frames.  </li><li><strong>reID</strong>: it uses <code>Siamese network</code> to generate apearance feature to re-identify killed objects.  </li><li><strong>motion model</strong>: it contains problems of <code>large camera motion</code> and <code>low video frame rate</code>.  </li></ol></blockquote><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><blockquote><ol><li><strong>tracking</strong>: it choosed Faster <strong>R-CNN</strong> with ResNet-101 as backbone network, it also provided <strong>FPN</strong> and <code>other strategy</code>.  </li><li><strong>reID</strong>: it trained <code>TriNet</code> with ResNet-50 as backbone network, and <code>triplet loss</code> as loss function.  </li><li><strong>CMC and ECC</strong> to deal with large camera motion, <strong>CVA</strong> to tackle low video frame rate problem.  </li></ol></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in <a href="https://github.com/Gojay001/tracking_wo_bnw" target="_blank" rel="noopener">here</a> with citing tracking_wo_bnw<a href="https://github.com/phil-bergmann/tracking_wo_bnw" target="_blank" rel="noopener">[2]</a>.  </p></blockquote><h2 id="Tracktor-algorithm"><a href="#Tracktor-algorithm" class="headerlink" title="Tracktor algorithm"></a>Tracktor algorithm</h2><p><img src="https://i.loli.net/2019/11/09/zXQkF7Ve2W8bgoC.png" alt="tracking_algorithm.png"></p><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2019/11/09/hEpR1WycrkVTtH9.png" alt="tracking_Improvement.png"></p><blockquote><p>some methods to improve <strong>accuracy</strong> or accelerate <strong>speed</strong> can add into this program.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Bergmann, Philipp, Tim Meinhardt, and Laura Leal-Taixe. “Tracking without bells and whistles.” arXiv preprint arXiv:1903.05625 (2019).<br>[2] tracking_wo_bnw. <a href="https://github.com/phil-bergmann/tracking_wo_bnw" target="_blank" rel="noopener">https://github.com/phil-bergmann/tracking_wo_bnw</a>  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tracktor&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.05625&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is used to &lt;strong&gt;detect objects&lt;/strong&gt; from videos in each frame, while &lt;strong&gt;forming tracks&lt;/strong&gt; by linking corresponding detections across time. There are some details of reading and implementing it.  &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Tracking" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/"/>
    
      <category term="Tracktor" scheme="https://gojay.top/categories/DeepLearning/Object-Tracking/Tracktor/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Tracking" scheme="https://gojay.top/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://gojay.top/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>Faster R-CNN</title>
    <link href="https://gojay.top/2019/10/19/Faster-R-CNN/"/>
    <id>https://gojay.top/2019/10/19/Faster-R-CNN/</id>
    <published>2019-10-19T15:47:35.000Z</published>
    <updated>2020-08-19T08:14:08.079Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>Faster R-CNN</strong><a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">[1]</a> is used to detect objects in images, with outputing bounding box and class scores. There are some details of reading and implementing it.  </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>(NIPS 2015 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Detection/Faster%20R-CNN/Code/README.md" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Detection/Faster%20R-CNN/Note" target="_blank" rel="noopener">Faster R-CNN</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2019/10/22/FHvUPsjBV4IMxl9.png" alt="Faster-RCNN_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly proposed a method called <strong>Faster R-CNN</strong>, which introduced a Region Proposal Network (RPN) and further merge RPN and Fast R-CNN to detect objects.  </p><blockquote><ol><li>It introduced a <strong>RPN</strong>. An RPN is a fully convolutional network that simultaneously predicts object <code>bounds</code> and objectness <code>scores</code> at each position. Besides, RPN using the recently popular terminology of neural networks with <code>“attention” mechanisms</code> to generate proposals.  </li><li>It <strong>merged RPN</strong> and Fast R-CNN into a single network. The unified network detects objects by <code>sharing their convolutional features</code> enabling nearly <code>cost-free</code> region proposals.</li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2019/10/22/toubNvlhg57fPnQ.png" alt="Faster-RCNN_PD.png"> </p><blockquote><p>It shows the <strong>purpose</strong> of Faster R-CNN and <strong>exsiting methods</strong> about solving this problem.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2019/10/22/lmHGjeqgKiDESCd.png" alt="Faster-RCNN_PS.png">  </p><blockquote><p>It intrudued a network called <strong>RPN</strong>, including <code>how it works</code> and <code>what it roles</code>.  </p></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2019/10/22/wZkmTdo6t4jClxi.png" alt="Faster-RCNN_CU.png">  </p><blockquote><p>It describes the whole <strong>architecture</strong> of Faster R-CNN, including <code>how it works</code> and what <code>ouputs</code> in each mudules. </p></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2019/10/22/NidM78RsBGCXqWF.png" alt="Faster-RCNN_CC.png">  </p><blockquote><p>It denotes the <code>most important</code> conception of Faster R-CNN mudules, and it explains the <strong>Conv layers</strong> (conv + relu + pooling), <strong>RPN</strong> (feature maps -&gt; proposals), <strong>RoI pooling</strong> (feature maps + proposals -&gt; proposal feature maps), <strong>Classification</strong> (proposal feature maps -&gt; bbox + cls) respectively.  </p></blockquote><p>Besides, the <strong>network architecture</strong> shows below.<br><img src="https://i.loli.net/2019/10/22/BeWR2YPCEa3c5rz.png" alt="Faster-RCNN_NA.png"><br><img src="https://i.loli.net/2019/10/22/f6do2RgYuCnyzNA.jpg" alt="Faster-RCNN_Net.jpg"></p><h2 id="Details-of-implementation"><a href="#Details-of-implementation" class="headerlink" title="Details of implementation"></a>Details of implementation</h2><p><img src="https://i.loli.net/2019/10/22/R4xMUNBY896iSWX.png" alt="Faster-RCNN_Doi.png">   </p><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><blockquote><ol><li><strong>anchors</strong>: it seleted k(3*3) anchor boxes with outputing 2k scores and 4k coordinates.  </li><li><strong>classication + regression</strong>: it takes RPN outputs as inputs, generating positive anchors and bbox regression.  </li><li><strong>proposal layers</strong>: it contains pre_nms_topN, ignore cross-boundary, NMS, topN to generate proposals.  </li></ol></blockquote><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><blockquote><ol><li><strong>loss function</strong>: it considers classification loss and regression loss as loss function.  </li><li><strong>training</strong>: it choosed <code>alternating training</code>, that is to say, RPN -&gt; Fast R-CNN -&gt; RPN2 -&gt; unifiled network(RPN + Fast R-CNN).  </li></ol></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in <a href="https://github.com/Gojay001/faster-rcnn.pytorch" target="_blank" rel="noopener">here</a> with citing faster-rcnn.pytorch<a href="https://github.com/jwyang/faster-rcnn.pytorch" target="_blank" rel="noopener">[2]</a>.  </p></blockquote><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><blockquote><p>default datasets include <strong>PASCAL_VOC</strong> and <strong>COCO</strong> files. As my own data, it should transform to VOC or COCO format files.<br>The details of data format as follows.  </p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">`PASCAL_VOC`:</span><br><span class="line">|- VOCdevkit2007</span><br><span class="line">    |- VOC2007</span><br><span class="line">        |- Annotations</span><br><span class="line">            |- .xml</span><br><span class="line">        |- ImageSets</span><br><span class="line">            |- Main</span><br><span class="line">                |- trainval.txt</span><br><span class="line">                |- test.txt</span><br><span class="line">        |- JPEGImages</span><br><span class="line">            |- .jpg</span><br><span class="line">|- VOCdevkit2012</span><br><span class="line">    |- VOC2012</span><br><span class="line">        |- ...</span><br></pre></td></tr></table></figure><h2 id="Program-improvement"><a href="#Program-improvement" class="headerlink" title="Program improvement"></a>Program improvement</h2><blockquote><ol><li>Modified files to be compatible with my own <strong>machine</strong>.  </li><li>Changed custom <strong>datasets</strong> and classes to train.  </li></ol></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details of <strong>Faster R-CNN conception</strong> about anchors, loss and etc. can be found in <a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">[3]</a>.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Ren, Shaoqing, et al. “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems. 2015.<br>[2] faster-rcnn.pytorch. <a href="https://github.com/jwyang/faster-rcnn.pytorch" target="_blank" rel="noopener">https://github.com/jwyang/faster-rcnn.pytorch</a><br>[3] Shang Bai. “A paper understanding Faster R-CNN.” <a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31426458</a>.  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Faster R-CNN&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1506.01497.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is used to detect objects in images, with outputing bounding box and class scores. There are some details of reading and implementing it.  &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Object Detection" scheme="https://gojay.top/categories/DeepLearning/Object-Detection/"/>
    
      <category term="Faster R-CNN" scheme="https://gojay.top/categories/DeepLearning/Object-Detection/Faster-R-CNN/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Detection" scheme="https://gojay.top/tags/Detection/"/>
    
      <category term="Faster R-CNN" scheme="https://gojay.top/tags/Faster-R-CNN/"/>
    
  </entry>
  
  <entry>
    <title>ResNet</title>
    <link href="https://gojay.top/2019/09/08/ResNet/"/>
    <id>https://gojay.top/2019/09/08/ResNet/</id>
    <published>2019-09-08T04:45:35.000Z</published>
    <updated>2020-06-18T06:43:59.824Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>ResNet</strong><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">[1]</a> is used to classify images with deep residual learning. There are some details of reading and implementing it.</p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: [<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a>(CVPR 2016 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Note" target="_blank" rel="noopener">ResNet</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2020/02/03/W6Ot498efiFGwRP.png" alt="ResNet_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly present a residual learning framework named <strong>ResNet</strong>, which based on the residual <strong>building block</strong> for classification and detection.  </p><blockquote><ol><li>It reformulate the layers as <code>learning residual functions</code> with reference to the layer inputs.  </li><li>Residual networks are easier to <code>optimize</code>, and can gain <code>accuracy</code> from considerably increased depth.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2020/02/03/CRHZhpAyPid8Mju.png" alt="ResNet_PD.png"> </p><blockquote><p>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? </p></blockquote><p><img src="https://i.loli.net/2020/02/03/Kp7oy3PSfJrYI6k.png" alt="ResNet_Errors.png"></p><blockquote><p>It shows <code>higher error</code> with <code>deeper network</code>.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2020/02/03/TnPYl9WzCZJKHIG.png" alt="ResNet_PS.png"><br><img src="https://i.loli.net/2020/02/03/pOEIhJM4FS1A3sT.png" alt="ResNet_Residual.png">  </p><blockquote><ol><li>Intuitively, the residual learning needs <code>less to learn</code>, because the residual is generally smaller. Therefore, the learning difficulty is smaller.  </li><li>Mathematically speaking, the gradients will not be vanished due to the <code>shortcut connection</code>, that is why residual is easier to learn.</li></ol></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2020/02/03/8JrsHxhlRZBCMtq.png" alt="ResNet_CU.png"><br><img src="https://i.loli.net/2020/02/03/81rBlvAth5nw3GP.png" alt="ResNet_BuildingBlock.png"> </p><blockquote><ol><li>It presents two version of <strong>building block</strong> for ResNet, including <code>BasicBlock</code> and <code>Bottleneck</code>.    </li><li>It describes the choise about different dimensions, as to input and output with <code>same dimensions</code> or <code>dimensions increase</code>.  </li><li>It shows the function of <code>1x1</code> and <code>3x3</code> convolution layers works.  </li></ol></blockquote><h2 id="Core-Concept"><a href="#Core-Concept" class="headerlink" title="Core Concept"></a>Core Concept</h2><p><img src="https://i.loli.net/2020/02/03/IqQ7JZr5uBLktFD.png" alt="ResNet_Network.png"><br><img src="https://i.loli.net/2020/06/18/hzUBHQwcVE7RM2l.png" alt="ResNet_Design.png"><br><img src="https://i.loli.net/2020/06/18/mXtacPyw31of9Ts.png" alt="ResNet_RN.png"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><img src="https://i.loli.net/2020/02/03/ZaPFoCcN7xeOgip.png" alt="ResNet_ImageNet.png"><br><img src="https://i.loli.net/2020/02/03/yKv4EIaN6Spk2TM.png" alt="ResNet_Experiments.png"></p><blockquote><p>It includes image classification datasets ImageNet, CIFAR-10, and objection detection datasets PASCAL VOC, COCO.  </p></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><blockquote><p>The <strong>complete code</strong> can be found in [ResNet]<a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code" target="_blank" rel="noopener">[2]</a>.  </p></blockquote><h2 id="Details-of-implementation"><a href="#Details-of-implementation" class="headerlink" title="Details of implementation"></a>Details of implementation</h2><p><img src="https://i.loli.net/2020/02/03/aQcSEiLFmr96Vg2.png" alt="ResNet_Architectures.png"></p><h2 id="Convolution-layer"><a href="#Convolution-layer" class="headerlink" title="Convolution layer"></a>Convolution layer</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv3x3</span><span class="hljs-params">(in_planes, out_planes, stride=<span class="hljs-number">1</span>, groups=<span class="hljs-number">1</span>, dilation=<span class="hljs-number">1</span>)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""3x3 convolution with padding"""</span></span><br><span class="line">    <span class="hljs-keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="hljs-number">3</span>, stride=stride,</span><br><span class="line">                     padding=dilation, groups=groups, bias=<span class="hljs-literal">False</span>, dilation=dilation)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">conv1x1</span><span class="hljs-params">(in_planes, out_planes, stride=<span class="hljs-number">1</span>)</span>:</span></span><br><span class="line">    <span class="hljs-string">"""1x1 convolution"""</span></span><br><span class="line">    <span class="hljs-keyword">return</span> nn.Conv2d(in_planes, out_planes, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="Building-block"><a href="#Building-block" class="headerlink" title="Building block"></a>Building block</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BasicBlock</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=None, groups=<span class="hljs-number">1</span>,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">                 base_width=<span class="hljs-number">64</span>, dilation=<span class="hljs-number">1</span>, norm_layer=None)</span>:</span></span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        <span class="hljs-keyword">if</span> groups != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> base_width != <span class="hljs-number">64</span>:</span><br><span class="line">            <span class="hljs-keyword">raise</span> ValueError(</span><br><span class="line">                <span class="hljs-string">'BasicBlock only supports groups=1 and base_width=64'</span>)</span><br><span class="line">        <span class="hljs-keyword">if</span> dilation &gt; <span class="hljs-number">1</span>:</span><br><span class="line">            <span class="hljs-keyword">raise</span> NotImplementedError(</span><br><span class="line">                <span class="hljs-string">"Dilation &gt; 1 not supported in BasicBlock"</span>)</span><br><span class="line">        <span class="hljs-comment"># Both self.conv1 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = norm_layer(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = norm_layer(planes)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Bottleneck</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="hljs-number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, inplanes, planes, stride=<span class="hljs-number">1</span>, downsample=None, groups=<span class="hljs-number">1</span>,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">                 base_width=<span class="hljs-number">64</span>, dilation=<span class="hljs-number">1</span>, norm_layer=None)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        width = int(planes * (base_width / <span class="hljs-number">64.</span>)) * groups</span><br><span class="line">        <span class="hljs-comment"># Both self.conv2 and self.downsample layers downsample the input when stride != 1</span></span><br><span class="line">        self.conv1 = conv1x1(inplanes, width)</span><br><span class="line">        self.bn1 = norm_layer(width)</span><br><span class="line">        self.conv2 = conv3x3(width, width, stride, groups, dilation)</span><br><span class="line">        self.bn2 = norm_layer(width)</span><br><span class="line">        self.conv3 = conv1x1(width, planes * self.expansion)</span><br><span class="line">        self.bn3 = norm_layer(planes * self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        identity = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ResNet</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, block, layers, num_classes=<span class="hljs-number">1000</span>, zero_init_residual=False,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">                 groups=<span class="hljs-number">1</span>, width_per_group=<span class="hljs-number">64</span>, replace_stride_with_dilation=None,</span></span></span><br><span class="line"><span class="hljs-function"><span class="hljs-params">                 norm_layer=None)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            norm_layer = nn.BatchNorm2d</span><br><span class="line">        self._norm_layer = norm_layer</span><br><span class="line"></span><br><span class="line">        self.inplanes = <span class="hljs-number">64</span></span><br><span class="line">        self.dilation = <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span> replace_stride_with_dilation <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">            <span class="hljs-comment"># each element in the tuple indicates if we should replace</span></span><br><span class="line">            <span class="hljs-comment"># the 2x2 stride with a dilated convolution instead</span></span><br><span class="line">            replace_stride_with_dilation = [<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]</span><br><span class="line">        <span class="hljs-keyword">if</span> len(replace_stride_with_dilation) != <span class="hljs-number">3</span>:</span><br><span class="line">            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">"replace_stride_with_dilation should be None "</span></span><br><span class="line">                             <span class="hljs-string">"or a 3-element tuple, got &#123;&#125;"</span>.format(replace_stride_with_dilation))</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.base_width = width_per_group</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, self.inplanes, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>,</span><br><span class="line">                               bias=<span class="hljs-literal">False</span>)</span><br><span class="line">        self.bn1 = norm_layer(self.inplanes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="hljs-number">64</span>, layers[<span class="hljs-number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="hljs-number">128</span>, layers[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>,</span><br><span class="line">                                       dilate=replace_stride_with_dilation[<span class="hljs-number">0</span>])</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="hljs-number">256</span>, layers[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>,</span><br><span class="line">                                       dilate=replace_stride_with_dilation[<span class="hljs-number">1</span>])</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="hljs-number">512</span>, layers[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>,</span><br><span class="line">                                       dilate=replace_stride_with_dilation[<span class="hljs-number">2</span>])</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="hljs-number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():</span><br><span class="line">            <span class="hljs-keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(</span><br><span class="line">                    m.weight, mode=<span class="hljs-string">'fan_out'</span>, nonlinearity=<span class="hljs-string">'relu'</span>)</span><br><span class="line">            <span class="hljs-keyword">elif</span> isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="hljs-number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Zero-initialize the last BN in each residual branch,</span></span><br><span class="line">        <span class="hljs-comment"># so that the residual branch starts with zeros, and each residual block behaves like an identity.</span></span><br><span class="line">        <span class="hljs-comment"># This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677</span></span><br><span class="line">        <span class="hljs-keyword">if</span> zero_init_residual:</span><br><span class="line">            <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():</span><br><span class="line">                <span class="hljs-keyword">if</span> isinstance(m, Bottleneck):</span><br><span class="line">                    nn.init.constant_(m.bn3.weight, <span class="hljs-number">0</span>)</span><br><span class="line">                <span class="hljs-keyword">elif</span> isinstance(m, BasicBlock):</span><br><span class="line">                    nn.init.constant_(m.bn2.weight, <span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_make_layer</span><span class="hljs-params">(self, block, planes, blocks, stride=<span class="hljs-number">1</span>, dilate=False)</span>:</span></span><br><span class="line">        norm_layer = self._norm_layer</span><br><span class="line">        downsample = <span class="hljs-literal">None</span></span><br><span class="line">        previous_dilation = self.dilation</span><br><span class="line">        <span class="hljs-keyword">if</span> dilate:</span><br><span class="line">            self.dilation *= stride</span><br><span class="line">            stride = <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.inplanes != planes * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                conv1x1(self.inplanes, planes * block.expansion, stride),</span><br><span class="line">                norm_layer(planes * block.expansion),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,</span><br><span class="line">                            self.base_width, previous_dilation, norm_layer))</span><br><span class="line">        self.inplanes = planes * block.expansion</span><br><span class="line">        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, blocks):</span><br><span class="line">            layers.append(block(self.inplanes, planes, groups=self.groups,</span><br><span class="line">                                base_width=self.base_width, dilation=self.dilation,</span><br><span class="line">                                norm_layer=norm_layer))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = torch.flatten(x, <span class="hljs-number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="Pretrain"><a href="#Pretrain" class="headerlink" title="Pretrain"></a>Pretrain</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_resnet</span><span class="hljs-params">(arch, block, layers, pretrained, progress, **kwargs)</span>:</span></span><br><span class="line">    model = ResNet(block, layers, **kwargs)</span><br><span class="line">    <span class="hljs-keyword">if</span> pretrained:</span><br><span class="line">        state_dict = load_state_dict_from_url(model_urls[arch],</span><br><span class="line">                                              progress=progress)</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">    <span class="hljs-keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="Different-layer"><a href="#Different-layer" class="headerlink" title="Different layer"></a>Different layer</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet18</span><span class="hljs-params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">'resnet18'</span>, BasicBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet34</span><span class="hljs-params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">'resnet34'</span>, BasicBlock, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet50</span><span class="hljs-params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">'resnet50'</span>, Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>], pretrained, progress,</span><br><span class="line">                   **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet101</span><span class="hljs-params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">'resnet101'</span>, Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>], pretrained, progress, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">resnet152</span><span class="hljs-params">(pretrained=False, progress=True, **kwargs)</span>:</span></span><br><span class="line">    <span class="hljs-keyword">return</span> _resnet(<span class="hljs-string">'resnet152'</span>, Bottleneck, [<span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span>, <span class="hljs-number">3</span>], pretrained, progress, **kwargs)</span><br></pre></td></tr></table></figure><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details about code implementation can be found in <a href="https://blog.csdn.net/darkeyers/article/details/90475475" target="_blank" rel="noopener">[3]</a>, <a href="https://zhuanlan.zhihu.com/p/31852747" target="_blank" rel="noopener">[4]</a>.</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.<br>[2] ResNet. <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code" target="_blank" rel="noopener">https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code</a><br>[3] Darkeyers. ResNet implementaion by Pytorch official. <a href="https://blog.csdn.net/darkeyers/article/details/90475475" target="_blank" rel="noopener">https://blog.csdn.net/darkeyers/article/details/90475475</a><br>[4] Little teenager. CNN model you have to know: ResNet. <a href="https://zhuanlan.zhihu.com/p/31852747" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31852747</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;ResNet&lt;/strong&gt;&lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is used to classify images with deep residual learning. There are some details of reading and implementing it.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Classification" scheme="https://gojay.top/categories/DeepLearning/Classification/"/>
    
      <category term="ResNet" scheme="https://gojay.top/categories/DeepLearning/Classification/ResNet/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Classification" scheme="https://gojay.top/tags/Classification/"/>
    
      <category term="ResNet" scheme="https://gojay.top/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>GoogLeNet</title>
    <link href="https://gojay.top/2019/09/05/GoogLeNet/"/>
    <id>https://gojay.top/2019/09/05/GoogLeNet/</id>
    <published>2019-09-05T07:01:37.000Z</published>
    <updated>2020-06-17T12:26:30.895Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>GoogLeNet</strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="noopener">[1]</a> is used to classify images with inception v1. There are some details of reading and implementing it.  </p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Going deeper with convolutions</a>(CVPR 2015 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/GoogLeNet/Code" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/GoogLeNet/Note" target="_blank" rel="noopener">GoogLeNet</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2019/10/29/ztR8rPIYdhQajLb.png" alt="googlenet_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly proposed a CNN architecture codenamed <strong>Inception</strong>, so that to build a inception-based network with 22 layers called <strong>GoogLeNet</strong> for classification and detection.  </p><blockquote><ol><li>bulid <strong>Inception</strong> architecture based on the <code>Hebbian principle</code> and the intuition of <code>multi-scale</code> processing.  </li><li>It increased the depth and width of the network while keeping the <strong>computational budget constant</strong>.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2019/10/29/3fer8KJgdqv79oH.png" alt="googlenet_PD.png"> </p><blockquote><p>It shows the <strong>purpose</strong> of GoogLenet and the <strong>drawbacks</strong> of exsiting methods about solving this problem.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2019/10/29/p4gKhc2B7RtkAUX.png" alt="googlenet_PS.png">  </p><blockquote><p>It proposal a network architecture named <strong>Inception</strong>, including <code>what it can do</code> and <code>how it works</code>.  </p></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2019/10/29/gqm8RnSbXYweZ1o.png" alt="googlenet_CU.png">  </p><blockquote><p>It describes two version of <strong>architecture</strong> of Inception, including <code>naive version</code> and <code>inception_v1</code>.  </p></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2019/10/29/Q7uRCnAoJ6vam9M.png" alt="googlenet_CC.png">  </p><blockquote><p>It denotes the <code>most important</code> conception of Inception mudules, and it explains convolution on <strong>multiple scales</strong> to extract features, and using <strong>spare matrix</strong> to accelerate convergence speed with a <code>instance</code>.  </p></blockquote><p>Besides, the <strong>network architecture</strong> shows below.<br><img src="https://i.loli.net/2019/10/30/RPK2nCyZqXFz5ix.jpg" alt="googlenet_network.jpg"><br><img src="https://i.loli.net/2019/10/30/sCOkLz8ovmjT56U.png" alt="googlenet_incarnation.png"></p><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><blockquote><p>The <strong>complete code</strong> can be found in <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/GoogLeNet/Code" target="_blank" rel="noopener">here</a>.  </p></blockquote><h2 id="Details-of-implementation"><a href="#Details-of-implementation" class="headerlink" title="Details of implementation"></a>Details of implementation</h2><p>the whole network architecture:<br><img src="https://i.loli.net/2019/10/29/j5CPhTV9IdWztus.png" alt="googlenet_Doi.png"><br>the details of googlenet:<br><img src="https://i.loli.net/2019/10/29/kyvzUOxl2XduY9Q.jpg" alt="googlenet_details.jpg"><br>the step of implementation:<br><img src="https://i.loli.net/2019/10/29/owpufOnQLmaE8V6.png" alt="googlenet_code.png">  </p><blockquote><p>It includes the whole network <strong>architeture</strong> and the implementation of <strong>auxiliary classfier</strong>.  </p></blockquote><h2 id="Inception"><a href="#Inception" class="headerlink" title="Inception"></a>Inception</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BasicConv2d</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channels, out_channels, **kwargs)</span>:</span></span><br><span class="line">        super(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, bias=<span class="hljs-literal">False</span>, **kwargs)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_channels, eps=<span class="hljs-number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        <span class="hljs-keyword">return</span> F.relu(x, inplace=<span class="hljs-literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inception</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj)</span>:</span></span><br><span class="line">        super(Inception, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="hljs-number">1</span>),</span><br><span class="line">            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="hljs-number">1</span>),</span><br><span class="line">            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, ceil_mode=<span class="hljs-literal">True</span>),</span><br><span class="line">            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        branch1 = self.branch1(x)</span><br><span class="line">        branch2 = self.branch2(x)</span><br><span class="line">        branch3 = self.branch3(x)</span><br><span class="line">        branch4 = self.branch4(x)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="hljs-keyword">return</span> torch.cat(outputs, <span class="hljs-number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="Auxiliary-classifer"><a href="#Auxiliary-classifer" class="headerlink" title="Auxiliary classifer"></a>Auxiliary classifer</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">InceptionAux</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channels, num_classes)</span>:</span></span><br><span class="line">        super(InceptionAux, self).__init__()</span><br><span class="line">        self.conv = BasicConv2d(in_channels, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">1024</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="hljs-number">1024</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-comment"># aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, (<span class="hljs-number">4</span>, <span class="hljs-number">4</span>))</span><br><span class="line">        <span class="hljs-comment"># aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="hljs-comment"># N x 128 x 4 x 4</span></span><br><span class="line">        x = torch.flatten(x, <span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-comment"># N x 2048</span></span><br><span class="line">        x = F.relu(self.fc1(x), inplace=<span class="hljs-literal">True</span>)</span><br><span class="line">        <span class="hljs-comment"># N x 2048</span></span><br><span class="line">        x = F.dropout(x, <span class="hljs-number">0.7</span>, training=self.training)</span><br><span class="line">        <span class="hljs-comment"># N x 2048</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="hljs-comment"># N x 1024</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GoogLeNet</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, num_classes=<span class="hljs-number">1000</span>, aux_logits=True, transform_input=False, init_weights=True)</span>:</span></span><br><span class="line">        super(GoogLeNet, self).__init__()</span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line">        self.transform_input = transform_input</span><br><span class="line"></span><br><span class="line">        self.conv1 = BasicConv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)</span><br><span class="line">        self.conv2 = BasicConv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>)</span><br><span class="line">        self.conv3 = BasicConv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception3a = Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)</span><br><span class="line">        self.inception3b = Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception4a = Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>, <span class="hljs-number">48</span>, <span class="hljs-number">64</span>)</span><br><span class="line">        self.inception4b = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)</span><br><span class="line">        self.inception4c = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)</span><br><span class="line">        self.inception4d = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)</span><br><span class="line">        self.inception4e = Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.inception5a = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)</span><br><span class="line">        self.inception5b = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> aux_logits:</span><br><span class="line">            self.aux1 = InceptionAux(<span class="hljs-number">512</span>, num_classes)</span><br><span class="line">            self.aux2 = InceptionAux(<span class="hljs-number">528</span>, num_classes)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))</span><br><span class="line">        self.dropout = nn.Dropout(<span class="hljs-number">0.2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="hljs-number">1024</span>, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> init_weights:</span><br><span class="line">            self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_initialize_weights</span><span class="hljs-params">(self)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():</span><br><span class="line">            <span class="hljs-keyword">if</span> isinstance(m, nn.Conv2d) <span class="hljs-keyword">or</span> isinstance(m, nn.Linear):</span><br><span class="line">                <span class="hljs-keyword">import</span> scipy.stats <span class="hljs-keyword">as</span> stats</span><br><span class="line">                X = stats.truncnorm(<span class="hljs-number">-2</span>, <span class="hljs-number">2</span>, scale=<span class="hljs-number">0.01</span>)</span><br><span class="line">                values = torch.as_tensor(</span><br><span class="line">                    X.rvs(m.weight.numel()), dtype=m.weight.dtype)</span><br><span class="line">                values = values.view(m.weight.size())</span><br><span class="line">                <span class="hljs-keyword">with</span> torch.no_grad():</span><br><span class="line">                    m.weight.copy_(values)</span><br><span class="line">            <span class="hljs-keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="hljs-number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        <span class="hljs-keyword">if</span> self.transform_input:</span><br><span class="line">            x_ch0 = torch.unsqueeze(x[:, <span class="hljs-number">0</span>], <span class="hljs-number">1</span>) * \</span><br><span class="line">                (<span class="hljs-number">0.229</span> / <span class="hljs-number">0.5</span>) + (<span class="hljs-number">0.485</span> - <span class="hljs-number">0.5</span>) / <span class="hljs-number">0.5</span></span><br><span class="line">            x_ch1 = torch.unsqueeze(x[:, <span class="hljs-number">1</span>], <span class="hljs-number">1</span>) * \</span><br><span class="line">                (<span class="hljs-number">0.224</span> / <span class="hljs-number">0.5</span>) + (<span class="hljs-number">0.456</span> - <span class="hljs-number">0.5</span>) / <span class="hljs-number">0.5</span></span><br><span class="line">            x_ch2 = torch.unsqueeze(x[:, <span class="hljs-number">2</span>], <span class="hljs-number">1</span>) * \</span><br><span class="line">                (<span class="hljs-number">0.225</span> / <span class="hljs-number">0.5</span>) + (<span class="hljs-number">0.406</span> - <span class="hljs-number">0.5</span>) / <span class="hljs-number">0.5</span></span><br><span class="line">            x = torch.cat((x_ch0, x_ch1, x_ch2), <span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># N x 3 x 224 x 224</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="hljs-comment"># N x 64 x 112 x 112</span></span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        <span class="hljs-comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        <span class="hljs-comment"># N x 64 x 56 x 56</span></span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        <span class="hljs-comment"># N x 192 x 56 x 56</span></span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># N x 192 x 28 x 28</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        <span class="hljs-comment"># N x 256 x 28 x 28</span></span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        <span class="hljs-comment"># N x 480 x 28 x 28</span></span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        <span class="hljs-comment"># N x 480 x 14 x 14</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        <span class="hljs-comment"># N x 512 x 14 x 14</span></span><br><span class="line">        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.aux_logits:</span><br><span class="line">            aux1 = self.aux1(x)</span><br><span class="line"></span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        <span class="hljs-comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        <span class="hljs-comment"># N x 512 x 14 x 14</span></span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        <span class="hljs-comment"># N x 528 x 14 x 14</span></span><br><span class="line">        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.aux_logits:</span><br><span class="line">            aux2 = self.aux2(x)</span><br><span class="line"></span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        <span class="hljs-comment"># N x 832 x 14 x 14</span></span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        <span class="hljs-comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        <span class="hljs-comment"># N x 832 x 7 x 7</span></span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        <span class="hljs-comment"># N x 1024 x 7 x 7</span></span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        <span class="hljs-comment"># N x 1024 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-comment"># N x 1024</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="hljs-comment"># N x 1000 (num_classes)</span></span><br><span class="line">        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.aux_logits:</span><br><span class="line">            <span class="hljs-keyword">return</span> _GoogLeNetOutputs(x, aux2, aux1)</span><br><span class="line">        <span class="hljs-keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details of <strong>Inception</strong> about implementation can be found in <a href="https://my.oschina.net/u/876354/blog/1637819" target="_blank" rel="noopener">[2]</a>.<br>More details of conception about <strong>multi-scale</strong> and <strong>spare matrix</strong> can be found in <a href="https://zhuanlan.zhihu.com/p/32702031" target="_blank" rel="noopener">[3]</a>.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Szegedy, Christian, et al. “Going deeper with convolutions.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.<br>[2] Bing Xue. “Big word about CNN classic model.” <a href="https://my.oschina.net/u/876354/blog/1637819" target="_blank" rel="noopener">https://my.oschina.net/u/876354/blog/1637819</a>.<br>[3] Lei Zhang. “Deep understanding GoogLeNet architecture.” <a href="https://zhuanlan.zhihu.com/p/32702031" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32702031</a>.  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;GoogLeNet&lt;/strong&gt;&lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1]&lt;/a&gt; is used to classify images with inception v1. There are some details of reading and implementing it.  &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Classification" scheme="https://gojay.top/categories/DeepLearning/Classification/"/>
    
      <category term="GoogLeNet" scheme="https://gojay.top/categories/DeepLearning/Classification/GoogLeNet/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Classification" scheme="https://gojay.top/tags/Classification/"/>
    
      <category term="GoogLeNet" scheme="https://gojay.top/tags/GoogLeNet/"/>
    
      <category term="Inception" scheme="https://gojay.top/tags/Inception/"/>
    
  </entry>
  
  <entry>
    <title>NIN(Network In Network)</title>
    <link href="https://gojay.top/2019/08/31/NIN-Network-In-Network/"/>
    <id>https://gojay.top/2019/08/31/NIN-Network-In-Network/</id>
    <published>2019-08-31T09:42:39.000Z</published>
    <updated>2020-06-17T12:26:30.900Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>There are some details of reading and implementing the <strong>Network In Network</strong> for image classification.</p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#Note">Note</a></strong></li><li><strong><a href="#References">References</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network In Network</a>(arXiv 2013 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/NIN/Code" target="_blank" rel="noopener">PyTorch</a><br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/NIN/Note" target="_blank" rel="noopener">NIN</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2019/08/31/TahN9BAIMoEcbQm.png" alt="NIN_Abstract.png"><br>As <code>abstract</code> of the paper, their work mainly build a “micro network” called <strong>Network In Network</strong> (NIN) to replace traditional Convolutional Nerual Network(CNNs) and utilize <strong>global average pooling</strong> (GAP) instead of fully-connected layer(FC) to classify images.  </p><blockquote><ol><li>build <code>micro neural networks</code> with more complex structures to abstract the data within the receptive field.  </li><li>utilize <code>global average pooling</code> over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2019/08/31/uBxZzFv6HVGILJs.png" alt="NIN_PD.png"></p><blockquote><p>It shows the <strong>steps</strong> of classifing images as well as describes the traditional and state-of-the-art <strong>methods</strong>.  </p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2019/08/31/LKDNC5P2RIQAvUt.png" alt="NIN_PS.png"></p><blockquote><p>It includes MLP (<strong>multilayer perceotrn</strong>) layer and GAP (<strong>global average pooling</strong>).  </p></blockquote><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2019/08/31/yKJ7Yl9X1kfxNEt.png" alt="NIN_CU.png"></p><blockquote><p>It describes how does <strong>mlpconv</strong> works and what does <strong>GAP</strong> means.  </p></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2019/08/31/Lp8EucQwfoAOr2q.png" alt="NIN_CC.png"></p><blockquote><p>It denotes the <code>most important</code> conception of <strong>Network In Network</strong> (NIN) and explains the steps of traditional <strong>CNNs</strong> and novel <strong>NIN</strong> to classify images respectively.  </p></blockquote><p>Besides, the <strong>comparison</strong> shows below.<br><img src="https://i.loli.net/2019/08/31/JnUoASWKOIEc5db.png" alt="NIN_Comparison.png"></p><h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p><strong>datasets</strong>:  </p><ul><li>CIFAR-10</li><li>CIFAR-100</li><li>SVHN</li><li>MNIST</li></ul><blockquote><p><code>notes</code>:  </p><ol><li>fine-tuned local receptive field <strong>size</strong> and <strong>weight</strong> decay.  </li><li>using <strong>dropout</strong>.</li></ol></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><h2 id="Model-Detail"><a href="#Model-Detail" class="headerlink" title="Model Detail"></a>Model Detail</h2><p><img src="https://i.loli.net/2019/09/01/oR9xLV8Jr7EC6lc.png" alt="NIN_NA.png"></p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">NIN(</span><br><span class="line">  (features1): Sequential(</span><br><span class="line">    (0): Conv2d(3, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Conv2d(192, 160, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (5): ReLU()</span><br><span class="line">    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">    (7): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (features2): Sequential(</span><br><span class="line">    (0): Conv2d(96, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (5): ReLU()</span><br><span class="line">    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</span><br><span class="line">    (7): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (features3): Sequential(</span><br><span class="line">    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">    (1): ReLU()</span><br><span class="line">    (2): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (3): ReLU()</span><br><span class="line">    (4): Conv2d(192, 10, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    (5): ReLU()</span><br><span class="line">  )</span><br><span class="line">  (gap): AvgPool2d(kernel_size=8, stride=1, padding=0)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="PyTorch-Implementation"><a href="#PyTorch-Implementation" class="headerlink" title="PyTorch Implementation"></a>PyTorch Implementation</h2><blockquote><p>The <strong>complete code</strong> can be found in <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/NIN/Code" target="_blank" rel="noopener">here</a>.<br>The implementation of <strong>network</strong> as follows.  </p></blockquote><figure class="highlight py hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NIN</span><span class="hljs-params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">10</span>)</span>:</span></span><br><span class="line">        super(NIN, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.features1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">160</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">160</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),</span><br><span class="line">            nn.Dropout(<span class="hljs-number">0.5</span>)</span><br><span class="line">        )</span><br><span class="line">        self.features2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>),</span><br><span class="line">            nn.Dropout(<span class="hljs-number">0.5</span>)</span><br><span class="line">        )</span><br><span class="line">        self.features3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">192</span>, out_channels, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.gap = nn.AvgPool2d(kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span></span><br><span class="line">        x = self.features1(x)</span><br><span class="line">        x = self.features2(x)</span><br><span class="line">        x = self.features3(x)</span><br><span class="line">        x = self.gap(x)</span><br><span class="line">        x = x.view(x.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="TensorFlow-Implementation"><a href="#TensorFlow-Implementation" class="headerlink" title="TensorFlow Implementation"></a>TensorFlow Implementation</h2><blockquote><p>the code and more details can be found in <a href="https://embedai.wordpress.com/2017/07/23/network-in-network-implementation-using-tensorflow/" target="_blank" rel="noopener">[2]</a>.</p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><blockquote><p>More details of <strong>mlpconv</strong> and <strong>cccp</strong> can be found in <a href="https://blog.csdn.net/mounty_fsc/article/details/51746111" target="_blank" rel="noopener">[3]</a>.<br>What the effect of <strong>1 by 1 convolution kernel</strong> can be found in <a href="http://www.caffecn.cn/?/question/136" target="_blank" rel="noopener">[4]</a>.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Min Lin, Qiang Chen, and Shuicheng Yan. “Network in network.” arXiv preprint arXiv:1312.4400 (2013).<br>[2] Eugene. “NETWORK-IN-NETWORK IMPLEMENTATION USING TENSORFLOW.” <a href="https://embedai.wordpress.com/2017/07/23/network-in-network-implementation-using-tensorflow/" target="_blank" rel="noopener">https://embedai.wordpress.com/2017/07/23/network-in-network-implementation-using-tensorflow/</a><br>[3] Ou. “(Paper) Network Analysis of Network In Network.”  <a href="https://blog.csdn.net/mounty_fsc/article/details/51746111" target="_blank" rel="noopener">https://blog.csdn.net/mounty_fsc/article/details/51746111</a><br>[4] ysu. “What is the role of 1 by 1 convolution kernel?” <a href="http://www.caffecn.cn/?/question/136" target="_blank" rel="noopener">http://www.caffecn.cn/?/question/136</a>  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;There are some details of reading and implementing the &lt;strong&gt;Network In Network&lt;/strong&gt; for image classification.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Classification" scheme="https://gojay.top/categories/DeepLearning/Classification/"/>
    
      <category term="NIN" scheme="https://gojay.top/categories/DeepLearning/Classification/NIN/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Classification" scheme="https://gojay.top/tags/Classification/"/>
    
      <category term="NIN" scheme="https://gojay.top/tags/NIN/"/>
    
  </entry>
  
  <entry>
    <title>Classification Review</title>
    <link href="https://gojay.top/2019/08/25/Classification-Review/"/>
    <id>https://gojay.top/2019/08/25/Classification-Review/</id>
    <published>2019-08-25T08:27:27.000Z</published>
    <updated>2020-06-17T12:26:30.892Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>There are the list of typical Image Classification CNNs.</p></blockquote><a id="more"></a><h1 id="Todos"><a href="#Todos" class="headerlink" title="Todos"></a>Todos</h1><ul><li><input disabled type="checkbox"> <strong>LeNet-5</strong>: [ , 1, 32, 32] - [6, 16,  120, 84, 10]</li><li><input disabled type="checkbox"> <strong>AlexNet</strong> v1/v2: [ , 3, 224, 224] - [64, 192, 384, 256, 256,  4096, 4096, 1000]</li><li><input disabled type="checkbox"> <strong>VGG</strong> 11/13/16/19: [, 3, 224, 224] - [64,64, 128,128, 256…, 512…, 512…,  4096, 4096, 1000]</li><li><input checked disabled type="checkbox"> <strong>NIN</strong>: [MLP + AVG] instead of [CNNs + FC]</li><li><input checked disabled type="checkbox"> <strong>ResNet</strong> 18/34/50/101/152: [ , 3, 224, 224] - [64, 64<em>3, 128</em>4, 256<em>6, 512</em>3, 1000] – (1,3,1)</li><li><input checked disabled type="checkbox"> <strong>GoogLeNet</strong></li><li><input disabled type="checkbox"> <strong>MobileNet</strong></li><li><input disabled type="checkbox"> <strong>DenseNet</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;There are the list of typical Image Classification CNNs.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Classification" scheme="https://gojay.top/categories/DeepLearning/Classification/"/>
    
      <category term="Review" scheme="https://gojay.top/categories/DeepLearning/Classification/Review/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="Classification" scheme="https://gojay.top/tags/Classification/"/>
    
      <category term="Review" scheme="https://gojay.top/tags/Review/"/>
    
  </entry>
  
  <entry>
    <title>RN(Relation Network)</title>
    <link href="https://gojay.top/2019/08/21/RN-Realation-Network/"/>
    <id>https://gojay.top/2019/08/21/RN-Realation-Network/</id>
    <published>2019-08-21T07:51:05.000Z</published>
    <updated>2020-06-17T12:26:30.901Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>There are some details of reading and implementing the <strong>Relation Network</strong> for few-shot learning.</p></blockquote><a id="more"></a><h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr><ul><li><strong><a href="#Paper&Code&note">Paper &amp; Code &amp; note</a></strong></li><li><strong><a href="#Paper">Paper understanding</a></strong></li><li><strong><a href="#Code">Code understanding</a></strong></li><li><strong><a href="#References">References</a></strong></li><li><strong><a href="#Note">Note</a></strong></li></ul><h1 id="Paper-amp-Code-amp-note"><a href="#Paper-amp-Code-amp-note" class="headerlink" title="Paper &amp; Code &amp; note"></a>Paper &amp; Code &amp; note</h1><hr><blockquote><p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1711.06025" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning</a>(CVPR 2018 paper)<br><strong>Code</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Few-Shot%20Learning/RN/Code/README.md" target="_blank" rel="noopener">PyTorch</a>(Few-Shot Learning part)<br><strong>Note</strong>: <a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Few-Shot%20Learning/RN/Note" target="_blank" rel="noopener">RN for FSL</a></p></blockquote><h1 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h1><hr><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><img src="https://i.loli.net/2019/08/24/rmHGwBoZKsxAT2U.png" alt="RN_abstract.png"><br>As <code>abstract</code> of the paper, their work mainly proposed a method called <strong>Realation Network</strong> (RN) to recognise new classes given only few examples from each.  </p><blockquote><ol><li>It based on <strong>meta-learning</strong>. That is to say, the RN learns a <code>deep distance metric</code> to compare a number of images with episodes, and it is a <code>episode-based</code> method.  </li><li>It classify images of new classes by computing <strong>relation scores</strong>. That is to say, there is a score in each <code>query</code> image with their relations of <code>sample</code> images in each class.  </li></ol></blockquote><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p><img src="https://i.loli.net/2019/08/24/FkOWoMGP7dmp2T4.png" alt="RN_PD.png"> </p><blockquote><p>It shows the task of <strong>few-shot learning</strong> and the <strong>exists model</strong>.</p></blockquote><h2 id="Problem-Solution"><a href="#Problem-Solution" class="headerlink" title="Problem Solution"></a>Problem Solution</h2><p><img src="https://i.loli.net/2019/08/24/96ju1zHtcdQxVaA.png" alt="RN_PS.png">  </p><blockquote><p>It includes <strong>Embedding</strong> module and <strong>Relation</strong> module of the RN.  </p></blockquote><p>References: [36, 39], RNNs: [39, 32, 29], Fine-tuning: [29, 10].</p><h2 id="Conceptual-Understanding"><a href="#Conceptual-Understanding" class="headerlink" title="Conceptual Understanding"></a>Conceptual Understanding</h2><p><img src="https://i.loli.net/2019/08/24/2kxu5Hjbh9fJdyc.png" alt="RN_CU.png">  </p><blockquote><p>It describes what is <strong>meta-learning</strong> and how to <strong>classify</strong> query images. </p></blockquote><h2 id="Remaining-Problem"><a href="#Remaining-Problem" class="headerlink" title="Remaining Problem"></a>Remaining Problem</h2><p><img src="https://i.loli.net/2019/08/24/rQ2CDGwYA6cus8M.png" alt="RN_RP.png">  </p><blockquote><p>It is the <strong>question</strong> in my mind in terms of the paper and the code.  </p></blockquote><h2 id="Core-Conception"><a href="#Core-Conception" class="headerlink" title="Core Conception"></a>Core Conception</h2><p><img src="https://i.loli.net/2019/08/24/q1TDicJkyjeQUaL.png" alt="RN_CC.png">  </p><blockquote><p>It denotes the <code>most important</code> conception of <strong>Relation Network</strong> (RN) and explains the <strong>Embedding</strong> module and <strong>Relation</strong> module respectively.  </p></blockquote><p>Besides, the <strong>network architecture</strong> shows below.<br><img src="https://i.loli.net/2019/08/24/ERIMHV2C4T9XqzZ.png" alt="RN_RN.png"><br><img src="https://i.loli.net/2019/08/24/QsDW4IrZcgEvAUL.png" alt="RN_NA.png"></p><h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p><img src="https://i.loli.net/2019/08/24/Q5Wa2UtLKBgNueZ.png" alt="RN_Omniglot.png"><br><img src="https://i.loli.net/2019/08/24/xevhjHSwaf3Dbd7.png" alt="RN_miniImagenet.png">  </p><blockquote><p>There are results of carring RN on <strong>Omniglot</strong> and <strong>miniImagenet</strong> datasets in paper, which shows that RN got better performance when comparing with other state-of-the-art methods.</p></blockquote><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><hr><h2 id="Program-block"><a href="#Program-block" class="headerlink" title="Program block"></a>Program block</h2><p><img src="https://i.loli.net/2019/08/25/zc8sGRqvEoiI61J.png" alt="RN_PB.png"> </p><blockquote><p>It divides program files to three blocks, which are <strong>pre-process</strong>, <strong>train</strong> and <strong>test</strong> as well as the <strong>function</strong> list of the blocks.    </p></blockquote><h2 id="Program-explanation"><a href="#Program-explanation" class="headerlink" title="Program explanation"></a>Program explanation</h2><p><img src="https://i.loli.net/2019/08/25/pIaAiQFvR5BT8SZ.png" alt="RN_PE.png">  </p><blockquote><p>It explains the details of the <strong>code</strong> blocks in each process.  </p></blockquote><h2 id="Program-improvement"><a href="#Program-improvement" class="headerlink" title="Program improvement"></a>Program improvement</h2><p><img src="https://i.loli.net/2019/08/25/BdbMWplwnDFQjZY.png" alt="RN_PI.png">  </p><blockquote><p>Main work in my improved code are tackling <strong>problems</strong> and <strong>optimizing</strong> functions as well as train and test my personnal <strong>datasets</strong>.  </p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><hr><blockquote><p>[1] Sung F, Yang Y, Zhang L, et al. Learning to compare: Relation network for few-shot learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1199-1208.<br>[2] LearningToCompare_FSL. <a href="https://github.com/floodsung/LearningToCompare_FSL" target="_blank" rel="noopener">https://github.com/floodsung/LearningToCompare_FSL</a>.<br>[3] Pytorch. <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">https://github.com/pytorch/pytorch</a>.  </p></blockquote><h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><hr><p><img src="https://i.loli.net/2019/08/25/f71tFaJ6oyYwAHe.png" alt="RN_note.png">  </p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;There are some details of reading and implementing the &lt;strong&gt;Relation Network&lt;/strong&gt; for few-shot learning.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="https://gojay.top/categories/DeepLearning/"/>
    
      <category term="Few-Shot Learning" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Learning/"/>
    
      <category term="RN" scheme="https://gojay.top/categories/DeepLearning/Few-Shot-Learning/RN/"/>
    
    
      <category term="DL" scheme="https://gojay.top/tags/DL/"/>
    
      <category term="FSL" scheme="https://gojay.top/tags/FSL/"/>
    
      <category term="RN" scheme="https://gojay.top/tags/RN/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Blog+Pages</title>
    <link href="https://gojay.top/2019/08/07/Hexo-Blog-Pages/"/>
    <id>https://gojay.top/2019/08/07/Hexo-Blog-Pages/</id>
    <published>2019-08-06T16:21:43.000Z</published>
    <updated>2020-06-18T06:53:53.653Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This blog records the details about <strong>building hexo blogs</strong>, as well as deploying it to Github and Coding <strong>Pages</strong>.  </p></blockquote><a id="more"></a><h1 id="Basic-Installation"><a href="#Basic-Installation" class="headerlink" title="Basic Installation"></a>Basic Installation</h1><hr><h2 id="Install-Node-js-and-Git"><a href="#Install-Node-js-and-Git" class="headerlink" title="Install Node.js and Git"></a>Install <strong>Node.js</strong> and <strong>Git</strong></h2><h3 id="install-Node-js"><a href="#install-Node-js" class="headerlink" title="install Node.js"></a>install <strong>Node.js</strong></h3><blockquote><p>Just go to the <a href="https://nodejs.org/zh-cn/download/" target="_blank" rel="noopener">website</a> and you can <code>download</code> it. Then, it will be installed at <em>/usr/local/bin</em> .<br>After that, you can <code>validate</code> it as follows:  </p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">$ node -v</span><br><span class="line">v10.16.0</span><br><span class="line">$ npm -v</span><br><span class="line">6.9.0</span><br></pre></td></tr></table></figure><h3 id="install-Git"><a href="#install-Git" class="headerlink" title="install Git"></a>install <strong>Git</strong></h3><p>Also, you can just <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">download</a> and <code>check</code> your git version as follows:  </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">$ Git --version</span><br><span class="line">git version 2.20.1</span><br></pre></td></tr></table></figure><h2 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a>Install <strong>Hexo</strong></h2><p>Now, we can install the Hexo. Note that add <code>sudo</code> to solve the problem of permission. <code>-g</code> refers global install.</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">$ sudo npm install -g hexo-cli</span><br></pre></td></tr></table></figure><blockquote><p>I have met some <code>problems</code> in this way, and through another way <code>solved</code> it: </p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">sudo npm install -g hexo-cli --unsafe-perm</span><br></pre></td></tr></table></figure><h2 id="Initialize-Hexo"><a href="#Initialize-Hexo" class="headerlink" title="Initialize Hexo"></a><strong>Initialize</strong> Hexo</h2><ol><li>Create a folder(e.g. <code>blog</code> ) and <code>cd</code> into it.</li><li><code>Initialize</code> blog and download a series of files.</li><li>Install <code>npm</code> .</li><li>hexo <code>generate</code> and <code>server</code>.</li></ol><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">$ mkdir blog</span><br><span class="line">$ cd blog</span><br><span class="line">$ sudo hexo init</span><br><span class="line">$ sudo npm install</span><br><span class="line">$ hexo g</span><br><span class="line">$ hexo s</span><br></pre></td></tr></table></figure><h1 id="Install-Themes"><a href="#Install-Themes" class="headerlink" title="Install Themes"></a>Install <strong>Themes</strong></h1><hr><blockquote><p>Note that there are more than one <code>_config.yml</code> in the blog file.  </p><ul><li>one is in <code>root directory</code> .  </li><li>and every <code>themes directory</code> also have one.  </li></ul></blockquote><h2 id="Choose-themes"><a href="#Choose-themes" class="headerlink" title="Choose themes"></a>Choose themes</h2><p>It is possible for us to choose our own themes by the <a href="https://hexo.io/themes/" target="_blank" rel="noopener">website</a>.<br>For me, what I like is the <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener"> hexo-theme-icarus</a>.  </p><h2 id="Download-themes"><a href="#Download-themes" class="headerlink" title="Download themes"></a>Download themes</h2><p>Now, what we need to do is download it as follows:  </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">$ cd blog</span><br><span class="line">$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus</span><br></pre></td></tr></table></figure><h2 id="Change-themes"><a href="#Change-themes" class="headerlink" title="Change themes"></a>Change themes</h2><p>Then, for the <code>_config.yml</code> in <code>root directory</code> : Changing <code>theme: landscape</code> to <code>theme: yilia</code> .</p><h2 id="Reload-hexo"><a href="#Reload-hexo" class="headerlink" title="Reload hexo"></a>Reload hexo</h2><p>Last, just reload hexo <code>generate</code> and <code>server</code> .</p><h1 id="Customize-themes"><a href="#Customize-themes" class="headerlink" title="Customize themes"></a><strong>Customize</strong> themes</h1><hr><blockquote><p>There are a lot of ways to define your own functions. For me, a simple way is finding a suitalbe theme and just download it. Besides, I visited all the <code>issues</code> and <code>documents</code> of offical github.<br>The following is <strong>core functions</strong>. </p></blockquote><h2 id="“-config-yml”"><a href="#“-config-yml”" class="headerlink" title="“_config.yml”"></a>“_config.yml”</h2><p>This file in <code>root directory</code> represents the <code>global hexo settings</code> , and in themes directory just configure one theme.</p><h3 id="root-directory"><a href="#root-directory" class="headerlink" title="root directory"></a>root directory</h3><p>It includes <code>site</code> , <code>themes</code> , <code>deployment</code> , <code>sitemap</code> and so on. My complete configuration pushed at the <a href="https://github.com/Gojay001/Gojay001.github.io/blob/hexo/_config.yml" target="_blank" rel="noopener">Github</a> .</p><h3 id="icarus-directory"><a href="#icarus-directory" class="headerlink" title="icarus directory"></a>icarus directory</h3><p>Similarly, it contains important settings. Like <code>images</code> , <code>navbar</code> , <code>footer</code> , <code>search</code> , <code>comment</code>, <code>widgets</code>, and other <code>plugins</code> .</p><h2 id="Pages-amp-Domain"><a href="#Pages-amp-Domain" class="headerlink" title="Pages &amp; Domain"></a>Pages &amp; Domain</h2><ul><li><strong>Pages</strong>:<br>There are <code>Github Page</code> and <code>Coding Page</code> could be choose. More details can be find in their websites.</li><li><strong>Domain</strong>:<br>I register <code>.top</code> domain in <strong>AliCloud</strong> , following resolve the URL by <code>CNAME</code> .</li></ul><h2 id="Sitemap"><a href="#Sitemap" class="headerlink" title="Sitemap"></a>Sitemap</h2><ul><li>baidusitemap.xml</li><li>sitemap.xml(Google)</li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h1><hr><p>This blog just mentioned kinds of core <strong>keywords</strong> , if you have any question about that, just <code>google</code> or <code>baidu</code> it and then solve it.</p><blockquote><p>Notes:  </p><ol><li><strong>npm</strong> usage: <code>sudo cnpm install xxx</code> to use taobao mirrors.  </li><li>daily <strong>commands</strong>: <code>hexo clean</code> , <code>hexo s -g</code> , <code>hexo d -g</code> .  </li></ol></blockquote><h2 id="Source-Tree"><a href="#Source-Tree" class="headerlink" title="Source Tree"></a>Source Tree</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├─.deploy_git</span><br><span class="line">├─node_modules</span><br><span class="line">├─public</span><br><span class="line">├─scaffolds</span><br><span class="line">├─source</span><br><span class="line">  ├─_posts</span><br><span class="line">  ├─about</span><br><span class="line">  ├─gallery</span><br><span class="line">  └─CNAME</span><br><span class="line">└─themes</span><br><span class="line">    ├─landscape</span><br><span class="line">    └─icarus</span><br><span class="line">      ├─_config.yml</span><br><span class="line">      ├─source</span><br><span class="line">      └─...</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h1><blockquote><p>[1] <a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener"><strong>Hexo</strong> Documents</a><br>[2] <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener"><strong>Icarus</strong> Documents &amp; Issues</a><br>[3] <a href="https://www.alphalxy.com/2019/03/customize-icarus/" target="_blank" rel="noopener">customize <strong>Icarus</strong></a><br>[4] <a href="https://www.jianshu.com/p/934d4b501b18" target="_blank" rel="noopener">deploy <strong>git</strong> to Github &amp; Coding</a><br>[5] <a href="https://www.jianshu.com/p/7d3d87b52ad7?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation" target="_blank" rel="noopener">push <strong>sitemap</strong> to Baidu &amp; Google</a><br>[6] <a href="https://blog.csdn.net/u012195214/article/details/72721065" target="_blank" rel="noopener"><strong>back up</strong> hexo datas to Github</a>  </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;This blog records the details about &lt;strong&gt;building hexo blogs&lt;/strong&gt;, as well as deploying it to Github and Coding &lt;strong&gt;Pages&lt;/strong&gt;.  &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Toolkit" scheme="https://gojay.top/categories/Toolkit/"/>
    
      <category term="Hexo" scheme="https://gojay.top/categories/Toolkit/Hexo/"/>
    
    
      <category term="blog" scheme="https://gojay.top/tags/blog/"/>
    
      <category term="hexo" scheme="https://gojay.top/tags/hexo/"/>
    
      <category term="icarus" scheme="https://gojay.top/tags/icarus/"/>
    
  </entry>
  
  <entry>
    <title>Java Back-end Interview</title>
    <link href="https://gojay.top/2018/02/01/Java-Back-end-Interview/"/>
    <id>https://gojay.top/2018/02/01/Java-Back-end-Interview/</id>
    <published>2018-02-01T02:01:31.000Z</published>
    <updated>2020-06-17T12:26:30.898Z</updated>
    
    <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><hr><p>在大三上的期末时间段，偶然得知某滴信息安全部的后端开发实习岗位。由于厂牌和待遇的吸引，便抱着试一试的心态投了简历。下面就一面的提问进行技术总结，答案仅为个人理解，问题没有详细解释只提出大概要点。</p><a id="more"></a><h1 id="面试细节"><a href="#面试细节" class="headerlink" title="面试细节"></a>面试细节</h1><hr><h2 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h2><p>首先是介绍自己，包括求职岗位，个人情况（比如学校、年级、专业等），技术学习情况（技术栈、技术能力、项目等）。</p><h2 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h2><ul><li>项目架构</li><li>细节处理及实现</li></ul><p>由于在简历的项目经历必不可少，首先就选了一个项目进行细节剖析。这个就是把自己的实现思路表达出来，毕竟自己写的项目对所运用到的东西心里也有点数。针对这次面试，面试官对安全方面可能看得比较重，所以从头到尾一直在挖这边的细节。</p><h2 id="Java基础"><a href="#Java基础" class="headerlink" title="Java基础"></a>Java基础</h2><h3 id="String、StringBuffered"><a href="#String、StringBuffered" class="headerlink" title="String、StringBuffered"></a>String、StringBuffered</h3><ul><li>二者区别</li><li>存储在JVM哪里</li></ul><blockquote><p>可变，线程安全；<br>存储在常量池。</p></blockquote><h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><ul><li>Class文件编码</li><li>谈谈遇到过的乱码问题及如何解决</li><li>UTF-8和UTF-16区别</li></ul><blockquote><p>Class文件采用Unicode编码(UTF-16)；<br>乱码问题： <strong>页面乱码</strong>（HTML、JSP）， <strong>传值乱码</strong>（配置过滤器）， <strong>数据库乱码</strong>（检查Tomcat、Mysql配置）<br>其他相关： <code>http://www.qianxingzhem.com/post-1499.html</code></p></blockquote><h3 id="abstract、interface"><a href="#abstract、interface" class="headerlink" title="abstract、interface"></a>abstract、interface</h3><ul><li>两者区别</li></ul><blockquote><p>多继承实现，全部抽象；</p></blockquote><h3 id="JDK8"><a href="#JDK8" class="headerlink" title="JDK8"></a>JDK8</h3><blockquote><p>这个暂时不太了解，下面贴出关于新特性的官方说明和博客链接：<br>官方说明： <code>http://www.oracle.com/technetwork/java/javase/8-whats-new-2157071.html</code><br>博客： <code>http://blog.csdn.net/qiubabin/article/details/70256683</code></p></blockquote><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><ul><li>简述集合以及底层实现</li><li>HashMap、HashTable，如何改进</li><li>ArrayList、LinkedList，区别</li><li>HashMap、TreeMap、LinkedHashMap，区别</li></ul><blockquote><p>线程安全、效率；<br>数组查询，链表增删；<br><strong>HashMap</strong>：允许一条记录键为空，多条记录值为空，不同步；<br><strong>HashTable</strong>：不允许键值为空，同步，效率低；<br><strong>LinkedHashMap</strong>：保存记录的插入顺序；<br><strong>TreeMap</strong>：可以根据键排序。<br>参考： <code>http://blog.csdn.net/xin_jmail/article/details/25975085</code></p></blockquote><h3 id="类加载机制"><a href="#类加载机制" class="headerlink" title="类加载机制"></a>类加载机制</h3><blockquote><p>(1)全盘负责(2)父类委托(3)缓存机制<br>参考： <code>https://www.cnblogs.com/ityouknow/p/5603287.html</code></p></blockquote><h3 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h3><ul><li>内存模型</li><li>数据存储位置（如int i=0;）</li><li>GC（判断GC root，GC分区）</li></ul><blockquote><p>JVM相关： <code>http://blog.gojay.xin/2017/12/09/初识Java虚拟机/</code></p></blockquote><h2 id="专业相关"><a href="#专业相关" class="headerlink" title="专业相关"></a>专业相关</h2><ul><li><strong>如何保证信息没有被更改</strong></li><li><strong>认证、授权</strong></li><li><strong>数据加密</strong></li></ul><h2 id="后端相关"><a href="#后端相关" class="headerlink" title="后端相关"></a>后端相关</h2><h3 id="Servlet生命周期"><a href="#Servlet生命周期" class="headerlink" title="Servlet生命周期"></a>Servlet生命周期</h3><blockquote><p>容器加载类<br>实例化<br>init()<br>service()<br>destroy()</p></blockquote><h3 id="GET、POST"><a href="#GET、POST" class="headerlink" title="GET、POST"></a>GET、POST</h3><ul><li>二者区别</li></ul><blockquote><p>body，参数，安全性。</p></blockquote><h3 id="cookie、session"><a href="#cookie、session" class="headerlink" title="cookie、session"></a>cookie、session</h3><ul><li>二者区别</li><li>两台服务器负载均衡处理session</li></ul><blockquote><p>客户端（浏览器）、服务器；<br>session保持、复制、共享；</p></blockquote><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul><li>Stack特性</li><li>用stack实现时间复杂度为O(1)的getMin()方法</li></ul><blockquote><p>后进先出；<br>使用辅助栈<code>http://blog.csdn.net/sheepmu/article/details/38459165</code></p></blockquote><h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><ul><li>事务及特性</li></ul><blockquote><p><strong>ACID</strong> <code>https://www.cnblogs.com/nobounds/p/5409472.html</code></p></blockquote><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><p>后面就根据简历上面写的东西对其他方面提了一些问题，比如看过哪些技术书籍，平时怎么学习以及解决问题等等。</p><h1 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h1><hr><p>这次面试整体来说，主要是简历上写了哪些就问的相关技术点，熟悉、掌握、了解都基本会提到一点。感触较大的是什么问题他都能够一直往深处挖，到最后确实不知道或者自己看情况直接说不了解。由于这次招人比较紧急，要求不算太高，所以也就放得比较宽，没有太严格。我也是匆忙投简历，加上当天考英语六级，基本上就没怎么准备。后期会结合自身再改改简历，最重要的还是提升自己，针对技术方面还是需要稳扎稳打，夯实基础。过了几天换了一个人打电话来约二面的时候再次提到了入职和任职时间，最终因为没有协调好时间就没有进行二面。虽然有点遗憾，但是我也更加了解自己现在的水平，以及正式入职所需要具备的能力。这也算是一个转折点吧，最终我决定了考研，技术方面自然也会落下不少，技术博客最近一年也不会怎么更了吧，包括这一篇本应该一个多月前更的也拖到了现在。最后的最后，祝自己考研顺利吧，不忘初心，方能始终。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;在大三上的期末时间段，偶然得知某滴信息安全部的后端开发实习岗位。由于厂牌和待遇的吸引，便抱着试一试的心态投了简历。下面就一面的提问进行技术总结，答案仅为个人理解，问题没有详细解释只提出大概要点。&lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Interview" scheme="https://gojay.top/categories/JavaWeb/Interview/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="interview" scheme="https://gojay.top/tags/interview/"/>
    
  </entry>
  
  <entry>
    <title>Learning Java Virtual Machine(JVM)</title>
    <link href="https://gojay.top/2017/12/09/Learning-Java-Virtual-Machine(JVM)/"/>
    <id>https://gojay.top/2017/12/09/Learning-Java-Virtual-Machine(JVM)/</id>
    <published>2017-12-09T02:01:31.000Z</published>
    <updated>2020-06-17T12:26:30.898Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><hr><p>对于Java的学习也有一段时间了，却始终会有一些地方容易混淆，归结原因，还是偏底层的东西不太了解。前段时间便学习了关于Java虚拟机相关的内容，主要从阅读  <em>《深入理解Java虚拟机》</em> 进行总结。</p><a id="more"></a><p>首先<strong>Java技术体系</strong>主要由：<strong>Java第三方框架类库</strong>、<strong>Java API类库</strong>、<strong>Java程序设计语言</strong>、<strong>Class类文件格式</strong>、<strong>Java 虚拟机</strong>构成，把Java API类库、Java程序设计语言、Java虚拟机统称为JDK，用于支持Java程序开发的最小运行环境。<br>然后从 <strong>Java内存</strong> 相关的 <code>内存模型</code> 、 <code>内存分配</code> 、 <code>垃圾回收</code> 、 <code>内存溢出</code> ； <strong>虚拟机执行子系统</strong> 相关的 <code>Class类文件结构</code> 、 <code>类加载机制</code> 、 <code>字节码执行引擎</code> ； <strong>高效并发</strong> 相关的 <code>Java内存模型与线程</code> 、 <code>线程安全与锁优化</code> 几个部分进行了<strong>Java虚拟机</strong>初步的学习。</p><h1 id="Java内存"><a href="#Java内存" class="headerlink" title="Java内存"></a>Java内存</h1><hr><h2 id="内存模型"><a href="#内存模型" class="headerlink" title="内存模型"></a>内存模型</h2><p><img src="https://i.loli.net/2019/08/04/WiPuNh5pXHV3znS.png" alt="1.png"></p><h3 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h3><p><code>程序计数器(Program Counter Register)</code> 可以看做当前线程<strong>所执行的字节码的行号指示器</strong>，每条线程都有一个独立的程序计数器，称为 <code>线程私有</code> 内存。<br>如果线程正在执行一个<strong>Java方法</strong>，计数器记录的是正在执行的虚拟机<strong>字节码指令的地址</strong>；如果执行的是<strong>Native方法</strong>，则计数器值为<strong>空(Undefined)</strong>。</p><h3 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h3><p><code>虚拟机栈(Virtual Machine Stacks)</code> 是 <code>线程私有</code> ，每个<strong>方法在执行</strong>的同时都会创建一个<strong>栈帧(Stack Frame)</strong>，用于存储<code>局部变量表</code>、<code>操作数栈</code>、<code>动态链接</code>、<code>方法出口</code>等信息（具体内容在后面会讲到）。每个<strong>方法从调用到执行完成</strong>对应一个<strong>栈帧从入栈到出栈</strong>。  </p><blockquote><p>经常把Java内存分为堆内存(Heap)和栈内存(Stack)，这里指的栈就是虚拟机栈。</p></blockquote><h3 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h3><p><code>本地方法栈(Native Method Stack)</code> 与虚拟机栈作用类似，只不过不是为Java方法（也就是字节码）服务，而是为虚拟机使用的<strong>Native方法</strong>服务。</p><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p><code>堆(Heap)</code> 是被 <code>所有线程共享</code> 的一块内存区域，在<strong>虚拟机启动时创建</strong>。用于<strong>存放对象实例</strong>，几乎所有的<code>对象实例</code>以及<code>数组</code>都在堆上分配内存。<br>Java堆可以处于物理上不连续的内存空间中，只要逻辑上连续即可，也是<strong>垃圾收集器管理的主要区域</strong>。</p><h3 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h3><p><code>方法区(Method Area)</code> 是 <code>各个线程共享</code> 的内存区域。用于存储已经被虚拟机加载的<code>类信息</code>、<code>常量</code>、<code>静态变量</code>、<code>即时编译器编译后的代码</code>等数据。</p><blockquote><p>有别名叫Non-Heap(非堆)，也被称为“永久代”。</p></blockquote><h3 id="额外：HotSpot虚拟机对象"><a href="#额外：HotSpot虚拟机对象" class="headerlink" title="额外：HotSpot虚拟机对象"></a>额外：HotSpot虚拟机对象</h3><h4 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h4><ul><li><strong>类加载检查</strong>：虚拟机遇到new指令，检查这个指令的参数能否定位到一个类的符号引用；并检查这个符号引用代表的类是否已被加载、解析和初始化过。若没有，则先执行相应的类加载过程。</li><li><strong>为对象分配内存</strong>：把一块确定大小的内存从Java堆中划分出来。由Java堆是否规整有两种划分方式， <code>“指针碰撞”</code> ：中间放置一个指针作为分界点的指示器，分配内存就是指针向空闲空间挪动； <code>“空闲列表”</code> ：维护内存块可用的列表，分配内存就是从列表找出一块足够大的空间。</li><li><strong>初始化零值</strong>：将分配到的内存空间都初始化为零值，不包含对象头。</li><li><strong>必要设置</strong>：将对象的对象头信息取出进行必要的设置。</li><li><strong><init></init></strong>：执行new指令后会执行<init>方法，把对象按照程序员的意愿进行初始化。</init></li></ul><h4 id="对象的内存布局"><a href="#对象的内存布局" class="headerlink" title="对象的内存布局"></a>对象的内存布局</h4><ul><li><strong>对象头</strong>：对象头包含两部分信息，<code>“Mark Word”</code>用于存储对象自身的运行时数据；<code>“类型指针”</code>用于存储对象指向它的类元数据的指针。</li><li><strong>实例数据</strong>：在程序代码中所定义的各种类型的字段内容。</li><li><strong>对齐填充</strong>：不是必然存在的，只是起着占位符的作用。</li></ul><h4 id="内存的访问定位"><a href="#内存的访问定位" class="headerlink" title="内存的访问定位"></a>内存的访问定位</h4><ul><li><strong>使用句柄</strong>：Java堆划分一块内存作为句柄池，reference存储<strong>句柄地址</strong>。句柄中包含 <code>对象实例数据</code> 和 <code>类型数据的具体地址</code>。</li><li><strong>直接地址访问</strong>：Java堆的对象考虑如何放置访问类型数据的相关信息。reference存储的是<strong>对象地址</strong>。</li></ul><h2 id="内存溢出"><a href="#内存溢出" class="headerlink" title="内存溢出"></a>内存溢出</h2><h3 id="Java堆溢出"><a href="#Java堆溢出" class="headerlink" title="Java堆溢出"></a>Java堆溢出</h3><p>Java堆用于存储对象实例，不断创建对象，当避免垃圾回收机制，在对象数量达到最大堆的容量限制后就会产生内存溢出异常。</p><h3 id="虚拟机栈和本地方法栈溢出"><a href="#虚拟机栈和本地方法栈溢出" class="headerlink" title="虚拟机栈和本地方法栈溢出"></a>虚拟机栈和本地方法栈溢出</h3><ul><li>线程请求的栈深度大于虚拟机所允许的最大深度，将抛出 <code>StackOverflowError</code> 异常。</li><li>虚拟机在扩展栈时无法申请到足够的内存空间，将抛出 <code>OutOfMemoryError</code> 异常。</li></ul><h3 id="方法区溢出"><a href="#方法区溢出" class="headerlink" title="方法区溢出"></a>方法区溢出</h3><p>方法区用于存放Class相关信息，如类名、访问修饰符、常量池、字段描述、方法描述等，当运行时产生大量的类填满方法区时会产生内存溢出。</p><h2 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h2><h3 id="对象是否可回收"><a href="#对象是否可回收" class="headerlink" title="对象是否可回收"></a>对象是否可回收</h3><h4 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h4><p><code>引用计数算法</code> 是给对象添加一个<strong>引用计数器</strong>，当一个地方引用时，计数器值加1；当引用失效时，计数器值减1；当计数器值为0时说明对象不可用。</p><blockquote><p>不能解决对象之间相互循环引用的问题。</p></blockquote><h4 id="可达性分析算法"><a href="#可达性分析算法" class="headerlink" title="可达性分析算法"></a>可达性分析算法</h4><p><code>可达性分析</code> 是选取 <strong>“GC Roots”</strong> 对象作为起始点，向下搜索走过的路径称为 <code>引用链</code> ；当一个对象到GC Roots没有任何引用链的时候则说明对象不可用。</p><blockquote><p>Java语言中可作为引用链的对象包括：虚拟机栈、方法区中类静态属性、方法区常量、本地方法栈Native方法引用的对象。</p></blockquote><h4 id="两次标记"><a href="#两次标记" class="headerlink" title="两次标记"></a>两次标记</h4><p>在可达性分析算法中不可达的对象还需要经历两次标记才真正回收。</p><ul><li><strong>是否有必要执行finalize()方法</strong>：当对象 <code>没有覆盖finalize()方法</code> 或者finalize()方法 <code>已经被虚拟机调用过</code> 则视为没有必要执行。</li><li><strong>重新引用</strong>：如果对象有必要执行finalize()方法，则会将对象放置在F-Queue队列中，由低优先级Finalizer线程执行，在此过程中只要 <code>重新与引用链上一个对象建立关联</code> 则会移除回收队列。</li></ul><h3 id="垃圾回收算法"><a href="#垃圾回收算法" class="headerlink" title="垃圾回收算法"></a>垃圾回收算法</h3><h4 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h4><ul><li><strong>标记</strong>所有需要回收对象。</li><li><strong>回收</strong>所有被标记对象。</li></ul><h4 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h4><p>将内存按容量划分为两块，每次只使用其中一块。</p><ul><li><strong>标记</strong>回收对象。</li><li>将存活对象<strong>复制</strong>到另一块。</li><li>已使用内存空间全部<strong>回收</strong>。</li></ul><h4 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h4><ul><li><strong>标记</strong>回收对象。</li><li>将存活对象向一端<strong>移动</strong>。</li><li><strong>回收</strong>边界以外内存。</li></ul><h4 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h4><p>根据对象存活周期将Java堆划分为<strong>新生代</strong>和<strong>老年代</strong>，不同年代使用不同的垃圾回收算法。</p><h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><p><img src="https://i.loli.net/2019/08/04/q3ukFaliCT4Xv9d.png" alt="2.png"></p><h4 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h4><p><code>Serial收集器</code> 是负责<strong>新生代</strong>的收集的<strong>单线程</strong>收集器。垃圾回收时会<code>暂停其他所有的工作线程</code>，直到收集结束。</p><h4 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h4><p><code>ParNew收集器</code> 是Serial收集器的<strong>多线程</strong>版本。除了Serial收集器，只有它能与CMS收集器配合工作。</p><h4 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h4><p><code>Parallel Scavenge收集器</code> 也是使用<strong>复制算法</strong>的<strong>多线程</strong>收集器。目的是达到一个可控制的吞吐量， <code>吞吐量=运行用户代码时间 / （运行用户代码时间+垃圾回收时间）</code> ，也被称为 <code>“吞吐量优先”收集器</code>。</p><h4 id="Serial-Old收集器"><a href="#Serial-Old收集器" class="headerlink" title="Serial Old收集器"></a>Serial Old收集器</h4><p><code>Serial Old收集器</code> 是Serial收集器的<strong>老年代</strong>版本，使用<strong>标记-整理</strong>算法的<strong>单线程</strong>收集器。</p><h4 id="Parallel-Old收集器"><a href="#Parallel-Old收集器" class="headerlink" title="Parallel Old收集器"></a>Parallel Old收集器</h4><p><code>Parallel Old</code> 是Parallel Scavenge收集器的<strong>老年代</strong>版本，使用<strong>标记-整理</strong>算法的<strong>多线程</strong>收集器。</p><h4 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h4><p><code>CMS收集器</code> 是使用<strong>标记-清除</strong>算法的<strong>多线程</strong>收集器，目的是获取最短回收停顿时间。</p><ul><li>初始标记(CMS initial mark)</li><li>并发标记(CMS concurrent mark)</li><li>重新标记(CMS remark)</li><li>并发清除(CMS concurrent sweep)</li></ul><h4 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h4><p><code>G1收集器</code> 在后台维护一个优先列表，根据允许的收集时间，优先回收价值（回收所获得的空间大小以及所需时间）最大的Region。</p><ul><li>初始标记(Initial Marking)</li><li>并发标记(Concurrent Marking)</li><li>最终标记(Final Marking)</li><li>筛选回收(Live Data Counting and Evacuation)</li></ul><h3 id="理解GC日志"><a href="#理解GC日志" class="headerlink" title="理解GC日志"></a>理解GC日志</h3><p>GC日志是一些人为确定的规则，每个日志格式有收集器决定。通常来看由以下几个部分组成： <code>GC发生时间</code> 、 <code>GC停顿类型</code> 、 <code>GC发生区域</code> 、 <code>GC前后内存使用情况</code> 、 <code>GC所占用时间</code>。</p><h2 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h2><h3 id="对象优先在Eden分配"><a href="#对象优先在Eden分配" class="headerlink" title="对象优先在Eden分配"></a>对象优先在Eden分配</h3><p>大多情况下，对象在<strong>新生代Eden区域</strong>中分配。Eden空间不足时，虚拟机发起一次<strong>Minor GC</strong>。Minor GC过程：将Eden区域对象放入<strong>Survivor空间</strong>，若无法放入则通过分配担保机制提前进入到<strong>老年代</strong>。</p><blockquote><p>新生代GC(Minor GC)：Minor GC频繁，回收速度快。<br>老年代GC(Major GC/Full GC)：Full GC速度一般比Minor GC慢10倍。</p></blockquote><h4 id="空间分配担保"><a href="#空间分配担保" class="headerlink" title="空间分配担保"></a>空间分配担保</h4><p>在Minor GC前，虚拟机会先检查<strong>老年代连续空间</strong>是否大于<strong>新生代对象总大小</strong>。若成立，则Minor GC安全；若不成立，虚拟机会查看是否允许担保失败。<br><strong>担保</strong>：取每一次进入老年代对象的<strong>平均值</strong>与<strong>老年代剩余空间</strong>比较，若不足则进行Full GC。</p><h3 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h3><p><strong>大对象</strong>指需要 <code>大量连续内存空间</code> 的Java对象。当所需空间大于设置值时直接进入老年代分配，目的在于避免在Eden区及两个Survivor区之间发生大量的内存复制。</p><h3 id="长期存活的对象进入老年代"><a href="#长期存活的对象进入老年代" class="headerlink" title="长期存活的对象进入老年代"></a>长期存活的对象进入老年代</h3><p><strong>年龄计数器</strong>：对象在Survivor区每过一次Minor GC则年龄加1。当年龄大于设置值(默认为15)则进入老年代。<br><strong>对象年龄动态判断</strong>：如果在Survivor空间中， <code>相同年龄</code> 所有对象大小<strong>超过Survivor空间的一半</strong>，年龄大于或等于该年龄的对象进入老年代。</p><h1 id="虚拟机执行子系统"><a href="#虚拟机执行子系统" class="headerlink" title="虚拟机执行子系统"></a>虚拟机执行子系统</h1><hr><h2 id="Class类文件结构"><a href="#Class类文件结构" class="headerlink" title="Class类文件结构"></a>Class类文件结构</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul><li><strong>Class文件</strong>是一组以 <code>8个字节为单位</code> 的<strong>二进制流</strong>，对应着类或接口的定义信息，是实现 <code>平台无关性</code> 和 <code>语言无关性</code> 的基础。  </li><li><strong>Class文件格式</strong>采用 <code>伪结构</code> 存储数据，这种伪结构只有两种数据类型：<strong>无符号数</strong>(u1、u2、u4、u8代表x个字节的无符号数)，<strong>表</strong>(由多个无符号数或其他表构成，习惯以_info结尾)。</li></ul><h3 id="Class文件格式"><a href="#Class文件格式" class="headerlink" title="Class文件格式"></a>Class文件格式</h3><ul><li><strong>魔数</strong>：前 <code>4个字节</code> ，值为：0xCAFEBABE。</li><li><strong>Class版本号</strong>：紧接着魔数的 <code>4个字节</code> ，分别为：次版本号、主版本号。</li><li><strong>常量池</strong>：紧接着Class版本号，常量数量 <code>不固定</code> ，入口放置一项u2类型的 <code>常量池容量计数器</code> 。主要存放 <code>字面量</code> (Java中常量)和 <code>符号引用</code> (类、接口、字段、方法的名称和描述符)。</li><li><strong>访问标志</strong>：紧接着常量池的 <code>2个字节</code> ，用于标识一些类或接口层次的 <code>访问信息</code> 。</li><li><strong>类索引、父类索引、接口索引</strong>：排列着访问标志之后，类索引和父类索引用两个u2类型表示，接口索引是一组u2类型的集合；索引用于确定 <code>全限定名</code> 来确定这个类的 <code>继承关系</code> 。</li><li><strong>字段表集合</strong>：用于描述接口或者类中声明的 <code>变量信息</code> 。字段信息需要 <code>引用常量池</code> 中的常量来描述，无法固定大小。</li><li><strong>方法表集合</strong>：与字段表相似，用于 <code>描述方法</code> 定义的标志、名称索引、描述符索引。</li><li><strong>属性表集合</strong>：Class文件、字段表、方法表都可以携带自己的属性表集合，用于 <code>描述特定信息</code> 。预定义包含Code、Exception、LineNumberTable、LocalVariableTable等属性。</li></ul><blockquote><p><strong>全限定名</strong>：把<strong>类全名</strong>中的”.”替换成了”/“，如com/baidu/www/class/TestClass。<br><strong>简单名称</strong>：没有类型和参数修饰的<strong>方法或者字段名称</strong>，如inc()方法和m字段简称为inc和m。<br><strong>描述符</strong>：描述<strong>字段的数据类型</strong>、<strong>方法的参数类型</strong>、<strong>返回值</strong>。</p><ul><li><strong>基本类型</strong>和<strong>void</strong>用一个大写字符表示，I。</li><li><strong>对象类型</strong>用大写字符L加对象全限定名表示，Ljava/lang/String。</li><li><strong>数组类型</strong>的每一个维度使用一个前置的[字符描述，如[[Ljava/lang/String、[I。</li><li><strong>方法</strong>：先参数列表，后返回值，如()V、()Ljava/lang/String、([CII[CIII)I。</li></ul></blockquote><h3 id="字节码指令"><a href="#字节码指令" class="headerlink" title="字节码指令"></a>字节码指令</h3><blockquote><p>Java虚拟机的<strong>指令</strong>由<strong>操作码</strong>(一个字节长度的数字)和<strong>操作数</strong>(零至多个代表此操作所需的参数)构成。</p></blockquote><h4 id="字节码与数据类型"><a href="#字节码与数据类型" class="headerlink" title="字节码与数据类型"></a>字节码与数据类型</h4><p>由于Java虚拟机的<strong>操作码长度</strong>只有1个字节，指令集将会故意被设计为<strong>非完全独立</strong>，即并非每种数据类型和每一种操作都有对应的指令。<br>大部分的指令都没有支持<strong>boolean</strong>、<strong>byte</strong>、<strong>char</strong>、<strong>short</strong>类型的操作，实际上都是使用相应的<strong>int类型</strong>作为运算类型。</p><h4 id="加载和存储指令"><a href="#加载和存储指令" class="headerlink" title="加载和存储指令"></a>加载和存储指令</h4><p><code>加载和存储指令</code> 用于将数据在栈帧中的<strong>局部变量表</strong>和<strong>操作数栈</strong>之间来回<strong>传输</strong>。</p><ul><li><strong>将局部变量加载到操作数栈</strong>：iload等。</li><li><strong>将一个数值从操作数栈存储到局部变量表</strong>：istore等。</li><li><strong>将一个常量加载到操作数栈</strong>：bipush、sipush、iconst_m1等。</li><li><strong>扩充局部变量表的访问索引指令</strong>：wide。</li></ul><h4 id="运算指令"><a href="#运算指令" class="headerlink" title="运算指令"></a>运算指令</h4><p><code>运算或算数指令</code> 用于对两个操作数栈的值<strong>进行某种特定运算</strong>，并把结果重新存入到<strong>操作数栈顶</strong>。都使用Java虚拟机的数据类型，boolean、byte、char、short的运算都会转为int类型。<br><strong>算数指令</strong>有：<strong>加法</strong>(iadd、ladd、fadd、dadd)、<strong>减法</strong>(sub)、<strong>乘法</strong>(mul)、<strong>除法</strong>(div)、<strong>求余</strong>(rem)、<strong>取反</strong>(neg)、<strong>位移</strong>、<strong>按位或</strong>、<strong>按位与</strong>、<strong>按位异或</strong>、<strong>局部变量自增</strong>、<strong>比较</strong>。</p><h4 id="类型转换指令"><a href="#类型转换指令" class="headerlink" title="类型转换指令"></a>类型转换指令</h4><p>类型转换指令用于将两种不同的数值<strong>类型进行相互转换</strong>。 <code>宽化</code> 类型转换无需显式的转换指令。 <code>窄化</code> 必须显示转换：i2b、i2c、i2s、l2i、f2i、f2l、d2i、d2l、d2f。</p><h4 id="其他指令"><a href="#其他指令" class="headerlink" title="其他指令"></a>其他指令</h4><p>其他指令可以查看虚拟机字节码指令，这里不全部列出，主要有：<strong>对象创建与访问指令</strong>、<strong>操作数栈管理指令</strong>、<strong>控制转移指令</strong>、<strong>方法调用和返回指令</strong>、<strong>异常处理指令</strong>、<strong>同步指令</strong>。</p><h2 id="虚拟机类加载机制"><a href="#虚拟机类加载机制" class="headerlink" title="虚拟机类加载机制"></a>虚拟机类加载机制</h2><p><strong>类加载机制</strong>指虚拟机把<strong>描述类的数据</strong>从<strong>Class文件</strong>加载到<strong>内存</strong>，并对数据进行校验、转换解析、初始化，最终形成可以被虚拟机直接使用的<strong>Java类型</strong>。</p><h3 id="类加载的时机"><a href="#类加载的时机" class="headerlink" title="类加载的时机"></a>类加载的时机</h3><h4 id="类的生命周期"><a href="#类的生命周期" class="headerlink" title="类的生命周期"></a>类的生命周期</h4><ul><li>加载、连接（验证、准备、解析）、初始化、使用、卸载。</li></ul><h4 id="对类主动引用"><a href="#对类主动引用" class="headerlink" title="对类主动引用"></a>对类主动引用</h4><p>有且只有5中情况需要立即对类进行<strong>初始化</strong>：</p><ul><li>遇到<strong>new</strong>、<strong>getstatic</strong>、<strong>putstatic</strong>、<strong>invokestatic</strong>这4个字节码时，如果类没有过初始化，则需要先触发其初始化。Java代码场景： <code>new实例化对象</code> 、 <code>读取或设置一个类的静态字段</code> 、 <code>调用一个类的静态方法</code>。</li><li>使用 <code>java.lang.reflect包</code> 的方法对类进行<strong>反射调用</strong>时，该类没有过初始化需要触发初始化。</li><li>当<strong>初始化一个类</strong>，其<strong>父类</strong>没有过初始化需要先初始化父类。</li><li><strong>虚拟机启动</strong>时，需要先初始化含main()方法的<strong>主类</strong>。</li><li>java.lang.invoke.MethodHandle实例<strong>解析句柄对应的类</strong>需要初始化。</li></ul><h4 id="被动引用"><a href="#被动引用" class="headerlink" title="被动引用"></a>被动引用</h4><p>所有被动引用类都不会触发初始化。</p><ul><li>通过<strong>子类引用父类的静态字段</strong>，只会触发父类的初始化，而不会触发子类的初始化。</li><li>通过<strong>数组定义来引用类</strong>，不会触发此类的初始化。</li><li><strong>调用类的常量</strong>，由于常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用类，不会触发该类的初始化。</li></ul><blockquote><p>接口不要求其父接口都完成了初始化，只有在真正使用父接口的时候(引用接口的常量)才会初始化。</p></blockquote><h3 id="类加载的过程"><a href="#类加载的过程" class="headerlink" title="类加载的过程"></a>类加载的过程</h3><p>Java虚拟机中类加载全过程：加载、验证、准备、解析、初始化。</p><h4 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h4><ul><li>通过一个<strong>类的全限定名</strong>来获取定义此<strong>类的二进制流</strong>。</li><li>将这个字节流按照虚拟机所需的格式<strong>存储在方法区中</strong>。</li><li>在内存中生成一个代表这个类的<strong>java.lang.Class对象</strong>(HotSpot虚拟机存放在方法区中)，作为方法区这个类的各种数据的访问入口。</li></ul><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>确保Class文件的字节流中包含的信息符合虚拟机的要求。</p><ul><li><strong>文件格式验证</strong>：对<strong>Class文件格式</strong>中魔数、版本号、常量池等进行验证，保证字节流能正常<strong>解析并存储</strong>到方法区。</li><li><strong>元数据验证</strong>：对<strong>字节码描述的信息</strong>该类是否有父类、是否继承final类等进行语义分析，保证符合Java语言规范。</li><li><strong>字节码验证</strong>：通过数据流和控制流分析，确定<strong>程序语义</strong>是符合逻辑的。</li><li><strong>符号引用验证</strong>：对类自身以外(<strong>常量池中的各种符号引用</strong>)的信息进行匹配性验证。</li></ul><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>为<strong>类变量</strong>(static修饰)<strong>分配内存</strong>并<strong>设置初值</strong>(数据类型的零值)。</p><h4 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h4><p>虚拟机将常量池内的符号引用替换为直接引用。</p><blockquote><ul><li>符号引用：以一组符号来描述所引用的目标，引用目标并不一定已经加载到内存中。</li><li>直接引用：直接引用是直接指向目标的指针、偏移量或者是一个能够间接定位到目标的句柄。</li></ul></blockquote><p>解析动作主要针对<strong>类或接口</strong>、<strong>字段</strong>、<strong>类方法</strong>、<strong>接口方法</strong>、<strong>方法类型</strong>、<strong>方法句柄</strong>、<strong>调用点限定符</strong>这几类符号引用进行。</p><h5 id="类或接口的解析"><a href="#类或接口的解析" class="headerlink" title="类或接口的解析"></a>类或接口的解析</h5><p>假设当前代码所处的 <code>类为D</code> ，要把一个从未解析过的 <code>符号引用N</code> 解析为一个 <code>类或接口C</code> 的直接引用：</p><ul><li>如果<strong>C不是一个数组类型</strong>，虚拟机会把代表<strong>N的全限定名</strong>传递给<strong>D的类加载器</strong>去加载这个类C；可能会触发其他相关类的加载，如父类或实现的接口。</li><li>如果<strong>C是一个数组类型</strong>，数组的元素类型为<strong>对象</strong>按照上一点规则加载；数组的元素类型为<strong>基本类型</strong>则由虚拟机生成一个代表此数组维度和元素的数组对象。</li><li>如果以上步骤没有异常，则C在虚拟机中已经成为一个有效的类或接口。</li></ul><h5 id="字段解析"><a href="#字段解析" class="headerlink" title="字段解析"></a>字段解析</h5><ul><li>解析字段<strong>所属的类或接口</strong>的符号引用。</li><li>与<strong>类中</strong>匹配目标的<strong>简单名称</strong>和<strong>字段描述符</strong>。</li><li>按照继承关系从下往上递归搜索<strong>接口和父接口</strong>。</li><li>如果不是java.lang.Object，搜索其<strong>父类</strong>。</li><li>否则，查找失败。</li></ul><h5 id="类方法解析"><a href="#类方法解析" class="headerlink" title="类方法解析"></a>类方法解析</h5><ul><li>解析方法<strong>所属的类或接口</strong>的符号引用。</li><li>在<strong>类中</strong>查找简单名称和描述符。</li><li>在<strong>类的父类</strong>中查找简单名称和描述符。</li><li>在<strong>类实现的接口列表和父接口</strong>中匹配。</li><li>否则，查找失败。</li></ul><h5 id="接口方法解析"><a href="#接口方法解析" class="headerlink" title="接口方法解析"></a>接口方法解析</h5><ul><li>解析方法<strong>所属的类或接口</strong>的符号引用。</li><li>在<strong>接口中</strong>查找简单名称和描述符。</li><li>在<strong>父接口</strong>中查找简单名称和描述符。</li><li>否则，查找失败。</li></ul><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>开始执行类中定义的Java程序代码(字节码)。初始化阶段时<strong>执行类构造器<clinit>()方法</clinit></strong>的过程。</p><ul><li><clinit>()方法是由编译器自动收集类中的<strong>类变量赋值</strong>和<strong>静态语句块</strong>。静态语句块中只能访问到定义在静态语句块之前变量， <code>之后的变量只能赋值不能访问</code>。</clinit></li><li><clinit>()方法实例构造器<init>()方法不同，<strong>不需要显式调用父类构造器</strong>，<strong>保证父类<clinit>()方法在子类<clinit>()方法前执行</clinit></clinit></strong>。</init></clinit></li><li>由于父类先执行<clinit>()方法，父类定义的<strong>静态语句块</strong>先于子类，第一个被执行<clinit>()方法的类是<strong>java.lang.Object</strong>。</clinit></clinit></li><li><clinit>()方法对呀类或接口<strong>不是必需</strong>的。</clinit></li><li>虚拟机会保证一个类的<clinit>()方法在<strong>多线程环境中正确被加锁、同步</strong>。</clinit></li></ul><h3 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h3><p><strong>类加载器</strong>是实现让<strong>应用程序</strong>自己决定如何去<strong>获取所需要的类</strong>的代码模块。</p><h4 id="类与类加载器"><a href="#类与类加载器" class="headerlink" title="类与类加载器"></a>类与类加载器</h4><p>对于任意一个类，都需要由加载它的类加载器和这个类本身一起确立其在Java虚拟机中的唯一性。</p><h4 id="双亲委派模型"><a href="#双亲委派模型" class="headerlink" title="双亲委派模型"></a>双亲委派模型</h4><ul><li><strong>启动类加载器(Bootstrap ClassLoader)</strong>：负责将存放在<strong>&lt;JAVA_HOME\lib&gt;</strong>目录下的类库加载到虚拟机内存中。</li><li><strong>扩展类加载器(Extension ClassLoader)</strong>：负责加载<strong>&lt;JAVA_HOME\lib\ext&gt;</strong>目录中的类库。</li><li><strong>应用程序类加载器(Application ClassLoader)</strong>：负责加载<strong>用户类路径(ClassPath)</strong>指定的类库。</li></ul><p>双亲委派模型的工作流程：如果一个类加载器<strong>收到类加载的请求</strong>，首先将这个请求委派给<strong>父类加载器</strong>去完成，最终传送到<strong>顶层的启动类加载器</strong>；当父加载器反馈无法完成这个加载请求，<strong>子加载器</strong>才会尝试加载。</p><h4 id="破坏双亲委派模型"><a href="#破坏双亲委派模型" class="headerlink" title="破坏双亲委派模型"></a>破坏双亲委派模型</h4><ul><li>第一次：JDK 1.2发布前，重写loadClass()方法。</li><li>第二次：模型自身缺陷，线程上下文类加载器可以实现父类加载器请求子类加载器去完成类加载动作。</li><li>第三次：对程序动态性的追求。</li></ul><h2 id="字节码执行引擎"><a href="#字节码执行引擎" class="headerlink" title="字节码执行引擎"></a>字节码执行引擎</h2><h3 id="运行时栈帧结构"><a href="#运行时栈帧结构" class="headerlink" title="运行时栈帧结构"></a>运行时栈帧结构</h3><p><code>栈帧(Stack Frame)</code> 是用于支持虚拟机进行方法调用和方法执行的数据结构，存储了<strong>局部变量表</strong>、<strong>操作数栈</strong>、<strong>动态连接</strong>、<strong>方法返回地址</strong>等信息。每一个<strong>方法</strong>从调用开始到执行完成对应栈帧在虚拟机从<strong>入栈到出栈</strong>的过程。</p><h4 id="局部变量表"><a href="#局部变量表" class="headerlink" title="局部变量表"></a>局部变量表</h4><p><code>局部变量表(Local Variable Table)</code> 是一组变量值存储空间，用于存放<strong>方法参数</strong>和方法内部定义的<strong>局部变量</strong>。<br>局部变量表的容量以变量槽(Variable Slot)为最小单位，每个<strong>Slot</strong>都能存放一个 <code>boolean</code> 、 <code>byte</code> 、 <code>char</code> 、 <code>short</code> 、 <code>int</code> 、 <code>float</code> 、 <code>reference</code> 、 <code>returnAddress</code> 类型的数据。<br><strong>reference类型</strong>表示对一个<strong>对象的引用</strong>，通过引用要做到两点：从此引用中直接或间接地查找到到对象在 <code>Java堆</code> 中的数据存放的起始 <code>地址索引</code> ；此引用中直接或间接查找到对象所属数据类型在 <code>方法区</code> 中的存储的 <code>类型</code> 信息。<br>虚拟机通过<strong>索引定位</strong>的方式使用局部变量表。如果执行的<strong>实例方法</strong>(非static)，局部变量表的第0位索引的Slot默认是用于传递方法<strong>所属对象实例的引用</strong>，在方法中可以通过<strong>关键字this</strong>访问该隐含参数。其他参数按照参数表顺序排序，局部变量表的Slot可以重用。<br>如果一个局部变量 <code>定义</code> 了但没有 <code>赋初始值</code> 会导致类加载失败。</p><h4 id="操作数栈"><a href="#操作数栈" class="headerlink" title="操作数栈"></a>操作数栈</h4><p><code>操作数栈(Operand Stack)</code> 是一个 <code>后入先出(Last In First Out)</code> 栈，每一个元素可以是任意的Java数据类型。<br>在方法执行过程，各种字节码指令往操作数栈中写入和提取内容，也就是<strong>入栈/出栈</strong>操作。</p><h4 id="动态连接"><a href="#动态连接" class="headerlink" title="动态连接"></a>动态连接</h4><p>每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用。在每一次<strong>运行时期</strong>将<strong>符号引用</strong>转化为<strong>直接引用</strong>称为动态连接。</p><h4 id="返回地址"><a href="#返回地址" class="headerlink" title="返回地址"></a>返回地址</h4><p>方法在退出后，都需要返回到<strong>方法被调用的位置</strong>，栈帧保存返回地址信息。</p><h3 id="方法调用"><a href="#方法调用" class="headerlink" title="方法调用"></a>方法调用</h3><p>方法调用的目的是确定被调用方法的版本，即<strong>调用哪一个方法</strong>。</p><h4 id="解析-1"><a href="#解析-1" class="headerlink" title="解析"></a>解析</h4><p>所有的方法调用中的目标方法在Class文件中都是一个<strong>常量池中的符号引用</strong>。在类加载的解析阶段，将会把一部分<strong>符号引用转化为直接引用</strong>，解析前提是 <strong>“编译期可知，运行期不可变”</strong> 。</p><ul><li>对应的调用字节码指令：invokestatic、invokespecial、invokevirtual、invokeinterface、invokedynamic。</li><li>符合条件：<strong>静态方法</strong>、<strong>私有方法</strong>、<strong>实例构造器</strong>、<strong>父类方法</strong>、<strong>final修饰方法</strong>。</li></ul><h4 id="分派"><a href="#分派" class="headerlink" title="分派"></a>分派</h4><h5 id="静态分派"><a href="#静态分派" class="headerlink" title="静态分派"></a>静态分派</h5><p>所有<strong>依赖静态类型</strong>来定位方法执行版本的分派动作称为静态分派。(<strong>方法重载</strong>)</p><blockquote><ul><li>静态类型在编译期可知。</li><li>实际类型变化的结果在运行期确定。</li></ul></blockquote><h6 id="重载方法匹配优先级"><a href="#重载方法匹配优先级" class="headerlink" title="重载方法匹配优先级"></a>重载方法匹配优先级</h6><p>以’a’为例：</p><ul><li>char-&gt;int-&gt;long-&gt;float-&gt;double</li><li>java.lang.Character</li><li>java.lang.Serializable、java.lang.Comparable<character></character></li><li>装箱转型为父类</li><li>变长参数</li></ul><h5 id="动态分派"><a href="#动态分派" class="headerlink" title="动态分派"></a>动态分派</h5><p>运行期根据<strong>实际类型</strong>确定方法执行版本的分派过程称为动态分派。(<strong>方法重写</strong>)</p><h5 id="单分配与多分配"><a href="#单分配与多分配" class="headerlink" title="单分配与多分配"></a>单分配与多分配</h5><p><strong>方法宗量：方法接受者+方法参数</strong>。根据分派基于多少种宗量划分为单分配和多分配。</p><ul><li>静态分派：选择目标方法。(静态类型+方法参数)</li><li>动态分派：方法接受者的实际类型。</li></ul><blockquote><p>Java是静态多分配、动态单分配的语言。</p></blockquote><h1 id="高效并发"><a href="#高效并发" class="headerlink" title="高效并发"></a>高效并发</h1><hr><h2 id="Java内存模型与线程"><a href="#Java内存模型与线程" class="headerlink" title="Java内存模型与线程"></a>Java内存模型与线程</h2><h3 id="Java内存模型"><a href="#Java内存模型" class="headerlink" title="Java内存模型"></a>Java内存模型</h3><p><img src="https://i.loli.net/2019/08/04/14QEPD5J7Lr8lkY.png" alt="3.png"></p><h4 id="主内存与工作内存"><a href="#主内存与工作内存" class="headerlink" title="主内存与工作内存"></a>主内存与工作内存</h4><p><strong>Java内存模型</strong>的目的是定义程序中各个变量的访问规则，即在虚拟机中将<strong>变量存储到内存</strong>和从<strong>内存中取出变量</strong>的底层细节。</p><blockquote><p>这里的<strong>变量</strong>与Java编程中所说的变量不同，它包括<strong>实例字段</strong>、<strong>静态字段</strong>、<strong>构成数组对象的元素</strong>，不包括局部变量和方法参数。</p></blockquote><h5 id="Java内存模型规定"><a href="#Java内存模型规定" class="headerlink" title="Java内存模型规定"></a>Java内存模型规定</h5><ul><li>所有变量都<strong>存储在主内存</strong>。</li><li>每天线程有自己的<strong>工作内存</strong>。</li><li><strong>工作内存</strong>保存主内存的<strong>副本拷贝</strong>。</li><li>线程对变量的所有<strong>操作</strong>（读取、赋值等）都在<strong>工作内存</strong>中进行。</li></ul><blockquote><p><strong>主内存</strong>对应于<strong>Java堆</strong>中的对象实例数据部分。<br><strong>工作内存</strong>对应<strong>虚拟机栈</strong>中的部分区域。</p></blockquote><h4 id="内存间交互操作"><a href="#内存间交互操作" class="headerlink" title="内存间交互操作"></a>内存间交互操作</h4><ul><li><strong>lock</strong>：把一个变量标识为一条线程独占状态。</li><li><strong>unlock</strong>：释放锁定变量。</li><li><strong>read</strong>：把变量从主内存读取到工作线程。</li><li><strong>load</strong>：把read操作读取的变量值放入工作内存变量副本中。</li><li><strong>use</strong>：把工作内存的变量值传递给执行引擎。</li><li><strong>assign</strong>：从执行引擎接收变量值到工作线程。</li><li><strong>store</strong>：把工作线程的变量值传到主内存中。</li><li><strong>write</strong>：把store操作得到的变量值放入主内存的变量中。</li></ul><h4 id="原子性、可见性、有序性"><a href="#原子性、可见性、有序性" class="headerlink" title="原子性、可见性、有序性"></a>原子性、可见性、有序性</h4><ul><li><strong>原子性</strong>：线程从运行开始会一直到运行结束，不会被方法调度打断或进行线程切换。</li><li><strong>可见性</strong>：当一个线程修改了共享变量的值，其他线程能够立即得到这个修改。</li><li><strong>有序性</strong>：禁止指令重排序，在本线程中表现为串行，整体表现为指令重排序。</li></ul><h4 id="volatile关键字"><a href="#volatile关键字" class="headerlink" title="volatile关键字"></a>volatile关键字</h4><ul><li>保证了<strong>可见性</strong>和<strong>有序性</strong>。  </li><li>Java运算并非原子操作，导致volatile变量的运算在并发下不安全。</li><li>需要确保运算结果并不依赖变量的当前值来<strong>保证原子性</strong>。</li></ul><blockquote><p><strong>synchronized同步块</strong>同时保证了原子性、可见性、有序性。</p></blockquote><h4 id="先行发生原则"><a href="#先行发生原则" class="headerlink" title="先行发生原则"></a>先行发生原则</h4><p>先行发生是Java内存模型中定义的两项操作之间的偏序关系。如果操作A先行于操作B，则操作A产生的影响能够被操作B观察到。 </p><h5 id="Java内存模型预定义的先行发生关系"><a href="#Java内存模型预定义的先行发生关系" class="headerlink" title="Java内存模型预定义的先行发生关系"></a>Java内存模型预定义的先行发生关系</h5><ul><li><strong>程序次序规则</strong>：一个线程中，按照程序代码顺序。</li><li><strong>管程锁定规则</strong>：unlock操作先行于同一个锁的lock操作。</li><li><strong>volatile变量规则</strong>：volatile变量的写操作先行于后面的读操作。</li><li><strong>线程启动规则</strong>：Thread对象的start()方法优先。</li><li><strong>线程终止规则</strong>：Thread.join()方法结束最后。</li><li><strong>线程中断规则</strong>：interrupt()方法先行于中断事件。</li><li><strong>对象终结规则</strong>：一个对象的初始化完成先行于finalize()方法。</li><li><strong>传递性</strong>：A先行于B，B先行于C，则A先行于C。</li></ul><blockquote><p>时间先后顺序与先行发生规则基本没有关系。</p></blockquote><h3 id="Java与线程"><a href="#Java与线程" class="headerlink" title="Java与线程"></a>Java与线程</h3><h4 id="线程的实现"><a href="#线程的实现" class="headerlink" title="线程的实现"></a>线程的实现</h4><p>各个<strong>线程</strong>既可以<strong>共享进程资源</strong>，又可以<strong>独立调度</strong>。</p><h5 id="实现线程的方式"><a href="#实现线程的方式" class="headerlink" title="实现线程的方式"></a>实现线程的方式</h5><ul><li>使用内核线程实现：直接由操作系统内核支持的线程，用内核线程支持 <strong>轻量级进程(LWP)</strong> 实现。</li><li>使用用户线程实现：<strong>用户线程(UT)</strong> 完全建立在用户空间的线程库，不需要切换到内核态。</li><li>混合实现：既存在<strong>用户线程</strong>，也存在<strong>轻量级进程</strong>。</li></ul><h4 id="Java线程调度"><a href="#Java线程调度" class="headerlink" title="Java线程调度"></a>Java线程调度</h4><p><strong>线程调度</strong>指系统为线程<strong>分配处理器的使用权</strong>。</p><ul><li><strong>协同式调度</strong>：线程的执行时间由<strong>自己控制</strong>，线程自身执行完后<strong>主动通知</strong>系统切换到另一个线程。</li><li>抢占式调度：每个线程由<strong>系统分配</strong>执行时间，可以设置<strong>线程优先级</strong>。</li></ul><h4 id="状态转换"><a href="#状态转换" class="headerlink" title="状态转换"></a>状态转换</h4><ul><li><strong>新建(New)</strong>：创建后尚未启动的线程。</li><li><strong>运行(Runable)</strong>：包括Running和Ready。</li><li><strong>无限期等待(Wating)</strong>：等待其他线程显式唤醒。</li><li><strong>限期等待(Timed Wating)</strong>：一定时间后由系统自动唤醒。</li><li><strong>阻塞(Blocked)</strong>：等待获取一个排它锁，在另一个线程放弃这个锁时发生。</li><li><strong>结束(Terminated)</strong>：已终止线程。</li></ul><h2 id="线程安全与锁优化"><a href="#线程安全与锁优化" class="headerlink" title="线程安全与锁优化"></a>线程安全与锁优化</h2><h3 id="线程安全"><a href="#线程安全" class="headerlink" title="线程安全"></a>线程安全</h3><p>线程安全指当<strong>多个线程访问一个对象</strong>时，调用这个对象的行为都可以<strong>像单线程</strong>一样得到正确的结果。</p><h4 id="共享数据类型"><a href="#共享数据类型" class="headerlink" title="共享数据类型"></a>共享数据类型</h4><ul><li><strong>不可变</strong>：不可变的对象一定是线程安全的，如String。</li><li><strong>绝对线程安全</strong>：在多线程环境中需要在方法调用端做额外的同步措施。</li><li><strong>相对线程安全</strong>：需要保证这个对象单独的操作是线程安全的，如：Vector、HashTable等。</li><li><strong>线程兼容</strong>：对象本身并不是线程安全的，可以通过在调用端正确使用同步手段保证线程安全，如ArrayList、HashMap等。</li><li><strong>线程对立</strong>：无论调用端是否采用同步措施，都无法在多线程环境中并发使用。</li></ul><h4 id="线程安全的实现"><a href="#线程安全的实现" class="headerlink" title="线程安全的实现"></a>线程安全的实现</h4><ul><li><strong>互斥同步</strong>：synchronized或java.util.concurrent包中ReentrantLock实现。</li><li><strong>非阻塞同步</strong>：先进行操作，产生了冲突再采取补偿措施，也称为“乐观锁”。</li><li><strong>无同步方案</strong>：可重入代码、线程本地存储。</li></ul><h3 id="锁优化"><a href="#锁优化" class="headerlink" title="锁优化"></a>锁优化</h3><ul><li><strong>自旋锁</strong>：不放弃处理器的执行时间，让线程执行一个忙循环（自旋）。</li><li><strong>锁消除</strong>：对一些代码要求同步却被检测到不可能存在共享数据竞争的锁进行消除。</li><li><strong>锁粗化</strong>：如果一系列的连续操作都对同一个对象反复加锁和解锁，则将加锁同步范围扩展到整个操作序列的外部。</li><li>轻量级锁</li><li>偏向锁</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;对于Java的学习也有一段时间了，却始终会有一些地方容易混淆，归结原因，还是偏底层的东西不太了解。前段时间便学习了关于Java虚拟机相关的内容，主要从阅读  &lt;em&gt;《深入理解Java虚拟机》&lt;/em&gt; 进行总结。&lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="JVM" scheme="https://gojay.top/categories/JavaWeb/JVM/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="JVM" scheme="https://gojay.top/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Shiro（六）-Realm及相关对象</title>
    <link href="https://gojay.top/2017/11/30/%E8%B7%9F%E6%88%91%E5%AD%A6Shiro%EF%BC%88%E5%85%AD%EF%BC%89-Realm%E5%8F%8A%E7%9B%B8%E5%85%B3%E5%AF%B9%E8%B1%A1/"/>
    <id>https://gojay.top/2017/11/30/跟我学Shiro（六）-Realm及相关对象/</id>
    <published>2017-11-30T09:18:00.000Z</published>
    <updated>2020-06-17T12:26:30.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Realm"><a href="#Realm" class="headerlink" title="Realm"></a>Realm</h1><hr><h2 id="定义实体及关系"><a href="#定义实体及关系" class="headerlink" title="定义实体及关系"></a>定义实体及关系</h2><p><img src="https://i.loli.net/2019/08/04/9IzlrkhGxKUiEab.png" alt="6-1.png"></p><a id="more"></a><p><strong>用户-角色</strong>之间是多对多关系，<strong>角色-权限</strong>之间是多对多关系；且<code>用户</code>和<code>权限</code>之间通过<code>角色</code>建立关系。<br>在系统中验证时通过<code>权限验证</code>，角色只是权限集合，即所谓的显式角色。</p><ul><li><strong>用户</strong>实体包括：编号<code>id</code>、用户名<code>username</code>、密码<code>password</code>、盐<code>salt</code>、是否锁定<code>locked</code>；</li><li><strong>角色</strong>实体包括：、编号<code>id</code>、角色标识符<code>role</code>、描述<code>description</code>、是否可用<code>available</code>；</li><li><strong>权限</strong>实体包括：编号<code>id</code>、权限标识符<code>permission</code>、描述<code>description</code>、是否可用<code>available</code>。</li></ul><blockquote><p>另外还有两个<strong>关系实体</strong>：<br><strong>用户-角色</strong>实体：用户编号、角色编号，且组合为复合主键）；<br><strong>角色-权限</strong>实体：角色编号、权限编号，且组合为复合主键）。</p></blockquote><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>为了方便数据库操作，使用 <code>org.springframework: spring-jdbc: 4.0.0.RELEASE</code> 依赖。</p><h3 id="定义Service及Dao"><a href="#定义Service及Dao" class="headerlink" title="定义Service及Dao"></a>定义Service及Dao</h3><blockquote><p>为了实现的简单性，只实现必须的功能，其他的可以自己实现即可。</p></blockquote><h4 id="PermissionService"><a href="#PermissionService" class="headerlink" title="PermissionService"></a>PermissionService</h4><p>实现基本的<strong>创建/删除</strong>权限。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface PermissionService &#123;  </span><br><span class="line">    public Permission createPermission(Permission permission);  </span><br><span class="line">    public void deletePermission(Long permissionId);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="RoleService"><a href="#RoleService" class="headerlink" title="RoleService"></a>RoleService</h4><p>相对于PermissionService多了<strong>关联/移除关联角色-权限</strong>功能。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface RoleService &#123;  </span><br><span class="line">    public Role createRole(Role role);  </span><br><span class="line">    public void deleteRole(Long roleId);  </span><br><span class="line">    // 添加角色-权限之间关系  </span><br><span class="line">    public void correlationPermissions(Long roleId, Long... permissionIds);  </span><br><span class="line">    // 移除角色-权限之间关系  </span><br><span class="line">    public void uncorrelationPermissions(Long roleId, Long... permissionIds);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="UserService"><a href="#UserService" class="headerlink" title="UserService"></a>UserService</h4><p>使用 <strong>findByUsername</strong> 、 <strong>findRoles</strong> 及 <strong>findPermissions</strong> 来查找用户名对应的帐号、角色及权限信息。  </p><blockquote><p>之后的Realm就使用这些方法来查找相关信息。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface UserService &#123;  </span><br><span class="line">    // 创建账户</span><br><span class="line">    public User createUser(User user);   </span><br><span class="line">    // 修改密码</span><br><span class="line">    public void changePassword(Long userId, String newPassword);  </span><br><span class="line">    // 添加用户-角色关系</span><br><span class="line">    public void correlationRoles(Long userId, Long... roleIds);   </span><br><span class="line">    // 移除用户-角色关系</span><br><span class="line">    public void uncorrelationRoles(Long userId, Long... roleIds);  </span><br><span class="line">    // 根据用户名查找用户</span><br><span class="line">    public User findByUsername(String username);  </span><br><span class="line">    // 根据用户名查找其角色</span><br><span class="line">    public Set&lt;String&gt; findRoles(String username); </span><br><span class="line">    // 根据用户名查找其权限  </span><br><span class="line">    public Set&lt;String&gt; findPermissions(String username); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="UserServiceImpl"><a href="#UserServiceImpl" class="headerlink" title="UserServiceImpl"></a>UserServiceImpl</h4><p>在创建账户及修改密码时直接把<strong>生成密码操作</strong>委托给 <code>PasswordHelper</code>。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public User createUser(User user) &#123;  </span><br><span class="line">    // 加密密码  </span><br><span class="line">    passwordUtils.encryptPassword(user);  </span><br><span class="line">    return userDao.createUser(user);  </span><br><span class="line">&#125;  </span><br><span class="line">public void changePassword(Long userId, String newPassword) &#123;  </span><br><span class="line">    User user =userDao.findOne(userId);  </span><br><span class="line">    user.setPassword(newPassword);  </span><br><span class="line">    passwordUtils.encryptPassword(user);  </span><br><span class="line">    userDao.updateUser(user);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="PasswordUtils"><a href="#PasswordUtils" class="headerlink" title="PasswordUtils"></a>PasswordUtils</h4><p>之后的CredentialsMatcher需要和此处加密的算法一样。<br>user.getCredentialsSalt()辅助方法返回username+salt。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class PasswordUtils &#123;  </span><br><span class="line">    private RandomNumberGenerator randomNumberGenerator =  </span><br><span class="line">     new SecureRandomNumberGenerator();  </span><br><span class="line">    private String algorithmName = &quot;md5&quot;;  </span><br><span class="line">    private final int hashIterations = 2;  </span><br><span class="line">    public void encryptPassword(User user) &#123;  </span><br><span class="line">        user.setSalt(randomNumberGenerator.nextBytes().toHex());  </span><br><span class="line">        String newPassword = new SimpleHash(  </span><br><span class="line">                algorithmName,  </span><br><span class="line">                user.getPassword(),  </span><br><span class="line">                ByteSource.Util.bytes(user.getCredentialsSalt()),  </span><br><span class="line">                hashIterations).toHex();  </span><br><span class="line">        user.setPassword(newPassword);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>为了节省篇幅，对于<strong>DAO/Service</strong>的接口及实现，具体请参考源码 <code>com.github.gojay001</code> ；<br>另外参考Service层的测试用例 <code>com.github.gojay001.service.ServiceTest</code> 。</p></blockquote><h3 id="定义Realm"><a href="#定义Realm" class="headerlink" title="定义Realm"></a>定义Realm</h3><h4 id="RetryLimitHashedCredentialsMatcher"><a href="#RetryLimitHashedCredentialsMatcher" class="headerlink" title="RetryLimitHashedCredentialsMatcher"></a>RetryLimitHashedCredentialsMatcher</h4><blockquote><p>com.github.gojay001.credentials</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class RetryLimitHashedCredentialsMatcher extends HashedCredentialsMatcher &#123;</span><br><span class="line"></span><br><span class="line">    private Ehcache passwordRetryCache;</span><br><span class="line"></span><br><span class="line">    public RetryLimitHashedCredentialsMatcher() &#123;</span><br><span class="line">        CacheManager cacheManager = CacheManager.newInstance(CacheManager.class.getClassLoader().getResource(&quot;ehcache.xml&quot;));</span><br><span class="line">        passwordRetryCache = cacheManager.getCache(&quot;passwordRetryCache&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info) &#123;</span><br><span class="line">        String username = (String)token.getPrincipal();</span><br><span class="line">        // retry count + 1</span><br><span class="line">        Element element = passwordRetryCache.get(username);</span><br><span class="line">        if(element == null) &#123;</span><br><span class="line">            element = new Element(username , new AtomicInteger(0));</span><br><span class="line">            passwordRetryCache.put(element);</span><br><span class="line">        &#125;</span><br><span class="line">        AtomicInteger retryCount = (AtomicInteger)element.getObjectValue();</span><br><span class="line">        if(retryCount.incrementAndGet() &gt; 5) &#123;</span><br><span class="line">            // if retry count &gt; 5 throw</span><br><span class="line">            throw new ExcessiveAttemptsException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        boolean matches = super.doCredentialsMatch(token, info);</span><br><span class="line">        if(matches) &#123;</span><br><span class="line">            // clear retry count</span><br><span class="line">            passwordRetryCache.remove(username);</span><br><span class="line">        &#125;</span><br><span class="line">        return matches;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="UserRealm"><a href="#UserRealm" class="headerlink" title="UserRealm"></a>UserRealm</h4><blockquote><p>com.github.gojay001.realm</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class UserRealm extends AuthorizingRealm &#123;</span><br><span class="line">    private UserService userService = new UserServiceImpl();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123;</span><br><span class="line">        String username = (String)principals.getPrimaryPrincipal();</span><br><span class="line"></span><br><span class="line">        SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo();</span><br><span class="line">        authorizationInfo.setRoles(userService.findRoles(username));</span><br><span class="line">        authorizationInfo.setStringPermissions(userService.findPermissions(username));</span><br><span class="line"></span><br><span class="line">        return authorizationInfo;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123;</span><br><span class="line"></span><br><span class="line">        String username = (String)token.getPrincipal();</span><br><span class="line"></span><br><span class="line">        User user = userService.findByUsername(username);</span><br><span class="line"></span><br><span class="line">        //没找到帐号</span><br><span class="line">        if(user == null) &#123;</span><br><span class="line">            throw new UnknownAccountException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        //帐号锁定</span><br><span class="line">        if(Boolean.TRUE.equals(user.getLocked())) &#123;</span><br><span class="line">            throw new LockedAccountException();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 交给AuthenticatingRealm使用CredentialsMatcher进行密码匹配，如果觉得人家的不好可以自定义实现</span><br><span class="line">        return new SimpleAuthenticationInfo(</span><br><span class="line">                user.getUsername(),</span><br><span class="line">                user.getPassword(),</span><br><span class="line">                // salt=username+salt</span><br><span class="line">                ByteSource.Util.bytes(user.getCredentialsSalt()),</span><br><span class="line">                //realm name</span><br><span class="line">                getName()</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>UserRealm父类AuthorizingRealm将获取Subject相关信息分成两步</strong>：获取身份验证信息 <code>doGetAuthenticationInfo</code> 及授权信息 <code>doGetAuthorizationInfo</code> ；</li><li><strong>doGetAuthenticationInfo</strong>：首先根据传入的用户名获取User信息；在组装SimpleAuthenticationInfo信息时，需要传入：身份信息<code>用户名</code>、凭据<code>密文密码</code>、盐<code>username+salt</code>， <em>CredentialsMatcher</em> 使用盐加密传入的明文密码和此处的密文密码进行匹配。</li><li><strong>doGetAuthorizationInfo</strong>：<code>PrincipalCollection</code> 是一个身份集合，因为我们现在就一个Realm，所以直接调用getPrimaryPrincipal得到之前传入的<code>用户名</code>即可；然后根据用户名<code>调用UserService接口</code>获取角色及权限信息。</li></ul><h1 id="AuthenticationToken"><a href="#AuthenticationToken" class="headerlink" title="AuthenticationToken"></a>AuthenticationToken</h1><hr><p><img src="https://i.loli.net/2019/08/04/QgMFCh1X7aeAEI6.png" alt="6-2.png"></p><p><strong>AuthenticationToken</strong>用于收集用户提交的身份 <code>用户名</code> 及凭据 <code>密码</code> ：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface AuthenticationToken extends Serializable &#123;  </span><br><span class="line">    Object getPrincipal(); </span><br><span class="line">    Object getCredentials(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>RememberMeAuthenticationToken</strong>：提供了 <code>boolean isRememberMe()</code> <strong>记住我</strong>的功能；</li><li><strong>HostAuthenticationToken</strong>：提供了 <code>String getHost()</code> 方法用于获取用户<strong>主机</strong>的功能。</li></ul><blockquote><p>Shiro提供了一个直接拿来用的<strong>UsernamePasswordToken</strong>，用于实现用户名/密码Token组。实现了 <code>RememberMeAuthenticationToken</code> 和 <code>HostAuthenticationToken</code> ，可以实现记住我及主机验证的支持。</p></blockquote><h1 id="AuthenticationInfo"><a href="#AuthenticationInfo" class="headerlink" title="AuthenticationInfo"></a>AuthenticationInfo</h1><hr><p><img src="https://i.loli.net/2019/08/04/FxzAB7hVGevZ5cl.png" alt="6-3.png"></p><p><strong>AuthenticationInfo</strong>有两个作用：</p><ul><li>如果Realm是 <code>AuthenticatingRealm</code> 子类，则提供给 <code>AuthenticatingRealm</code> 内部使用的 <code>CredentialsMatcher</code> 进行凭据验证；（如果没有继承它需要在自己的Realm中自己实现验证）。</li><li>提供给 <code>SecurityManager</code> 来创建 <code>Subject</code> （提供身份信息）。</li></ul><blockquote><p><strong>MergableAuthenticationInfo</strong>用于提供在多Realm时<code>合并AuthenticationInfo的功能</code>，主要合并Principal。<br>比如 <code>HashedCredentialsMatcher</code> ，在验证时会判断 <code>AuthenticationInfo</code> 是否是<strong>SaltedAuthenticationInfo</strong>子类，来获取盐信息。<br><strong>Account</strong>相当于我们之前的 <code>User</code> ， <code>SimpleAccount</code> 是其一个实现。<br>其他情况一般返回<strong>SimpleAuthenticationInfo</strong>即可。</p></blockquote><h1 id="PrincipalCollection"><a href="#PrincipalCollection" class="headerlink" title="PrincipalCollection"></a>PrincipalCollection</h1><hr><p><img src="https://i.loli.net/2019/08/04/OPzuYTWsefwZ2gX.png" alt="6-4.png"></p><p>因为我们可以在Shiro中同时配置多个Realm，所以身份信息可能就有多个；因此其提供了<strong>PrincipalCollection</strong>用于<code>聚合这些身份信息</code>：  </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface PrincipalCollection extends Iterable, Serializable &#123;  </span><br><span class="line">    // 得到主要的身份</span><br><span class="line">    Object getPrimaryPrincipal();   </span><br><span class="line">    // 根据身份类型获取第一个</span><br><span class="line">    &lt;T&gt; T oneByType(Class&lt;T&gt; type);   </span><br><span class="line">    // 根据身份类型获取一组</span><br><span class="line">    &lt;T&gt; Collection&lt;T&gt; byType(Class&lt;T&gt; type);   </span><br><span class="line">    // 转换为List</span><br><span class="line">    List asList();   </span><br><span class="line">    // 转换为Set</span><br><span class="line">    Set asSet();</span><br><span class="line">    // 根据Realm名字获取</span><br><span class="line">    Collection fromRealm(String realmName);   </span><br><span class="line">    // 获取所有身份验证通过的Realm名字</span><br><span class="line">    Set&lt;String&gt; getRealmNames();   </span><br><span class="line">    // 判断是否为空</span><br><span class="line">    boolean isEmpty();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><strong>getPrimaryPrincipal</strong>：如果只有一个Principal那么直接返回即可，如果有多个Principal，则返回第一个（因为内部使用Map存储，所以可以认为是返回任意一个）；<br><strong>oneByType/byType</strong>：根据<code>凭据的类型</code>返回相应的Principal；<br><strong>fromRealm</strong>根据<code>Realm名字</code>（每个Principal都与一个Realm关联）获取相应的Principal。</p></blockquote><p><strong>MutablePrincipalCollection</strong>是一个可变的PrincipalCollection接口，即提供了如下可变方法：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface MutablePrincipalCollection extends PrincipalCollection &#123;  </span><br><span class="line">    // 添加Realm-Principal的关联</span><br><span class="line">    void add(Object principal, String realmName);   </span><br><span class="line">    // 添加一组Realm-Principal的关联</span><br><span class="line">    void addAll(Collection principals, String realmName);</span><br><span class="line">    // 添加PrincipalCollection</span><br><span class="line">    void addAll(PrincipalCollection principals);</span><br><span class="line">    // 清空</span><br><span class="line">    void clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="AuthorizationInfo"><a href="#AuthorizationInfo" class="headerlink" title="AuthorizationInfo"></a>AuthorizationInfo</h1><hr><p><img src="https://i.loli.net/2019/08/04/sCxRSW21bDFd7Gz.png" alt="6-5.png"></p><p><strong>AuthorizationInfo</strong>用于<code>聚合授权信息</code>的：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface AuthorizationInfo extends Serializable &#123;  </span><br><span class="line">    // 获取角色字符串信息</span><br><span class="line">    Collection&lt;String&gt; getRoles();</span><br><span class="line">    // 获取权限字符串信息</span><br><span class="line">    Collection&lt;String&gt; getStringPermissions(); </span><br><span class="line">    // 获取Permission对象信息</span><br><span class="line">    Collection&lt;Permission&gt; getObjectPermissions(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当我们使用 <code>AuthorizingRealm</code> 时，如果身份验证成功，在进行授权时就通过 <code>doGetAuthorizationInfo</code> 方法获取角色/权限信息用于授权验证。  </p><blockquote><p>Shiro提供了一个实现 <strong>SimpleAuthorizationInfo</strong> ，大多数时候使用这个即可。</p></blockquote><h1 id="Subject"><a href="#Subject" class="headerlink" title="Subject"></a>Subject</h1><hr><p><img src="https://i.loli.net/2019/08/04/fBt62JiSgTW1ex4.png" alt="6-6.png"></p><p><strong>Subject</strong>是Shiro的核心对象，基本所有<code>身份验证</code>、<code>授权</code>都是通过Subject完成。</p><h2 id="身份信息获取"><a href="#身份信息获取" class="headerlink" title="身份信息获取"></a>身份信息获取</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">// Primary Principal</span><br><span class="line">Object getPrincipal(); </span><br><span class="line">// PrincipalCollection</span><br><span class="line">PrincipalCollection getPrincipals();</span><br></pre></td></tr></table></figure><h2 id="身份验证"><a href="#身份验证" class="headerlink" title="身份验证"></a>身份验证</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">void login(AuthenticationToken token) throws AuthenticationException;  </span><br><span class="line">boolean isAuthenticated();  </span><br><span class="line">boolean isRemembered();</span><br></pre></td></tr></table></figure><h2 id="角色授权验证"><a href="#角色授权验证" class="headerlink" title="角色授权验证"></a>角色授权验证</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">boolean hasRole(String roleIdentifier);  </span><br><span class="line">boolean[] hasRoles(List&lt;String&gt; roleIdentifiers);  </span><br><span class="line">boolean hasAllRoles(Collection&lt;String&gt; roleIdentifiers);  </span><br><span class="line">void checkRole(String roleIdentifier) throws AuthorizationException;  </span><br><span class="line">void checkRoles(Collection&lt;String&gt; roleIdentifiers) throws AuthorizationException;  </span><br><span class="line">void checkRoles(String... roleIdentifiers) throws AuthorizationException;</span><br></pre></td></tr></table></figure><h2 id="权限授权验证"><a href="#权限授权验证" class="headerlink" title="权限授权验证"></a>权限授权验证</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">boolean isPermitted(String permission);  </span><br><span class="line">boolean isPermitted(Permission permission);  </span><br><span class="line">boolean[] isPermitted(String... permissions);  </span><br><span class="line">boolean[] isPermitted(List&lt;Permission&gt; permissions);  </span><br><span class="line">boolean isPermittedAll(String... permissions);  </span><br><span class="line">boolean isPermittedAll(Collection&lt;Permission&gt; permissions);  </span><br><span class="line">void checkPermission(String permission) throws AuthorizationException;  </span><br><span class="line">void checkPermission(Permission permission) throws AuthorizationException;  </span><br><span class="line">void checkPermissions(String... permissions) throws AuthorizationException;  </span><br><span class="line">void checkPermissions(Collection&lt;Permission&gt; permissions) throws AuthorizationException;</span><br></pre></td></tr></table></figure><h2 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">// 相当于getSession(true)</span><br><span class="line">Session getSession(); </span><br><span class="line">Session getSession(boolean create);</span><br></pre></td></tr></table></figure><blockquote><p>如果 <code>create=false</code> 如果没有会话将<code>返回null</code>；而 <code>create=true</code> 如果没有会话会强制<code>创建一个</code>。</p></blockquote><h2 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">void logout();</span><br></pre></td></tr></table></figure><h2 id="RunAs"><a href="#RunAs" class="headerlink" title="RunAs"></a>RunAs</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">void runAs(PrincipalCollection principals) throws NullPointerException, IllegalStateException;  </span><br><span class="line">boolean isRunAs();  </span><br><span class="line">PrincipalCollection getPreviousPrincipals();  </span><br><span class="line">PrincipalCollection releaseRunAs();</span><br></pre></td></tr></table></figure><p><strong>RunAs</strong>即实现<strong>允许A假设为B身份进行访问</strong>：  </p><blockquote><p>通过调用 <code>subject.runAs(b)</code> 进行访问；<br>接着调用 <code>subject.getPrincipals</code> 将获取到B的身份；<br>此时调用 <code>isRunAs</code> 将返回true，而a的身份需要通过 <code>subject.getPreviousPrincipals</code> 获取；<br>如果不需要RunAs了调用 <code>subject.releaseRunAs</code> 即可。</p></blockquote><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;V&gt; V execute(Callable&lt;V&gt; callable) throws ExecutionException;  </span><br><span class="line">void execute(Runnable runnable);  </span><br><span class="line">&lt;V&gt; Callable&lt;V&gt; associateWith(Callable&lt;V&gt; callable);  </span><br><span class="line">Runnable associateWith(Runnable runnable);</span><br></pre></td></tr></table></figure><blockquote><p>在多线程执行中需要传播到相应的线程才能获取到相应的Subject。<br>最简单的办法就是通过 <code>execute(runnable/callable实例)</code> 直接调用；<br>或者通过 <code>associateWith(runnable/callable实例)</code> 得到一个包装后的实例；<br>它们都是通过：把当前线程的Subject绑定过去；在线程执行结束后自动释放。</p></blockquote><p>Subject自己不会实现相应的身份验证/授权逻辑，而是通过DelegatingSubject委托给SecurityManager实现。</p><p>如果想自定义创建，可以通过：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">new Subject.Builder().principals(身份).authenticated(true/false).buildSubject()</span><br></pre></td></tr></table></figure><h2 id="Subject一般流程"><a href="#Subject一般流程" class="headerlink" title="Subject一般流程"></a>Subject一般流程</h2><ul><li><strong>身份验证</strong>（login）</li><li><strong>授权</strong>（hasRole*/isPermitted*或checkRole*/checkPermission*）</li><li>将相应的数据存储到<strong>会话</strong>（Session）</li><li><strong>切换身份</strong>（RunAs）/<strong>多线程</strong>身份传播</li><li><strong>退出</strong></li></ul><blockquote><p>必须的功能就是1、2、5。到目前为止我们就可以使用Shiro进行应用程序的安全控制了，但是还是缺少如对Web验证、Java方法验证等的一些简化实现。</p></blockquote><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><hr><h2 id="Realm-1"><a href="#Realm-1" class="headerlink" title="Realm"></a>Realm</h2><ul><li>Permission</li><li>Role</li><li>User</li><li>User-Role</li><li>Role-Permission</li></ul><h2 id="AuthenticationToken-1"><a href="#AuthenticationToken-1" class="headerlink" title="AuthenticationToken"></a>AuthenticationToken</h2><ul><li>Principal</li><li>Credentials</li><li>RemeberMeAuthenticationToken</li><li>HostAuthenticationToken</li><li>UsernamePasswordToken</li></ul><h2 id="AuthenticationInfo-1"><a href="#AuthenticationInfo-1" class="headerlink" title="AuthenticationInfo"></a>AuthenticationInfo</h2><ul><li>提供身份信息</li><li>提供凭据验证</li><li>SimpleAuthenticationInfo</li></ul><h2 id="PrincipalCollection-1"><a href="#PrincipalCollection-1" class="headerlink" title="PrincipalCollection"></a>PrincipalCollection</h2><ul><li>Principal</li><li>MutablePrincipalCollection</li><li>PrincipalMap</li></ul><h2 id="AuthorizationInfo-1"><a href="#AuthorizationInfo-1" class="headerlink" title="AuthorizationInfo"></a>AuthorizationInfo</h2><ul><li>Roles</li><li>StringPermissions</li><li>ObjectPermissions</li><li>SimpleAuthorizationInfo</li></ul><h2 id="Subject-1"><a href="#Subject-1" class="headerlink" title="Subject"></a>Subject</h2><ul><li>身份获取</li><li>身份验证</li><li>角色授权</li><li>权限授权</li><li>会话</li><li>退出</li><li>RunAs</li><li>多线程</li></ul><blockquote><p>参考代码： <code>https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter6</code></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Realm&quot;&gt;&lt;a href=&quot;#Realm&quot; class=&quot;headerlink&quot; title=&quot;Realm&quot;&gt;&lt;/a&gt;Realm&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;定义实体及关系&quot;&gt;&lt;a href=&quot;#定义实体及关系&quot; class=&quot;headerlink&quot; title=&quot;定义实体及关系&quot;&gt;&lt;/a&gt;定义实体及关系&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2019/08/04/9IzlrkhGxKUiEab.png&quot; alt=&quot;6-1.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Shiro" scheme="https://gojay.top/categories/JavaWeb/Shiro/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Shiro" scheme="https://gojay.top/tags/Shiro/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Shiro（五）-编码及加密</title>
    <link href="https://gojay.top/2017/11/29/%E8%B7%9F%E6%88%91%E5%AD%A6Shiro%EF%BC%88%E4%BA%94%EF%BC%89-%E7%BC%96%E7%A0%81%E5%8F%8A%E5%8A%A0%E5%AF%86/"/>
    <id>https://gojay.top/2017/11/29/跟我学Shiro（五）-编码及加密/</id>
    <published>2017-11-29T06:54:50.000Z</published>
    <updated>2020-06-17T12:26:30.908Z</updated>
    
    <content type="html"><![CDATA[<p>在涉及到<strong>密码存储</strong>问题上，应该加密/生成密码摘要存储，而不是存储明文密码。</p><a id="more"></a><h1 id="编码-解码"><a href="#编码-解码" class="headerlink" title="编码/解码"></a>编码/解码</h1><p>Shiro提供了 <strong>base64</strong> 和 <strong>16进制字符串</strong> 编码/解码的API支持，方便一些编码解码操作。Shiro内部的一些数据的存储/表示都使用了base64和16进制字符串。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String str = &quot;hello&quot;;  </span><br><span class="line">String base64Encoded = Base64.encodeToString(str.getBytes());  </span><br><span class="line">String str2 = Base64.decodeToString(base64Encoded);  </span><br><span class="line">Assert.assertEquals(str, str2);</span><br></pre></td></tr></table></figure><p>通过如上方式可以进行<code>base64编码/解码</code>操作。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String str = &quot;hello&quot;;  </span><br><span class="line">String base64Encoded = Hex.encodeToString(str.getBytes());  </span><br><span class="line">String str2 = new String(Hex.decode(base64Encoded.getBytes()));  </span><br><span class="line">Assert.assertEquals(str, str2);</span><br></pre></td></tr></table></figure><p>通过如上方式可以进行<code>16进制字符串编码/解码</code>操作。</p><blockquote><p>还有一个可能经常用到的类<code>CodecSupport</code>，提供了<code>toBytes(str, &quot;utf-8&quot;)</code> / <code>toString(bytes, &quot;utf-8&quot;)</code>用于在byte数组/String之间转换。</p></blockquote><h1 id="散列（Hash）算法"><a href="#散列（Hash）算法" class="headerlink" title="散列（Hash）算法"></a>散列（Hash）算法</h1><p><strong>散列算法</strong>一般用于<code>生成数据的摘要信息</code>，是一种不可逆的算法，一般适合存储密码之类的数据，常见的散列算法如MD5、SHA等。</p><p>一般进行散列时最好提供一个<code>salt</code>（盐），如用户名和ID（即盐）；这样散列的对象是“密码+用户名+ID”，这样生成的散列值相对来说更难破解。</p><h2 id="MD5"><a href="#MD5" class="headerlink" title="MD5"></a>MD5</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String str = &quot;hello&quot;;  </span><br><span class="line">String salt = &quot;123&quot;;  </span><br><span class="line">String md5 = new Md5Hash(str, salt).toString();//还可以转换为 toBase64()/toHex()</span><br></pre></td></tr></table></figure><p>如上代码使用<code>MD5算法</code>通过盐“123”生成MD5散列。<br>另外散列时还可以指定散列次数，如2次表示：md5(md5(str))：<br><code>new Md5Hash(str, salt, 2).toString()</code>。</p><h2 id="SHA"><a href="#SHA" class="headerlink" title="SHA"></a>SHA</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String str = &quot;hello&quot;;  </span><br><span class="line">String salt = &quot;123&quot;;  </span><br><span class="line">String sha1 = new Sha256Hash(str, salt).toString();</span><br></pre></td></tr></table></figure><p>使用<code>SHA256算法</code>生成相应的散列数据，另外还有如SHA1、SHA512算法。</p><h2 id="通用的散列支持"><a href="#通用的散列支持" class="headerlink" title="通用的散列支持"></a>通用的散列支持</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String str = &quot;hello&quot;;  </span><br><span class="line">String salt = &quot;123&quot;;  </span><br><span class="line">//内部使用MessageDigest  </span><br><span class="line">String simpleHash = new SimpleHash(&quot;SHA-1&quot;, str, salt).toString();</span><br></pre></td></tr></table></figure><p>通过调用 <code>SimpleHash</code> 时指定散列算法，其内部使用了Java的 <code>MessageDigest</code> 实现。</p><p>为了方便使用，Shiro提供了 <code>HashService</code> ，默认提供了 <code>DefaultHashService</code> 实现。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">// 默认算法SHA-512</span><br><span class="line">DefaultHashService hashService = new DefaultHashService(); </span><br><span class="line">hashService.setHashAlgorithmName(&quot;SHA-512&quot;);</span><br><span class="line">// 私盐，默认无</span><br><span class="line">hashService.setPrivateSalt(new SimpleByteSource(&quot;123&quot;)); </span><br><span class="line">// 是否生成公盐，默认false</span><br><span class="line">hashService.setGeneratePublicSalt(true);</span><br><span class="line">// 用于生成公盐。默认就这个</span><br><span class="line">hashService.setRandomNumberGenerator(new SecureRandomNumberGenerator());</span><br><span class="line">// 生成Hash值的迭代次数</span><br><span class="line">hashService.setHashIterations(1); </span><br><span class="line">  </span><br><span class="line">HashRequest request = new HashRequest.Builder()  </span><br><span class="line">            .setAlgorithmName(&quot;MD5&quot;).setSource(ByteSource.Util.bytes(&quot;hello&quot;))  </span><br><span class="line">            .setSalt(ByteSource.Util.bytes(&quot;123&quot;)).setIterations(2).build();  </span><br><span class="line">String hex = hashService.computeHash(request).toHex();</span><br></pre></td></tr></table></figure><ul><li>首先创建一个 <code>DefaultHashService</code> ，默认使用<code>SHA-512算法</code>；</li><li>可以通过 <code>hashAlgorithmName</code> 属性修改算法；</li><li>可以通过 <code>privateSalt</code> 设置一个<code>私盐</code>，其在散列时自动与用户传入的公盐混合产生一个新盐；</li><li>可以通过 <code>generatePublicSalt</code> 属性在用户没有传入公盐的情况下是否生成<code>公盐</code>；</li><li>可以设置 <code>randomNumberGenerator</code> 用于生成<code>公盐</code>；</li><li>可以设置 <code>hashIterations</code> 属性来修改默认加密<code>迭代次数</code>；</li><li>需要构建一个 <code>HashRequest</code> ，传入算法、数据、公盐、迭代次数。</li></ul><p>SecureRandomNumberGenerator用于<strong>生成一个随机数</strong>：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">SecureRandomNumberGenerator randomNumberGenerator =  </span><br><span class="line">     new SecureRandomNumberGenerator();  </span><br><span class="line">randomNumberGenerator.setSeed(&quot;123&quot;.getBytes());  </span><br><span class="line">String hex = randomNumberGenerator.nextBytes().toHex();</span><br></pre></td></tr></table></figure><h1 id="加密-解密"><a href="#加密-解密" class="headerlink" title="加密/解密"></a>加密/解密</h1><p>Shiro提供<strong>对称式加密/解密算法</strong>的支持，如AES、Blowfish等。<br>当前还没有提供对非对称加密/解密算法支持，未来版本可能提供。</p><h2 id="AES算法实现"><a href="#AES算法实现" class="headerlink" title="AES算法实现"></a>AES算法实现</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">AesCipherService aesCipherService = new AesCipherService();</span><br><span class="line">// 设置key长度</span><br><span class="line">aesCipherService.setKeySize(128); </span><br><span class="line">//生成key  </span><br><span class="line">Key key = aesCipherService.generateNewKey();  </span><br><span class="line">String text = &quot;hello&quot;;  </span><br><span class="line">//加密  </span><br><span class="line">String encrptText =   </span><br><span class="line">aesCipherService.encrypt(text.getBytes(), key.getEncoded()).toHex();  </span><br><span class="line">//解密  </span><br><span class="line">String text2 =  </span><br><span class="line"> new String(aesCipherService.decrypt(Hex.decode(encrptText), key.getEncoded()).getBytes());  </span><br><span class="line">  </span><br><span class="line">Assert.assertEquals(text, text2);</span><br></pre></td></tr></table></figure><h1 id="PasswordService-CredentialsMatcher"><a href="#PasswordService-CredentialsMatcher" class="headerlink" title="PasswordService/CredentialsMatcher"></a>PasswordService/CredentialsMatcher</h1><p>Shiro提供了 <code>PasswordService</code> 及 <code>CredentialsMatcher</code> 用于提供<strong>加密</strong>密码及<strong>验证</strong>密码服务。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface PasswordService &#123;  </span><br><span class="line">    // 输入明文密码得到密文密码</span><br><span class="line">    String encryptPassword(Object plaintextPassword) throws IllegalArgumentException;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public interface CredentialsMatcher &#123;  </span><br><span class="line">    // 匹配用户输入的token的凭证（未加密）与系统提供的凭证（已加密）  </span><br><span class="line">    boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Shiro默认提供了<strong>PasswordService</strong>实现<code>DefaultPasswordService</code>；<strong>CredentialsMatcher</strong>实现<code>PasswordMatcher</code>及<code>HashedCredentialsMatcher</code>（更强大）。</p><h2 id="DefaultPasswordService配合PasswordMatcher实现简单的密码加密与验证服务"><a href="#DefaultPasswordService配合PasswordMatcher实现简单的密码加密与验证服务" class="headerlink" title="DefaultPasswordService配合PasswordMatcher实现简单的密码加密与验证服务"></a>DefaultPasswordService配合PasswordMatcher实现简单的密码加密与验证服务</h2><h3 id="定义Realm"><a href="#定义Realm" class="headerlink" title="定义Realm"></a>定义Realm</h3><blockquote><p>com.github.gojay001.relam.MyRealm</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class MyRealm extends AuthorizingRealm &#123;  </span><br><span class="line">    private PasswordService passwordService;  </span><br><span class="line">    public void setPasswordService(PasswordService passwordService) &#123;  </span><br><span class="line">        this.passwordService = passwordService;  </span><br><span class="line">    &#125;  </span><br><span class="line">     // 省略doGetAuthorizationInfo，具体看代码   </span><br><span class="line">    @Override  </span><br><span class="line">    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123;  </span><br><span class="line">        return new SimpleAuthenticationInfo(  </span><br><span class="line">                &quot;root&quot;,  </span><br><span class="line">                passwordService.encryptPassword(&quot;root&quot;),  </span><br><span class="line">                getName());  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>为了方便，直接注入一个<code>passwordService</code>来加密密码；实际使用时需要在<code>Service层</code>使用passwordService<code>加密</code>密码<code>并存储</code>到数据库。</p></blockquote><h3 id="INI配置"><a href="#INI配置" class="headerlink" title="INI配置"></a>INI配置</h3><blockquote><p>shiro-passwordservice.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[main]  </span><br><span class="line">passwordService=org.apache.shiro.authc.credential.DefaultPasswordService  </span><br><span class="line">hashService=org.apache.shiro.crypto.hash.DefaultHashService  </span><br><span class="line">passwordService.hashService=$hashService  </span><br><span class="line">hashFormat=org.apache.shiro.crypto.hash.format.Shiro1CryptFormat  </span><br><span class="line">passwordService.hashFormat=$hashFormat  </span><br><span class="line">hashFormatFactory=org.apache.shiro.crypto.hash.format.DefaultHashFormatFactory  </span><br><span class="line">passwordService.hashFormatFactory=$hashFormatFactory  </span><br><span class="line">  </span><br><span class="line">passwordMatcher=org.apache.shiro.authc.credential.PasswordMatcher  </span><br><span class="line">passwordMatcher.passwordService=$passwordService  </span><br><span class="line">  </span><br><span class="line">myRealm=com.github.gojay001.realm.MyRealm  </span><br><span class="line">myRealm.passwordService=$passwordService  </span><br><span class="line">myRealm.credentialsMatcher=$passwordMatcher  </span><br><span class="line">securityManager.realms=$myRealm</span><br></pre></td></tr></table></figure><ul><li><strong>passwordService</strong>使用<code>DefaultPasswordService</code>，如果有必要也可以自定义；</li><li><strong>hashService</strong>定义散列密码使用的HashService，默认使用<code>DefaultHashService</code>（默认SHA-256算法）；</li><li><strong>hashFormat</strong>用于对散列出的值进行格式化，默认使用<code>Shiro1CryptFormat</code>，另外提供了Base64Format和HexFormat，对于有salt的密码请<code>自定义实现</code>ParsableHashFormat然后把salt格式化到散列值中；</li><li><strong>hashFormatFactory</strong>用于根据散列值得到散列的密码和salt；因为如果使用如SHA算法，那么会生成一个salt，此salt需要保存到散列后的值中以便之后与传入的密码比较时使用；默认使用<code>DefaultHashFormatFactory</code>；</li><li><strong>passwordMatcher</strong>使用<code>PasswordMatcher</code>，其是一个CredentialsMatcher实现；</li><li>将<code>credentialsMatcher</code>赋值给<strong>myRealm</strong>，myRealm间接继承了AuthenticatingRealm，其在调用getAuthenticationInfo方法获取到AuthenticationInfo信息后，会使用credentialsMatcher来验证凭据是否匹配，如果不匹配将抛出IncorrectCredentialsException异常。</li></ul><blockquote><p>测试用例参考： <code>com.github.gojay001.test.PasswordTest</code> ，包含JdbcRealm测试用例。<br><strong>缺点</strong>：salt保存在散列值中，没有实现如密码重试次数限制。</p></blockquote><h2 id="HashedCredentialsMatcher实现密码验证服务"><a href="#HashedCredentialsMatcher实现密码验证服务" class="headerlink" title="HashedCredentialsMatcher实现密码验证服务"></a>HashedCredentialsMatcher实现密码验证服务</h2><p>Shiro提供了<strong>CredentialsMatcher</strong>的散列实现<code>HashedCredentialsMatcher</code>，和之前的PasswordMatcher不同的是，它只用于<code>密码验证</code>，且可以<code>提供自己的盐</code>，而不是随机生成盐，且生成密码散列值的算法需要自己写，因为能提供自己的盐。</p><h3 id="生成密码散列值"><a href="#生成密码散列值" class="headerlink" title="生成密码散列值"></a>生成密码散列值</h3><p>此处我们使用<code>MD5算法</code>，“密码+盐（用户名+随机数）”的方式生成散列值：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String algorithmName = &quot;md5&quot;;  </span><br><span class="line">String username = &quot;root&quot;;  </span><br><span class="line">String password = &quot;root&quot;;  </span><br><span class="line">String salt1 = username;  </span><br><span class="line">String salt2 = new SecureRandomNumberGenerator().nextBytes().toHex();  </span><br><span class="line">int hashIterations = 2;  </span><br><span class="line">  </span><br><span class="line">SimpleHash hash = new SimpleHash(algorithmName, password, salt1 + salt2, hashIterations);  </span><br><span class="line">String encodedPassword = hash.toHex();</span><br></pre></td></tr></table></figure><p>如果要写用户模块，需要在新增用户/重置密码时使用如上算法保存密码，将<code>生成的密码</code>及<code>salt2</code>存入数据库。<br>因为我们的散列算法是：md5(密码+username+salt2)。</p><h3 id="生成Realm"><a href="#生成Realm" class="headerlink" title="生成Realm"></a>生成Realm</h3><h4 id="自定义Realm"><a href="#自定义Realm" class="headerlink" title="自定义Realm"></a>自定义Realm</h4><blockquote><p>com.github.gojay001.realm.MyRealm2</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken token) throws AuthenticationException &#123;</span><br><span class="line">    // 用户名及salt1  </span><br><span class="line">    String username = &quot;liu&quot;; </span><br><span class="line">    // 加密后的密码  </span><br><span class="line">    String password = &quot;202cb962ac59075b964b07152d234b70&quot;; </span><br><span class="line">    String salt2 = &quot;202cb962ac59075b964b07152d234b70&quot;;  </span><br><span class="line">SimpleAuthenticationInfo ai =   </span><br><span class="line">        new SimpleAuthenticationInfo(username, password, getName());  </span><br><span class="line">    // 盐是用户名+随机数</span><br><span class="line">    ai.setCredentialsSalt(ByteSource.Util.bytes(username+salt2)); </span><br><span class="line">        return ai;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此处就是把步骤1中生成的相应数据组装为 <code>SimpleAuthenticationInfo</code> ，通过 <code>SimpleAuthenticationInfo</code> 的 <code>credentialsSalt</code> 设置盐， <code>HashedCredentialsMatcher</code> 会自动识别这个盐。</p><h4 id="JdbcRealm"><a href="#JdbcRealm" class="headerlink" title="JdbcRealm"></a>JdbcRealm</h4><p>需要修改获取用户信息（包括盐）的sql： <code>“select password, password_salt from users where username = ?”</code> ；<br>而我们的盐是由 <code>username+password_salt</code> 组成，所以需要通过如下ini配置（<code>shiro-jdbc-hashedCredentialsMatcher.ini</code>）修改：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">jdbcRealm.saltStyle=COLUMN  </span><br><span class="line">jdbcRealm.authenticationQuery=select password, concat(username,password_salt) from users where username = ?  </span><br><span class="line">jdbcRealm.credentialsMatcher=$credentialsMatcher</span><br></pre></td></tr></table></figure><ul><li><strong>saltStyle</strong>表示使用<code>密码+盐</code>的机制，authenticationQuery第一列是密码，第二列是盐；</li><li>通过 <code>authenticationQuery</code> 指定密码及盐查询SQL。</li></ul><h3 id="INI配置-1"><a href="#INI配置-1" class="headerlink" title="INI配置"></a>INI配置</h3><blockquote><p>shiro-hashedCredentialsMatcher.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[main]  </span><br><span class="line">credentialsMatcher=org.apache.shiro.authc.credential.HashedCredentialsMatcher  </span><br><span class="line">credentialsMatcher.hashAlgorithmName=md5  </span><br><span class="line">credentialsMatcher.hashIterations=2  </span><br><span class="line">credentialsMatcher.storedCredentialsHexEncoded=true  </span><br><span class="line">myRealm=com.github.gojay001.realm.MyRealm2  </span><br><span class="line">myRealm.credentialsMatcher=$credentialsMatcher  </span><br><span class="line">securityManager.realms=$myRealm</span><br></pre></td></tr></table></figure><ul><li>通过 <code>credentialsMatcher.hashAlgorithmName=md5</code> 指定散列算法为md5，需要和生成密码时的一样；</li><li><code>credentialsMatcher.hashIterations=2</code> ，散列迭代次数，需要和生成密码时的意义；</li><li><code>credentialsMatcher.storedCredentialsHexEncoded=true</code> 表示是否存储散列后的密码为16进制，需要和生成密码时的一样，默认是base64；</li></ul><blockquote><p>此处最需要注意的就是 <code>HashedCredentialsMatcher</code> 的算法需要和生成密码时的算法一样。另外HashedCredentialsMatcher会自动根据 <code>AuthenticationInfo</code> 的类型是否是 <code>SaltedAuthenticationInfo</code> 来<code>获取credentialsSalt盐</code>。</p></blockquote><h3 id="密码重试次数限制"><a href="#密码重试次数限制" class="headerlink" title="密码重试次数限制"></a>密码重试次数限制</h3><p>如在1个小时内密码最多重试5次，如果尝试次数超过5次就锁定1小时，1小时后可再次重试，如果还是重试失败，可以锁定如1天，以此类推，防止密码被暴力破解。我们通过<strong>继承HashedCredentialsMatcher</strong>，且使用<strong>Ehcache</strong>记录<code>重试次数</code>和<code>超时时间</code>。</p><blockquote><p>com.github.gojay001.credentials.RetryLimitHashedCredentialsMatcher</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public boolean doCredentialsMatch(AuthenticationToken token, AuthenticationInfo info) &#123;  </span><br><span class="line">       String username = (String)token.getPrincipal();  </span><br><span class="line">        //retry count + 1  </span><br><span class="line">        Element element = passwordRetryCache.get(username);  </span><br><span class="line">        if(element == null) &#123;  </span><br><span class="line">            element = new Element(username , new AtomicInteger(0));  </span><br><span class="line">            passwordRetryCache.put(element);  </span><br><span class="line">        &#125;  </span><br><span class="line">        AtomicInteger retryCount = (AtomicInteger)element.getObjectValue();  </span><br><span class="line">        if(retryCount.incrementAndGet() &gt; 5) &#123;  </span><br><span class="line">            //if retry count &gt; 5 throw  </span><br><span class="line">            throw new ExcessiveAttemptsException();  </span><br><span class="line">        &#125;  </span><br><span class="line">  </span><br><span class="line">        boolean matches = super.doCredentialsMatch(token, info);  </span><br><span class="line">        if(matches) &#123;  </span><br><span class="line">            //clear retry count  </span><br><span class="line">            passwordRetryCache.remove(username);  </span><br><span class="line">        &#125;  </span><br><span class="line">        return matches;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如上代码逻辑比较简单，即如果密码输入<code>正确</code>，<code>清除cache</code>中的记录；否则<code>cache中的重试次数+1</code>，如果超出5次那么抛出异常表示超出重试次数了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="编码-解码-1"><a href="#编码-解码-1" class="headerlink" title="编码/解码"></a>编码/解码</h2><ul><li>Base64</li><li>Hex</li><li>Hash()</li></ul><h2 id="加密-解密-1"><a href="#加密-解密-1" class="headerlink" title="加密/解密"></a>加密/解密</h2><ul><li>对称式加密/解密</li></ul><h2 id="加密-验证"><a href="#加密-验证" class="headerlink" title="加密/验证"></a>加密/验证</h2><h3 id="PasswordService"><a href="#PasswordService" class="headerlink" title="PasswordService"></a>PasswordService</h3><ul><li>DefaultPasswordService</li></ul><h3 id="CredentialsMatcher"><a href="#CredentialsMatcher" class="headerlink" title="CredentialsMatcher"></a>CredentialsMatcher</h3><ul><li>PasswordMatcher</li><li>HashedCredentialsMatcher</li></ul><h2 id="DefaultPasswordService配合PasswordMatcher"><a href="#DefaultPasswordService配合PasswordMatcher" class="headerlink" title="DefaultPasswordService配合PasswordMatcher"></a>DefaultPasswordService配合PasswordMatcher</h2><h3 id="Realm"><a href="#Realm" class="headerlink" title="Realm"></a>Realm</h3><ul><li>自定义Realm</li><li>JdbcRealm</li></ul><h3 id="ini配置"><a href="#ini配置" class="headerlink" title="ini配置"></a>ini配置</h3><ul><li>passwordService</li><li>hashService</li><li>hashFormat</li><li>hashFormatFactory</li><li>passwordMatcher</li><li>myRealm</li></ul><h2 id="HashedCredentialsMatcher"><a href="#HashedCredentialsMatcher" class="headerlink" title="HashedCredentialsMatcher"></a>HashedCredentialsMatcher</h2><h3 id="生成Realm-1"><a href="#生成Realm-1" class="headerlink" title="生成Realm"></a>生成Realm</h3><ul><li>使用MD5算法</li></ul><h3 id="ini配置-1"><a href="#ini配置-1" class="headerlink" title="ini配置"></a>ini配置</h3><ul><li>credentialsMatcher</li><li>hashAlgorithmName</li><li>hashIterations</li><li>myRealm</li></ul><h3 id="添加密码重试次数限制"><a href="#添加密码重试次数限制" class="headerlink" title="添加密码重试次数限制"></a>添加密码重试次数限制</h3><ul><li>记录重试次数</li></ul><blockquote><p>参考代码： <code>https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter5</code></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在涉及到&lt;strong&gt;密码存储&lt;/strong&gt;问题上，应该加密/生成密码摘要存储，而不是存储明文密码。&lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Shiro" scheme="https://gojay.top/categories/JavaWeb/Shiro/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Shiro" scheme="https://gojay.top/tags/Shiro/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Shiro（四）-INI配置</title>
    <link href="https://gojay.top/2017/11/28/%E8%B7%9F%E6%88%91%E5%AD%A6Shiro%EF%BC%88%E5%9B%9B%EF%BC%89-INI%E9%85%8D%E7%BD%AE/"/>
    <id>https://gojay.top/2017/11/28/跟我学Shiro（四）-INI配置/</id>
    <published>2017-11-28T06:50:37.000Z</published>
    <updated>2020-06-17T12:26:30.909Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SecurityManager"><a href="#SecurityManager" class="headerlink" title="SecurityManager"></a>SecurityManager</h1><p>Shiro是从根对象 <code>SecurityManager</code> 进行身份验证和授权的，这个对象是线程安全且真个应用只需要一个即可，因此Shiro提供了 <code>SecurityUtils</code> 让我们绑定它为<code>全局</code>的，方便后续操作。</p><a id="more"></a><blockquote><p>因为Shiro的类都是POJO的，因此都很容易放到任何IoC容器管理。<br>但是和一般的IoC容器的区别在于，Shiro从根对象securityManager开始导航。<br>Shiro支持的依赖注入：public空参构造器对象的创建、setter依赖注入。</p></blockquote><h2 id="纯Java代码写法"><a href="#纯Java代码写法" class="headerlink" title="纯Java代码写法"></a>纯Java代码写法</h2><blockquote><p>com.github.gojay001.test.NonConfigurationCreateTest</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">DefaultSecurityManager securityManager = new DefaultSecurityManager();</span><br><span class="line"></span><br><span class="line">//设置authenticator</span><br><span class="line">ModularRealmAuthenticator authenticator = new ModularRealmAuthenticator();</span><br><span class="line">authenticator.setAuthenticationStrategy(new AtLeastOneSuccessfulStrategy());</span><br><span class="line">securityManager.setAuthenticator(authenticator);</span><br><span class="line"></span><br><span class="line">//设置authorizer</span><br><span class="line">ModularRealmAuthorizer authorizer = new ModularRealmAuthorizer();</span><br><span class="line">authorizer.setPermissionResolver(new WildcardPermissionResolver());</span><br><span class="line">securityManager.setAuthorizer(authorizer);</span><br><span class="line"></span><br><span class="line">//设置Realm</span><br><span class="line">DruidDataSource ds = new DruidDataSource();</span><br><span class="line">ds.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;);</span><br><span class="line">ds.setUrl(&quot;jdbc:mysql://localhost:3306/shiro&quot;);</span><br><span class="line">ds.setUsername(&quot;root&quot;);</span><br><span class="line">ds.setPassword(&quot;root&quot;);</span><br><span class="line"></span><br><span class="line">JdbcRealm jdbcRealm = new JdbcRealm();</span><br><span class="line">jdbcRealm.setDataSource(ds);</span><br><span class="line">jdbcRealm.setPermissionsLookupEnabled(true);</span><br><span class="line">securityManager.setRealms(Arrays.asList((Realm) jdbcRealm));</span><br><span class="line"></span><br><span class="line">//将SecurityManager设置到SecurityUtils 方便全局使用</span><br><span class="line">SecurityUtils.setSecurityManager(securityManager);</span><br><span class="line"></span><br><span class="line">Subject subject = SecurityUtils.getSubject();</span><br><span class="line"></span><br><span class="line">UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;);</span><br><span class="line">subject.login(token);</span><br><span class="line"></span><br><span class="line">Assert.assertTrue(subject.isAuthenticated());</span><br></pre></td></tr></table></figure><h2 id="等价的INI配置"><a href="#等价的INI配置" class="headerlink" title="等价的INI配置"></a>等价的INI配置</h2><h3 id="shiro-config-ini："><a href="#shiro-config-ini：" class="headerlink" title="shiro-config.ini："></a>shiro-config.ini：</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[main]</span><br><span class="line">#覆盖默认的securityManager</span><br><span class="line">#securityManager=org.apache.shiro.mgt.DefaultSecurityManager</span><br><span class="line"></span><br><span class="line">#authenticator</span><br><span class="line">authenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticator</span><br><span class="line">authenticationStrategy=org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategy</span><br><span class="line">authenticator.authenticationStrategy=$authenticationStrategy</span><br><span class="line">securityManager.authenticator=$authenticator</span><br><span class="line"></span><br><span class="line">#authorizer</span><br><span class="line">authorizer=org.apache.shiro.authz.ModularRealmAuthorizer</span><br><span class="line">permissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolver</span><br><span class="line">authorizer.permissionResolver=$permissionResolver</span><br><span class="line">securityManager.authorizer=$authorizer</span><br><span class="line"></span><br><span class="line">#realm</span><br><span class="line">dataSource=com.alibaba.druid.pool.DruidDataSource</span><br><span class="line">dataSource.driverClassName=com.mysql.jdbc.Driver</span><br><span class="line">dataSource.url=jdbc:mysql://localhost:3306/shiro</span><br><span class="line">dataSource.username=root</span><br><span class="line">dataSource.password=root</span><br><span class="line"></span><br><span class="line">jdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealm</span><br><span class="line">jdbcRealm.dataSource=$dataSource</span><br><span class="line">jdbcRealm.permissionsLookupEnabled=true</span><br><span class="line">securityManager.realms=$jdbcRealm</span><br></pre></td></tr></table></figure><blockquote><p><code>对象名=全限定类名</code>  相对于调用public无参构造器创建对象<br><code>对象名.属性名=值</code>    相当于调用setter方法设置常量值<br><code>对象名.属性名=$对象引用</code>    相当于调用setter方法设置对象引用</p></blockquote><h3 id="com-github-gojay001-test-ConfigurationCreateTest："><a href="#com-github-gojay001-test-ConfigurationCreateTest：" class="headerlink" title="com.github.gojay001.test.ConfigurationCreateTest："></a>com.github.gojay001.test.ConfigurationCreateTest：</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">Factory&lt;SecurityManager&gt; factory =</span><br><span class="line">    new IniSecurityManagerFactory(&quot;classpath:shiro-config.ini&quot;);</span><br><span class="line"></span><br><span class="line">SecurityManager securityManager = factory.getInstance();</span><br><span class="line"></span><br><span class="line">//将SecurityManager设置到SecurityUtils 方便全局使用</span><br><span class="line">SecurityUtils.setSecurityManager(securityManager);</span><br><span class="line"></span><br><span class="line">Subject subject = SecurityUtils.getSubject();</span><br><span class="line"></span><br><span class="line">UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;);</span><br><span class="line">subject.login(token);</span><br><span class="line"></span><br><span class="line">Assert.assertTrue(subject.isAuthenticated());</span><br></pre></td></tr></table></figure><blockquote><p>如上代码是从Shiro <code>INI配置</code>中获取相应的<code>securityManager</code>实例：  </p><ol><li>默认情况先创建一个名字为 <code>securityManager</code> ，类型为 <code>org.apache.shiro.mgt.DefaultSecurityManager</code> 的默认的 <code>SecurityManager</code> ，如果想<code>自定义</code>，只需要在ini配置文件中指定“securityManager=SecurityManager实现类”即可，名字必须为securityManager，它是起始的根；  </li><li><code>IniSecurityManagerFactory</code> 是创建 <code>securityManager</code> 的工厂，其需要一个ini配置文件路径，其支持<code>classpath:</code>（类路径）、<code>file:</code>（文件系统）、<code>url:</code>（网络）三种路径格式，默认是<code>文件系统</code>；  </li><li>接着获取<code>SecuriyManager实例</code>，后续步骤和之前的一样。</li></ol></blockquote><p>如上可以看出Shiro INI配置方式本身提供了一个简单的<code>IoC/DI机制</code>方便在配置文件配置，但是是从 <code>securityManager</code> 这个根对象开始导航。</p><h1 id="INI配置"><a href="#INI配置" class="headerlink" title="INI配置"></a>INI配置</h1><p><code>ini配置文件</code>类似于Java中的 <code>properties（key=value）</code> ，不过提供了将key/value分类的特性，key是每个部分不重复即可，而不是整个配置文件。如下是INI配置分类：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[main]  </span><br><span class="line">#提供了对根对象securityManager及其依赖的配置  </span><br><span class="line">securityManager=org.apache.shiro.mgt.DefaultSecurityManager  </span><br><span class="line">…………  </span><br><span class="line">securityManager.realms=$jdbcRealm  </span><br><span class="line">  </span><br><span class="line">[users]  </span><br><span class="line">#提供了对用户/密码及其角色的配置，用户名=密码，角色1，角色2  </span><br><span class="line">username=password,role1,role2  </span><br><span class="line">  </span><br><span class="line">[roles]  </span><br><span class="line">#提供了角色及权限之间关系的配置，角色=权限1，权限2  </span><br><span class="line">role1=permission1,permission2  </span><br><span class="line">  </span><br><span class="line">[urls]  </span><br><span class="line">#用于web，提供了对web url拦截相关的配置，url=拦截器[参数]，拦截器  </span><br><span class="line">/index.html = anon  </span><br><span class="line">/admin/** = authc, roles[admin], perms[&quot;permission1&quot;]</span><br></pre></td></tr></table></figure><h2 id="main-部分"><a href="#main-部分" class="headerlink" title="[main]部分"></a>[main]部分</h2><p>提供了对根对象 <strong>securityManager</strong> 及其依赖对象的配置。</p><h3 id="创建对象"><a href="#创建对象" class="headerlink" title="创建对象"></a>创建对象</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">securityManager=org.apache.shiro.mgt.DefaultSecurityManager</span><br></pre></td></tr></table></figure><p>其构造器必须是<code>public空参构造器</code>，通过反射创建相应的实例。</p><h3 id="常量值setter注入"><a href="#常量值setter注入" class="headerlink" title="常量值setter注入"></a>常量值setter注入</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">dataSource.driverClassName=com.mysql.jdbc.Driver  </span><br><span class="line">jdbcRealm.permissionsLookupEnabled=true</span><br></pre></td></tr></table></figure><p>会自动调用 <code>jdbcRealm.setPermissionsLookupEnabled(true)</code> ，对于这种常量值会自动类型转换。</p><h3 id="对象引用setter注入"><a href="#对象引用setter注入" class="headerlink" title="对象引用setter注入"></a>对象引用setter注入</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">authenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticator  </span><br><span class="line">authenticationStrategy=org.apache.shiro.authc.pam.AtLeastOneSuccessfulStrategy  </span><br><span class="line">authenticator.authenticationStrategy=$authenticationStrategy  </span><br><span class="line">securityManager.authenticator=$authenticator</span><br></pre></td></tr></table></figure><p>会自动通过 <code>securityManager.setAuthenticator(authenticator)</code> 注入引用依赖。</p><h3 id="嵌套属性setter注入"><a href="#嵌套属性setter注入" class="headerlink" title="嵌套属性setter注入"></a>嵌套属性setter注入</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">securityManager.authenticator.authenticationStrategy=$authenticationStrategy</span><br></pre></td></tr></table></figure><p>支持这种嵌套方式的setter注入。</p><h3 id="byte数组setter注入"><a href="#byte数组setter注入" class="headerlink" title="byte数组setter注入"></a>byte数组setter注入</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">#base64 byte[]  </span><br><span class="line">authenticator.bytes=aGVsbG8=  </span><br><span class="line">#hex byte[]  </span><br><span class="line">authenticator.bytes=0x68656c6c6f</span><br></pre></td></tr></table></figure><p>默认需要使用Base64进行编码，也可以使用0x十六进制。</p><h3 id="Array-Set-List-setter注入"><a href="#Array-Set-List-setter注入" class="headerlink" title="Array/Set/List setter注入"></a>Array/Set/List setter注入</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">authenticator.array=1,2,3  </span><br><span class="line">authenticator.set=$jdbcRealm,$jdbcRealm</span><br></pre></td></tr></table></figure><p>多个之间通过“，”分割。</p><h3 id="Map-setter注入"><a href="#Map-setter注入" class="headerlink" title="Map setter注入"></a>Map setter注入</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">authenticator.map=$jdbcRealm:$jdbcRealm,1:1,key:abc</span><br></pre></td></tr></table></figure><p>格式是： <code>map=key：value，key：value</code> ，可以注入常量及引用值，常量的话都看作字符串（即使有泛型也不会自动造型）。  </p><h3 id="实例化-注入顺序"><a href="#实例化-注入顺序" class="headerlink" title="实例化/注入顺序"></a>实例化/注入顺序</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">realm=Realm1  </span><br><span class="line">realm=Realm12  </span><br><span class="line">  </span><br><span class="line">authenticator.bytes=aGVsbG8=  </span><br><span class="line">authenticator.bytes=0x68656c6c6f</span><br></pre></td></tr></table></figure><p>后边的<code>覆盖</code>前边的注入。</p><h2 id="users-部分"><a href="#users-部分" class="headerlink" title="[users]部分"></a>[users]部分</h2><p>配置用户名/密码及其角色，格式：<code>用户名=密码，角色1，角色2</code>，角色部分可省略。如：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[users]</span><br><span class="line">root=root,role1,role2</span><br><span class="line">gojay=test</span><br></pre></td></tr></table></figure><p>密码一般生成其摘要/加密存储。</p><h2 id="roles-部分"><a href="#roles-部分" class="headerlink" title="[roles]部分"></a>[roles]部分</h2><p>配置角色及权限之间的关系，格式：<code>角色=权限1，权限2</code>；如：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[roles]  </span><br><span class="line">role1=user:create,user:update  </span><br><span class="line">role2=*</span><br></pre></td></tr></table></figure><p>如果只有角色没有对应的权限，可以不配roles。</p><h2 id="urls-部分"><a href="#urls-部分" class="headerlink" title="[urls]部分"></a>[urls]部分</h2><p>配置url及相应的拦截器之间的关系，格式：<code>url=拦截器[参数]，拦截器[参数]</code>，如：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[urls]  </span><br><span class="line">/admin/** = authc, roles[admin], perms[&quot;permission1&quot;]</span><br></pre></td></tr></table></figure><blockquote><p>参考代码： <code>https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter4</code></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;SecurityManager&quot;&gt;&lt;a href=&quot;#SecurityManager&quot; class=&quot;headerlink&quot; title=&quot;SecurityManager&quot;&gt;&lt;/a&gt;SecurityManager&lt;/h1&gt;&lt;p&gt;Shiro是从根对象 &lt;code&gt;SecurityManager&lt;/code&gt; 进行身份验证和授权的，这个对象是线程安全且真个应用只需要一个即可，因此Shiro提供了 &lt;code&gt;SecurityUtils&lt;/code&gt; 让我们绑定它为&lt;code&gt;全局&lt;/code&gt;的，方便后续操作。&lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Shiro" scheme="https://gojay.top/categories/JavaWeb/Shiro/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Shiro" scheme="https://gojay.top/tags/Shiro/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Shiro（三）-授权</title>
    <link href="https://gojay.top/2017/11/23/%E8%B7%9F%E6%88%91%E5%AD%A6Shiro%EF%BC%88%E4%B8%89%EF%BC%89-%E6%8E%88%E6%9D%83/"/>
    <id>https://gojay.top/2017/11/23/跟我学Shiro（三）-授权/</id>
    <published>2017-11-23T03:41:24.000Z</published>
    <updated>2020-06-17T12:26:30.907Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><strong>授权</strong>：也叫访问控制，即在应用中控制谁能访问哪些资源（如访问页面/编辑数据/页面操作等）。在授权中需了解的几个关键对象：<code>主体</code>（Subject）、<code>资源</code>（Resource）、<code>权限</code>（Permission）、<code>角色</code>（Role）。</p><a id="more"></a><ul><li><strong>主体</strong>：即访问应用的用户，在Shiro中使用Subject代表该用户。用户只有授权后才允许访问相应的资源。</li><li><strong>资源</strong>：在应用中用户可以访问的任何东西，比如访问JSP页面、查看/编辑某些数据、访问某个业务方法、打印文本等等都是资源。用户只要授权后才能访问。</li><li><strong>权限</strong>：安全策略中的原子授权单位，通过权限我们可以表示在应用中用户有没有操作某个资源的权力。</li><li><strong>角色</strong>：角色代表了操作集合，可以理解为权限的集合，一般情况下我们会赋予用户角色而不是权限，即这样用户可以拥有一组权限，赋予权限时比较方便。</li><li><strong>隐式角色</strong>：即直接通过角色来验证用户有没有操作权限。</li><li><strong>显式角色</strong>：在程序中通过权限控制谁能访问某个资源，角色聚合一组权限集合。</li></ul><blockquote><p><strong>了解更多</strong>：搜索<code>“RBAC”</code>和<code>“RBAC新解”</code>分别了解<code>“基于角色的访问控制”</code>和<code>“基于资源的访问控制”</code>。</p></blockquote><h1 id="授权方式"><a href="#授权方式" class="headerlink" title="授权方式"></a>授权方式</h1><blockquote><p>Shiro支持三种方式的授权。</p></blockquote><h2 id="编程式"><a href="#编程式" class="headerlink" title="编程式"></a>编程式</h2><p>通过写if/else授权代码块完成： </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">Subject subject = SecurityUtils.getSubject();  </span><br><span class="line">if(subject.hasRole(“admin”)) &#123;  </span><br><span class="line">    //有权限  </span><br><span class="line">&#125; else &#123;  </span><br><span class="line">    //无权限  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="注解式"><a href="#注解式" class="headerlink" title="注解式"></a>注解式</h2><p>通过在执行的Java方法上放置相应的注解完成： </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@RequiresRoles(&quot;admin&quot;)  </span><br><span class="line">public void hello() &#123;  </span><br><span class="line">    //有权限  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>没有权限将抛出相应的异常。</p><h2 id="JSP-GSP标签"><a href="#JSP-GSP标签" class="headerlink" title="JSP/GSP标签"></a>JSP/GSP标签</h2><p>在JSP/GSP页面通过相应的标签完成：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;shiro:hasRole name=&quot;admin&quot;&gt;  </span><br><span class="line">&lt;!— 有权限 —&gt;  </span><br><span class="line">&lt;/shiro:hasRole&gt;</span><br></pre></td></tr></table></figure><h1 id="授权"><a href="#授权" class="headerlink" title="授权"></a>授权</h1><h2 id="基于角色的访问控制（隐式角色）"><a href="#基于角色的访问控制（隐式角色）" class="headerlink" title="基于角色的访问控制（隐式角色）"></a>基于角色的访问控制（隐式角色）</h2><h3 id="在ini配置文件配置用户拥有的角色"><a href="#在ini配置文件配置用户拥有的角色" class="headerlink" title="在ini配置文件配置用户拥有的角色"></a>在ini配置文件配置用户拥有的角色</h3><blockquote><p>shiro-role.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[users]</span><br><span class="line">root=root,role1,role2</span><br><span class="line">gojay=test,role1</span><br></pre></td></tr></table></figure><blockquote><p><strong>规则</strong>：<code>“用户名=密码,角色1，角色2”</code>，如果需要在应用中判断用户是否有相应角色，就需要在相应的Realm中返回角色信息；也就是说<code>Shiro不负责维护用户-角色信息</code>，需要应用提供，Shiro只是提供相应的接口方便验证。</p></blockquote><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><blockquote><p>com.github.gojay001.test.RoleTest</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testHasRole() &#123;</span><br><span class="line">    login(&quot;classpath:shiro-role.ini&quot;, &quot;root&quot;, &quot;root&quot;);</span><br><span class="line">    // 判断拥有角色：role1</span><br><span class="line">    Assert.assertTrue(subject().hasRole(&quot;role1&quot;));</span><br><span class="line">    // 判断拥有角色：role1 and role2</span><br><span class="line">    Assert.assertTrue(subject().hasAllRoles(Arrays.asList(&quot;role1&quot;, &quot;role2&quot;)));</span><br><span class="line">    // 判断拥有角色：role1 and role2 and !role3</span><br><span class="line">    boolean[] result = subject().hasRoles(Arrays.asList(&quot;role1&quot;, &quot;role2&quot;, &quot;role3&quot;));</span><br><span class="line">    Assert.assertEquals(true, result[0]);</span><br><span class="line">    Assert.assertEquals(true, result[1]);</span><br><span class="line">    Assert.assertEquals(false, result[2]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Shiro提供了<code>hasRole/hasAllRoles</code>用于判断用户是否拥有某个角色/某些权限；但是没有提供如hashAnyRole用于判断是否有某些权限中的某一个。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test(expected = UnauthorizedException.class)</span><br><span class="line">public void testCheckRole() &#123;</span><br><span class="line">    login(&quot;classpath:shiro-role.ini&quot;, &quot;root&quot;, &quot;root&quot;);</span><br><span class="line">    // 断言拥有角色：role1</span><br><span class="line">    subject().checkRole(&quot;role1&quot;);</span><br><span class="line">    // 断言拥有角色：role1 and role3 失败抛出异常</span><br><span class="line">    subject().checkRoles(&quot;role1&quot;, &quot;role3&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Shiro提供的<code>checkRole/checkRoles</code>和<code>hasRole/hasAllRoles</code>不同的地方是它在判断为假的情况下会<code>抛出UnauthorizedException异常</code>。</p></blockquote><p><strong>基于角色的访问控制</strong>（即隐式角色）的缺点：如果很多地方进行了角色判断，但是有一天不需要了那么就需要修改相应代码把<code>所有相关的地方</code>进行删除；这就是粗粒度造成的问题。</p><h2 id="基于资源的访问控制（显式角色）"><a href="#基于资源的访问控制（显式角色）" class="headerlink" title="基于资源的访问控制（显式角色）"></a>基于资源的访问控制（显式角色）</h2><h3 id="在ini配置文件配置用户拥有的角色及角色-权限关系"><a href="#在ini配置文件配置用户拥有的角色及角色-权限关系" class="headerlink" title="在ini配置文件配置用户拥有的角色及角色-权限关系"></a>在ini配置文件配置用户拥有的角色及角色-权限关系</h3><blockquote><p>shiro-permission.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[users]</span><br><span class="line">root=root,role1,role2</span><br><span class="line">gojay=test,role1</span><br><span class="line">[roles]</span><br><span class="line">role1=user:create,user:update</span><br><span class="line">role2=user:create,user:delete</span><br></pre></td></tr></table></figure><blockquote><p><strong>规则</strong>：<code>“用户名=密码，角色1，角色2”“角色=权限1，权限2”</code>，即首先根据用户名找到角色，然后根据角色再找到权限；即角色是权限集合；Shiro同样不进行权限的维护，需要我们通过Realm返回相应的权限信息。只需要维护“用户——角色”之间的关系即可。</p></blockquote><h3 id="测试用例-1"><a href="#测试用例-1" class="headerlink" title="测试用例"></a>测试用例</h3><blockquote><p>com.github.gojay001.test.PermissionTest</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testIsPermitted() &#123;</span><br><span class="line">    login(&quot;classpath:shiro-permission.ini&quot;, &quot;root&quot;, &quot;root&quot;);</span><br><span class="line">    // 判断拥有权限：user:create</span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;user:create&quot;));</span><br><span class="line">    // 判断拥有权限：user:update and user:delete</span><br><span class="line">    Assert.assertTrue(subject().isPermittedAll(&quot;user:update&quot;, &quot;user:delete&quot;));</span><br><span class="line">    // 判断没有权限：user:view</span><br><span class="line">    Assert.assertFalse(subject().isPermitted(&quot;user:view&quot;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>Shiro提供了<code>isPermitted</code>和<code>isPermittedAll</code>用于判断用户是否拥有某个权限或所有权限，也<code>没有提供如isPermittedAny</code>用于判断拥有某一个权限的接口。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test(expected = UnauthorizedException.class)</span><br><span class="line">public void testCheckPermission() &#123;</span><br><span class="line">    login(&quot;classpath:shiro-permission.ini&quot;, &quot;root&quot;, &quot;root&quot;);</span><br><span class="line">    // 断言拥有权限：user:create</span><br><span class="line">    subject().checkPermission(&quot;user:create&quot;);</span><br><span class="line">    // 断言拥有权限：user:delete and user:update</span><br><span class="line">    subject().checkPermissions(&quot;user:delete&quot;, &quot;user:update&quot;);</span><br><span class="line">    // 断言拥有权限：user:view 失败抛出异常</span><br><span class="line">    subject().checkPermissions(&quot;user:view&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>checkPermissions</code>失败的情况下会抛出UnauthorizedException异常。</p></blockquote><p><strong>基于资源的访问控制</strong>（显式角色），也可以叫基于权限的访问控制，这种方式的一般规则是<code>“资源标识符：操作”</code>，即是资源级别的粒度；这种方式的好处就是如果要修改基本都是一个资源级别的修改，不会对其他模块代码产生影响，粒度小。但是实现起来可能稍微复杂点，需要维护“用户——角色，角色——权限（资源：操作）”之间的关系。 </p><h1 id="Permission"><a href="#Permission" class="headerlink" title="Permission"></a>Permission</h1><p><strong>字符串通配符权限</strong><br><strong>规则</strong>：<code>“资源标识符：操作：对象实例ID”</code>  即对哪个资源的哪个实例可以进行什么操作。其默认支持通配符权限字符串，<code>“:”</code>表示资源/操作/实例的分割；<code>“,”</code>表示操作的分割；<code>“*”</code>表示任意资源/操作/实例。</p><h2 id="单个资源单个权限"><a href="#单个资源单个权限" class="headerlink" title="单个资源单个权限"></a>单个资源单个权限</h2><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;system:user:update&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>用户拥有资源<code>“system:user”</code>的<code>“update”</code>权限。</p></blockquote><h2 id="单个资源多个权限"><a href="#单个资源多个权限" class="headerlink" title="单个资源多个权限"></a>单个资源多个权限</h2><p>ini配置文件：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role41=system:user:update,system:user:delete</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;system:user:update&quot;, &quot;system:user:delete&quot;);</span><br></pre></td></tr></table></figure><p>可以简写为：<code>&quot;system:user:update,delete&quot;</code></p><blockquote><p>用户拥有资源<code>“system:user”</code>的<code>“update”</code>和<code>“delete”</code>权限。</p></blockquote><h2 id="单个资源全部权限"><a href="#单个资源全部权限" class="headerlink" title="单个资源全部权限"></a>单个资源全部权限</h2><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role51=&quot;system:user:create,update,delete,view&quot;</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;system:user:create,delete,update:view&quot;);</span><br></pre></td></tr></table></figure><p>可以简写为：<code>system:user:*</code></p><blockquote><p>用户拥有资源<code>“system:user”</code>的<code>“create”</code>、<code>“update”</code>、<code>“delete”</code>和<code>“view”</code>所有权限。</p></blockquote><h2 id="所有资源全部权限"><a href="#所有资源全部权限" class="headerlink" title="所有资源全部权限"></a>所有资源全部权限</h2><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role61=*:view</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;user:view&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>用户拥有所有资源的<code>“view”</code>所有权限。</p></blockquote><h2 id="实例级别的权限"><a href="#实例级别的权限" class="headerlink" title="实例级别的权限"></a>实例级别的权限</h2><h3 id="单个实例单个权限"><a href="#单个实例单个权限" class="headerlink" title="单个实例单个权限"></a>单个实例单个权限</h3><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role71=user:view:1</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;user:view:1&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>对资源user的1实例拥有view权限。</p></blockquote><h3 id="单个实例多个权限"><a href="#单个实例多个权限" class="headerlink" title="单个实例多个权限"></a>单个实例多个权限</h3><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role72=&quot;user:update,delete:1&quot;</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;user:delete,update:1&quot;);  </span><br><span class="line">subject().checkPermissions(&quot;user:update:1&quot;, &quot;user:delete:1&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>对资源user的1实例拥有update、delete权限。</p></blockquote><h3 id="单个实例所有权限"><a href="#单个实例所有权限" class="headerlink" title="单个实例所有权限"></a>单个实例所有权限</h3><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role73=user:*:1</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;user:update:1&quot;, &quot;user:delete:1&quot;, &quot;user:view:1&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>对资源user的1实例拥有所有权限。</p></blockquote><h3 id="所有实例单个权限"><a href="#所有实例单个权限" class="headerlink" title="所有实例单个权限"></a>所有实例单个权限</h3><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role74=user:auth:*</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;user:auth:1&quot;, &quot;user:auth:2&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>对资源user的1实例拥有所有权限。</p></blockquote><h3 id="所有实例所有权限"><a href="#所有实例所有权限" class="headerlink" title="所有实例所有权限"></a>所有实例所有权限</h3><p>ini配置：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">role75=user:*:*</span><br></pre></td></tr></table></figure><p>通过判断：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermissions(&quot;user:view:1&quot;, &quot;user:auth:2&quot;);</span><br></pre></td></tr></table></figure><blockquote><p>对资源user的1实例拥有所有权限。</p></blockquote><h2 id="Shiro对权限字符串缺失部分的处理"><a href="#Shiro对权限字符串缺失部分的处理" class="headerlink" title="Shiro对权限字符串缺失部分的处理"></a>Shiro对权限字符串缺失部分的处理</h2><ul><li>如<code>user:view</code> 等价于 <code>user:view:*</code>；<br><code>organization</code> 等价于 <code>organization:*</code> 或者 <code>organization:*:*</code>。<br>可以这么理解，这种方式实现了<strong>前缀匹配</strong>。</li><li>如<code>user:*</code> 可以匹配 <code>user:delete</code>；<br><code>user:delete</code> 可以匹配 <code>user:delete:1</code>；<br><code>user:*:1</code> 可以匹配 <code>user:view:1</code>；<br><code>user</code> 可以匹配 <code>user:view</code> 或 <code>user:view:1</code>等。<br>即*可以匹配所有，不加*可以进行前缀匹配；</li><li>如<code>*:view</code> 不能匹配 <code>system:user:view</code>；需要使用 <code>*:*:view</code>；<br>即<strong>后缀匹配</strong>必须指定前缀（多个冒号就需要多个*来匹配）。</li></ul><h2 id="WildcardPermission"><a href="#WildcardPermission" class="headerlink" title="WildcardPermission"></a>WildcardPermission</h2><p>如下两种方式是等价的：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">subject().checkPermission(&quot;menu:view:1&quot;);  </span><br><span class="line">subject().checkPermission(new WildcardPermission(&quot;menu:view:1&quot;));</span><br></pre></td></tr></table></figure><blockquote><p>因此没什么必要的话使用字符串更方便。</p></blockquote><h2 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a>性能问题</h2><p><code>通配符匹配</code>方式比<code>字符串匹配</code>来说是更复杂的，因此需要花费更长时间，但是一般系统的权限不会太多，且可以配合缓存来提供其性能，如果这样性能还达不到要求我们可以实现位操作算法实现性能更好的权限匹配。另外实例级别的权限验证如果数据量太大也不建议使用，可能造成查询权限及匹配变慢。可以考虑比如在sql查询时加上权限字符串之类的方式在查询时就完成了权限匹配。</p><h1 id="授权流程"><a href="#授权流程" class="headerlink" title="授权流程"></a>授权流程</h1><p><img src="https://i.loli.net/2019/08/04/uMhPXtxenUl51fv.png" alt="3-1.png"></p><h2 id="流程如下："><a href="#流程如下：" class="headerlink" title="流程如下："></a>流程如下：</h2><blockquote><ol><li>首先调用<code>Subject.isPermitted*/hasRole*</code>接口，其会委托给SecurityManager，而<code>SecurityManager</code>接着会委托给<code>Authorizer</code>；</li><li><code>Authorizer</code>是真正的授权者；如果我们调用如<code>isPermitted(“user:view”)</code>，其首先会通过<code>PermissionResolver</code>把字符串转换成相应的<code>Permission实例</code>；</li><li>在进行授权之前，其会<code>调用相应的Realm</code>获取Subject相应的<code>角色/权限</code>用于匹配传入的角色/权限；</li><li><code>Authorizer</code>会<code>判断</code>Realm的角色/权限是否和传入的<code>匹配</code>，如果有<code>多个Realm</code>，会委托给<code>ModularRealmAuthorizer</code>进行循环判断，如果匹配如isPermitted<em>/hasRole</em>会返回true，否则返回false表示授权失败。</li></ol></blockquote><ul><li>ModularRealmAuthorizer进行多Realm匹配流程：</li></ul><blockquote><ol><li>首先检查相应的<code>Realm</code>是否实现了<code>Authorizer</code>；</li><li>如果实现了Authorizer，那么接着调用其相应的<code>isPermitted*/hasRole*接口</code>进行匹配；</li><li>如果有一个<code>Realm匹配</code>那么将返回true，否则返回false。</li></ol></blockquote><ul><li>如果Realm进行授权的话，应该继承AuthorizingRealm，其流程是：</li></ul><blockquote><ol><li>如果调用<code>hasRole*</code>，则直接获取<code>AuthorizationInfo.getRoles()</code>与传入的角色比较即可；</li><li>如果调用如<code>isPermitted(“user:view”)</code>，首先通过<code>PermissionResolver</code>将权限字符串转换成相应的<code>Permission实例</code>，默认使用<code>WildcardPermissionResolver</code>，即转换为通配符的<code>WildcardPermission</code>；</li><li>通过<code>AuthorizationInfo.getObjectPermissions()</code>得到<code>Permission实例集合</code>；通过<code>AuthorizationInfo. getStringPermissions()</code>得到字符串集合并通过PermissionResolver解析为<code>Permission实例</code>；然后获取用户的角色，并通过<code>RolePermissionResolver</code>解析角色对应的<code>权限集合</code>（默认没有实现，可以自己提供）；</li><li>接着调用<code>Permission. implies(Permission p)</code>逐个与传入的权限比较，如果有<code>匹配</code>的则返回true，否则false。</li></ol></blockquote><h1 id="Authorizer"><a href="#Authorizer" class="headerlink" title="Authorizer"></a>Authorizer</h1><p><strong>Authorizer</strong>的职责是进行授权（访问控制），提供了相应的角色/权限判断接口。<code>SecurityManager</code>继承了<code>Authorizer接口</code>，且提供了<code>ModularRealmAuthorizer</code>用于多Realm时的授权匹配。<br><strong>PermissionResolver</strong>用于解析权限字符串到Permission实例。<br><strong>RolePermissionResolver</strong>用于根据角色解析相应的权限集合。</p><ul><li><p>可以通过如下ini配置更改<code>Authorizer</code>实现：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">authorizer=org.apache.shiro.authz.ModularRealmAuthorizer  </span><br><span class="line">securityManager.authorizer=$authorizer</span><br></pre></td></tr></table></figure></li><li><p>设置<code>ModularRealmAuthorizer</code>的<code>permissionResolver</code>，其会自动设置到相应的Realm上（其实现了<code>PermissionResolverAware</code>接口），如：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">permissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolver  </span><br><span class="line">authorizer.permissionResolver=$permissionResolver</span><br></pre></td></tr></table></figure></li><li><p>设置<code>ModularRealmAuthorizer</code>的<code>rolePermissionResolver</code>，其会自动设置到相应的Realm上（其实现了<code>RolePermissionResolverAware</code>接口），如：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">rolePermissionResolver=com.github.gojay001.permission.MyRolePermissionResolver  </span><br><span class="line">authorizer.rolePermissionResolver=$rolePermissionResolver</span><br></pre></td></tr></table></figure></li></ul><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="ini配置"><a href="#ini配置" class="headerlink" title="ini配置"></a>ini配置</h3><blockquote><p>shiro-jdbc-authorizer.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[main]</span><br><span class="line"># 自定义authorizer</span><br><span class="line">authorizer=org.apache.shiro.authz.ModularRealmAuthorizer</span><br><span class="line"># 自定义permissionResolver</span><br><span class="line"># permissionResolver=org.apache.shiro.authz.permission.WildcardPermissionResolver</span><br><span class="line">permissionResolver=com.github.gojay001.permission.BitAndWildPermissionResolver</span><br><span class="line">authorizer.permissionResolver=$permissionResolver</span><br><span class="line"># 自定义rolePermissionResolver</span><br><span class="line">rolePermissionResolver=com.github.gojay001.permission.MyRolePermissionResolver</span><br><span class="line">authorizer.rolePermissionResolver=$rolePermissionResolver</span><br><span class="line"></span><br><span class="line">securityManager.authorizer=$authorizer</span><br><span class="line"></span><br><span class="line"># 自定义realm 一定要放在securityManager.authorizer赋值之后</span><br><span class="line"># 因为调用setRealms会将realms设置给authorizer，并给各个Realm设置permissionResolver和rolePermissionResolver</span><br><span class="line">jdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealm</span><br><span class="line">dataSource=com.alibaba.druid.pool.DruidDataSource</span><br><span class="line">dataSource.driverClassName=com.mysql.jdbc.Driver</span><br><span class="line">dataSource.url=jdbc:mysql://localhost:3306/shiro</span><br><span class="line">dataSource.username=root</span><br><span class="line">dataSource.password=root</span><br><span class="line">jdbcRealm.dataSource=$dataSource</span><br><span class="line">jdbcRealm.permissionsLookupEnabled=true</span><br><span class="line">securityManager.realms=$jdbcRealm</span><br></pre></td></tr></table></figure><blockquote><p>不能使用<code>IniSecurityManagerFactory</code>创建的<code>IniRealm</code>，因为其初始化顺序的问题可能造成后续的初始化Permission造成影响。</p></blockquote><h3 id="定义BitAndWildPermissionResolver及BitPermission"><a href="#定义BitAndWildPermissionResolver及BitPermission" class="headerlink" title="定义BitAndWildPermissionResolver及BitPermission"></a>定义BitAndWildPermissionResolver及BitPermission</h3><blockquote><p><strong>BitPermission</strong>用于实现位移方式的权限，如规则是：<br>权限字符串格式：<code>+资源字符串+权限位+实例ID</code>；以+开头中间通过+分割；权限：<code>0 表示所有权限</code>；<code>1 新增</code>（二进制：0001）、<code>2 修改</code>（二进制：0010）、<code>4 删除</code>（二进制：0100）、<code>8 查看</code>（二进制：1000）；如 <code>+user+10</code> 表示对资源user拥有<code>修改/查看</code>权限。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class BitPermission implements Permission &#123;</span><br><span class="line">    private String resourceIdentify;</span><br><span class="line">    private int permissionBit;</span><br><span class="line">    private String instanceId;</span><br><span class="line"></span><br><span class="line">    public BitPermission(String permissionString) &#123;</span><br><span class="line">        String[] array = permissionString.split(&quot;\\+&quot;);</span><br><span class="line"></span><br><span class="line">        if (array.length &gt; 1) &#123;</span><br><span class="line">            resourceIdentify = array[1];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (StringUtils.isEmpty(resourceIdentify)) &#123;</span><br><span class="line">            resourceIdentify = &quot;*&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (array.length &gt; 2) &#123;</span><br><span class="line">            permissionBit = Integer.valueOf(array[2]);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (array.length &gt; 3) &#123;</span><br><span class="line">            instanceId = array[3];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if (StringUtils.isEmpty(instanceId)) &#123;</span><br><span class="line">            instanceId = &quot;*&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public boolean implies(Permission permission) &#123;</span><br><span class="line">        if(!(permission instanceof BitPermission)) &#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line">        BitPermission other = (BitPermission) permission;</span><br><span class="line"></span><br><span class="line">        if(!(&quot;*&quot;.equals(this.resourceIdentify) || this.resourceIdentify.equals(other.resourceIdentify))) &#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if(!(this.permissionBit ==0 || (this.permissionBit &amp; other.permissionBit) != 0)) &#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        if(!(&quot;*&quot;.equals(this.instanceId) || this.instanceId.equals(other.instanceId))) &#123;</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public String toString() &#123;</span><br><span class="line">        return &quot;BitPermission&#123;&quot; +</span><br><span class="line">                &quot;resourceIdentify=&apos;&quot; + resourceIdentify + &apos;\&apos;&apos; +</span><br><span class="line">                &quot;, permissionBit=&quot; + permissionBit +</span><br><span class="line">                &quot;, instanceId=&apos;&quot; + instanceId + &apos;\&apos;&apos; +</span><br><span class="line">                &apos;&#125;&apos;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Permission接口</strong>提供了<code>boolean implies(Permission p)方法</code>用于判断权限匹配的；</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class BitAndWildPermissionResolver implements PermissionResolver &#123;  </span><br><span class="line">    @Override  </span><br><span class="line">    public Permission resolvePermission(String permissionString) &#123;  </span><br><span class="line">        if(permissionString.startsWith(&quot;+&quot;)) &#123;  </span><br><span class="line">            return new BitPermission(permissionString);  </span><br><span class="line">        &#125;  </span><br><span class="line">        return new WildcardPermission(permissionString);  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>BitAndWildPermissionResolver</strong>实现了<code>PermissionResolver接口</code>，并根据权限字符串是否以“+”开头来解析权限字符串为BitPermission或WildcardPermission。</p><h3 id="定义MyRolePermissionResolver"><a href="#定义MyRolePermissionResolver" class="headerlink" title="定义MyRolePermissionResolver"></a>定义MyRolePermissionResolver</h3><p><strong>RolePermissionResolver</strong>用于根据角色字符串来解析得到权限集合。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class MyRolePermissionResolver implements RolePermissionResolver &#123;  </span><br><span class="line">    @Override  </span><br><span class="line">    public Collection&lt;Permission&gt; resolvePermissionsInRole(String roleString) &#123;  </span><br><span class="line">        if(&quot;role1&quot;.equals(roleString)) &#123;  </span><br><span class="line">            return Arrays.asList((Permission)new WildcardPermission(&quot;menu:*&quot;));  </span><br><span class="line">        &#125;  </span><br><span class="line">        return null;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="自定义Realm"><a href="#自定义Realm" class="headerlink" title="自定义Realm"></a>自定义Realm</h3><p>使用<strong>JdbcRealm</strong>，需要做的操作如下：</p><ul><li>执行<code>sql/shiro-init-data.sql</code> 插入相关的权限数据；</li><li>使用<code>shiro-jdbc-authorizer.ini配置文件</code>，需要<code>设置jdbcRealm.permissionsLookupEnabled为true</code>来开启权限查询。</li></ul><blockquote><p>这里也可以自定义实现Realm，可参考com.github.gojay001.realm.MyRealm</p></blockquote><h3 id="测试用例-2"><a href="#测试用例-2" class="headerlink" title="测试用例"></a>测试用例</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testIsPermitted2() &#123;</span><br><span class="line">    login(&quot;classpath:shiro-jdbc-authorizer.ini&quot;, &quot;root&quot;, &quot;root&quot;);</span><br><span class="line">    // 判断拥有权限：user:create</span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;user1:update&quot;));</span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;user2:update&quot;));</span><br><span class="line">    // 通过二进制位的方式表示权限</span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;+user1+2&quot;));// 新增权限</span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;+user1+8&quot;));// 查看权限</span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;+user2+10&quot;));// 新增及查看</span><br><span class="line"></span><br><span class="line">    Assert.assertFalse(subject().isPermitted(&quot;+user1+4&quot;));// 没有删除权限</span><br><span class="line"></span><br><span class="line">    Assert.assertTrue(subject().isPermitted(&quot;menu:view&quot;));// 通过MyRolePermissionResolver解析得到的权限</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="授权流程-1"><a href="#授权流程-1" class="headerlink" title="授权流程"></a>授权流程</h2><ul><li>Subject.isPermitted*/hasRole*</li><li>SecurityManager</li><li>Authorizer</li><li>PermissionResolver/RolePermissionResolver/Permission</li><li>Realm</li></ul><h2 id="Realm"><a href="#Realm" class="headerlink" title="Realm"></a>Realm</h2><ul><li>user</li><li>role</li><li>permission</li></ul><h2 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h2><ul><li>Authorizer</li><li>PermissionResolver</li><li>RolePermissionResolver</li><li>Realm</li></ul><h2 id="Subject"><a href="#Subject" class="headerlink" title="Subject"></a>Subject</h2><ul><li>hasRole/hasAllRoles</li><li>checkRole/checkRoles</li><li>isPermitted/isPermittedAll</li><li>checkPermission/checkPermissions</li></ul><blockquote><p>参考代码：<a href="https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter3" target="_blank" rel="noopener">https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter3</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;授权&lt;/strong&gt;：也叫访问控制，即在应用中控制谁能访问哪些资源（如访问页面/编辑数据/页面操作等）。在授权中需了解的几个关键对象：&lt;code&gt;主体&lt;/code&gt;（Subject）、&lt;code&gt;资源&lt;/code&gt;（Resource）、&lt;code&gt;权限&lt;/code&gt;（Permission）、&lt;code&gt;角色&lt;/code&gt;（Role）。&lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Shiro" scheme="https://gojay.top/categories/JavaWeb/Shiro/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Shiro" scheme="https://gojay.top/tags/Shiro/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Shiro（二）-身份认证</title>
    <link href="https://gojay.top/2017/11/22/%E8%B7%9F%E6%88%91%E5%AD%A6Shiro%EF%BC%88%E4%BA%8C%EF%BC%89-%E8%BA%AB%E4%BB%BD%E8%AE%A4%E8%AF%81/"/>
    <id>https://gojay.top/2017/11/22/跟我学Shiro（二）-身份认证/</id>
    <published>2017-11-22T03:16:43.000Z</published>
    <updated>2020-06-17T12:26:30.907Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p><strong>身份验证</strong>：在应用中能<code>证明他就是他本人</code>。一般提供一些标识信息来表明他就是他本人，如提供身份证，用户名/密码来证明。<br>在 shiro 中，用户需要提供 <code>principals</code> （身份）和 <code>credentials</code>（证明）给 shiro，从而应用能验证用户身份。 </p><a id="more"></a><ul><li><strong>principals</strong>：身份；即主体的标识属性，可以是任何东西，如用户名、邮箱等，唯一即可。一个主体可以有多个 principals，但<code>只有一个 Primary principals</code>，一般是用户名/密码/手机号。 </li><li><strong>credentials</strong>：证明/凭证；即只有主体知道的安全值，如密码/数字证书等。   </li></ul><blockquote><p>最常见的 principals 和 credentials 组合就是<code>用户名/密码</code>了。<br>另外两个相关的概念是之前提到的 <code>Subject</code> 及 <code>Realm</code>，分别是主体及验证主体的数据源。</p></blockquote><h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><h2 id="使用Maven构建"><a href="#使用Maven构建" class="headerlink" title="使用Maven构建"></a>使用Maven构建</h2><p>准备环境依赖：添加 <code>junit</code>、<code>common-logging</code> 及 <code>shiro-core</code> 依赖；</p><blockquote><p><strong>更新</strong>：加入slf4j-nop依赖包。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;4.12&lt;/version&gt;</span><br><span class="line">        &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;commons-logging&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;commons-logging&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.2&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;shiro-core&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.4.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.7.25&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><h1 id="登录-退出"><a href="#登录-退出" class="headerlink" title="登录/退出"></a>登录/退出</h1><h2 id="准备一些用户身份-凭据"><a href="#准备一些用户身份-凭据" class="headerlink" title="准备一些用户身份/凭据"></a>准备一些用户身份/凭据</h2><blockquote><p>shiro.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[users]</span><br><span class="line">gojay=test</span><br><span class="line">root=root</span><br></pre></td></tr></table></figure><p>此处使用ini配置文件，通过<code>[user]</code>指定两个主体。</p><h2 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h2><blockquote><p>com.github.gojay001.test.LoginLogoutTest<br><strong>更新</strong>：注意类过时。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void testLoginLogout() &#123;</span><br><span class="line">    //1、获取SecurityManager工厂，此处使用Ini配置文件初始化SecurityManager</span><br><span class="line">    Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory(&quot;classpath:shiro.ini&quot;);</span><br><span class="line"></span><br><span class="line">    //2、得到SecurityManager实例 并绑定给SecurityUtils</span><br><span class="line">    SecurityManager securityManager = factory.getInstance();</span><br><span class="line">    SecurityUtils.setSecurityManager(securityManager);</span><br><span class="line"></span><br><span class="line">    //3、得到Subject及创建用户名/密码身份验证Token（即用户身份/凭证）</span><br><span class="line">    Subject subject = SecurityUtils.getSubject();</span><br><span class="line">    UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;);</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">        //4、登录，即身份验证</span><br><span class="line">        subject.login(token);</span><br><span class="line">    &#125; catch (AuthenticationException e) &#123;</span><br><span class="line">        //5、身份验证失败</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Assert.assertEquals(true, subject.isAuthenticated());</span><br><span class="line"></span><br><span class="line">    //6、退出</span><br><span class="line">    subject.logout();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>首先通过new IniSecurityManagerFactory并指定一个ini配置文件来<code>创建一个SecurityManager工厂</code>；</li><li>接着<code>获取SecurityManager并绑定到SecurityUtils</code>，这是一个全局设置，设置一次即可；</li><li>通过SecurityUtils<code>得到Subject</code>，其会自动绑定到当前线程；如果在web环境在请求结束时需要解除绑定；然后<code>获取身份验证的Token</code>，如用户名/密码；</li><li><code>调用subject.login方法</code>进行登录，其会自动委托给SecurityManager.login方法进行登录；</li><li><code>如果身份验证失败捕获AuthenticationException或其子类</code>，常见的如： <code>DisabledAccountException</code>（禁用的帐号）、<code>LockedAccountException</code>（锁定的帐号）、<code>UnknownAccountException</code>（错误的帐号）、<code>ExcessiveAttemptsException</code>（登录失败次数过多）、<code>IncorrectCredentialsException</code> （错误的凭证）、<code>ExpiredCredentialsException</code>（过期的凭证）等，具体查看其继承关系；</li><li>最后可以<code>调用subject.logout退出</code>，其会自动委托给SecurityManager.logout方法退出。</li></ul><h2 id="总结步骤"><a href="#总结步骤" class="headerlink" title="总结步骤"></a>总结步骤</h2><blockquote><ol><li>收集用户身份/凭证，即如用户名/密码；</li><li>调用Subject.login进行登录，如果失败将得到相应的AuthenticationException异常，根据异常提示用户错误信息；否则登录成功；</li><li>最后调用Subject.logout进行退出操作。</li></ol></blockquote><h2 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h2><blockquote><ol><li><code>用户名/密码硬编码</code>在ini配置文件，以后需要改成如数据库存储，且密码需要加密存储；</li><li><code>用户身份Token</code>可能不仅仅是用户名/密码，也可能<code>还有其他的</code>，如登录时允许用户名/邮箱/手机号同时登录。 </li></ol></blockquote><h1 id="身份认证流程"><a href="#身份认证流程" class="headerlink" title="身份认证流程"></a>身份认证流程</h1><p><img src="https://i.loli.net/2019/08/04/tfpzHNlgqS5QomI.png" alt="2-1.png"></p><h2 id="流程如下："><a href="#流程如下：" class="headerlink" title="流程如下："></a>流程如下：</h2><blockquote><ol><li>首先<code>调用Subject.login(token)</code>进行登录，其会自动委托给Security Manager，调用之前必须通过SecurityUtils. setSecurityManager()设置；</li><li><code>SecurityManager</code>负责真正的身份验证逻辑；它会<code>委托给Authenticator进行身份验证</code>；</li><li>Authenticator才是真正的身份验证者，Shiro API中核心的身份认证入口点，此处<code>可以自定义插入自己的实现</code>；</li><li>Authenticator可能会委托给相应的AuthenticationStrategy<code>进行多Realm身份验证</code>，默认ModularRealmAuthenticator会调用AuthenticationStrategy进行多Realm身份验证；</li><li>Authenticator会<code>把相应的token传入Realm，从Realm获取身份验证信息</code>，如果没有返回/抛出异常表示身份验证失败了。此处可以配置多个Realm，将按照相应的顺序及策略进行访问。</li></ol></blockquote><h1 id="Realm"><a href="#Realm" class="headerlink" title="Realm"></a>Realm</h1><p><strong>Realm</strong>：域；Shiro从Realm获取安全数据（如用户、角色、权限），可以把Realm看成DataSource，即<code>安全数据源</code>。</p><p>org.apache.shiro.realm.Realm接口如下：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String getName(); //返回一个唯一的Realm名字  </span><br><span class="line">boolean supports(AuthenticationToken token); //判断此Realm是否支持此Token  </span><br><span class="line">AuthenticationInfo getAuthenticationInfo(AuthenticationToken token) throws AuthenticationException;  //根据Token获取认证信息</span><br></pre></td></tr></table></figure><h2 id="单Realm配置"><a href="#单Realm配置" class="headerlink" title="单Realm配置"></a>单Realm配置</h2><h3 id="自定义Realm实现"><a href="#自定义Realm实现" class="headerlink" title="自定义Realm实现"></a>自定义Realm实现</h3><blockquote><p>com.github.gojay001.realm.MyRealm1</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line"> public class MyRealm1 implements Realm &#123;</span><br><span class="line">    public String getName() &#123;</span><br><span class="line">        return &quot;myRealm1&quot;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public boolean supports(AuthenticationToken authenticationToken) &#123;</span><br><span class="line">        return authenticationToken instanceof UsernamePasswordToken;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public AuthenticationInfo getAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123;</span><br><span class="line">        // 得到用户名</span><br><span class="line">        String username = (String)authenticationToken.getPrincipal();</span><br><span class="line">        // 得到密码</span><br><span class="line">        String password = new String((char[])authenticationToken.getCredentials());</span><br><span class="line">        if(!&quot;root&quot;.equals(username)) &#123;</span><br><span class="line">            //用户名错误</span><br><span class="line">            throw new UnknownAccountException();</span><br><span class="line">        &#125;</span><br><span class="line">        if(!&quot;root&quot;.equals(password)) &#123;</span><br><span class="line">            //密码错误</span><br><span class="line">            throw new IncorrectCredentialsException();</span><br><span class="line">        &#125;</span><br><span class="line">        //如果身份认证验证成功，返回一个AuthenticationInfo实现；</span><br><span class="line">        return new SimpleAuthenticationInfo(username, password, getName());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ini配置文件指定自定义Realm实现"><a href="#ini配置文件指定自定义Realm实现" class="headerlink" title="ini配置文件指定自定义Realm实现"></a>ini配置文件指定自定义Realm实现</h3><blockquote><p>shiro-realm.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line"># 声明一个realm</span><br><span class="line">myRealm1=com.github.gojay001.realm.MyRealm1</span><br><span class="line"># 指定securityManager的realms实现</span><br><span class="line">securityManager.realms=$myRealm1</span><br></pre></td></tr></table></figure><p>通过$name来引入之前的realm定义。</p><h2 id="多Realm配置"><a href="#多Realm配置" class="headerlink" title="多Realm配置"></a>多Realm配置</h2><h3 id="ini配置文件"><a href="#ini配置文件" class="headerlink" title="ini配置文件"></a>ini配置文件</h3><blockquote><p>shiro-multi-realm.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line"># 声明一个realm  </span><br><span class="line">myRealm1=com.github.zhangkaitao.shiro.chapter2.realm.MyRealm1  </span><br><span class="line">myRealm2=com.github.zhangkaitao.shiro.chapter2.realm.MyRealm2  </span><br><span class="line"># 指定securityManager的realms实现  </span><br><span class="line">securityManager.realms=$myRealm1,$myRealm2</span><br></pre></td></tr></table></figure><p>securityManager会按照realms指定的顺序进行身份认证。</p><h2 id="Shiro默认提供的Realm"><a href="#Shiro默认提供的Realm" class="headerlink" title="Shiro默认提供的Realm"></a>Shiro默认提供的Realm</h2><p><img src="https://i.loli.net/2019/08/04/2gCrkAm37uDxsl8.png" alt="2-2.png"></p><p>以后一般继承<code>AuthorizingRealm</code>（授权）即可；其继承了<code>AuthenticatingRealm</code>（即身份验证），而且也间接继承了<code>CachingRealm</code>（带有缓存实现）。<br>其中主要默认实现如下：</p><ul><li><strong>org.apache.shiro.realm.text.IniRealm</strong>：<code>[users]部分</code>指定用户名/密码及其角色；<code>[roles]部分</code>指定角色即权限信息；</li><li><strong>org.apache.shiro.realm.text.PropertiesRealm</strong>： <code>user.username=password,role1,role2</code>指定用户名/密码及其角色；<code>role.role1=permission1,permission2</code>指定角色及权限信息；</li><li><strong>org.apache.shiro.realm.jdbc.JdbcRealm</strong>：通过sql查询相应的信息，如<code>“select password from users where username = ?”</code>获取用户密码，<code>“select role_name from user_roles where username = ?”</code>获取用户角色；<code>“select permission from roles_permissions where role_name = ?”</code>获取角色对应的权限信息；也可以调用相应的api进行自定义sql；</li></ul><h2 id="JDBC-Realm使用"><a href="#JDBC-Realm使用" class="headerlink" title="JDBC Realm使用"></a>JDBC Realm使用</h2><h3 id="数据库及依赖"><a href="#数据库及依赖" class="headerlink" title="数据库及依赖"></a>数据库及依赖</h3><blockquote><p><strong>更新</strong>：alibaba的druid包更新版本。</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;6.0.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;druid&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.1.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>本文将使用mysql数据库及druid连接池。</p><h3 id="数据库下建表"><a href="#数据库下建表" class="headerlink" title="数据库下建表"></a>数据库下建表</h3><p><code>users</code>（用户名/密码）、<code>user_roles</code>（用户/角色）、<code>roles_permissions</code>（角色/权限）；<br>具体请参照sql/shiro.sql；并添加一个用户记录，用户名/密码为root/root。</p><h3 id="ini配置"><a href="#ini配置" class="headerlink" title="ini配置"></a>ini配置</h3><blockquote><p>shiro-jdbc-realm.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">jdbcRealm=org.apache.shiro.realm.jdbc.JdbcRealm  </span><br><span class="line">dataSource=com.alibaba.druid.pool.DruidDataSource  </span><br><span class="line">dataSource.driverClassName=com.mysql.jdbc.Driver  </span><br><span class="line">dataSource.url=jdbc:mysql://localhost:3306/shiro  </span><br><span class="line">dataSource.username=root  </span><br><span class="line">dataSource.password=root</span><br><span class="line">jdbcRealm.dataSource=$dataSource  </span><br><span class="line">securityManager.realms=$jdbcRealm</span><br></pre></td></tr></table></figure><ol><li><code>变量名=全限定类名</code> 自动创建一个类实例</li><li><code>变量名.属性=值</code> 自动调用相应的setter方法进行赋值</li><li><code>$变量名</code> 引用之前的一个对象实例 </li><li><code>测试代码</code>和之前的没什么区别。</li></ol><h1 id="Authenticator及AuthenticationStrategy"><a href="#Authenticator及AuthenticationStrategy" class="headerlink" title="Authenticator及AuthenticationStrategy"></a>Authenticator及AuthenticationStrategy</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><strong>Authenticator</strong>的职责是验证用户帐号，<br>是Shiro API中身份验证核心的入口点：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public AuthenticationInfo authenticate(AuthenticationToken authenticationToken) throws AuthenticationException;</span><br></pre></td></tr></table></figure><p>如果验证成功，将返回AuthenticationInfo验证信息，此信息中包含了身份及凭证；<br>如果验证失败将抛出相应的AuthenticationException实现。</p><p><strong>SecurityManager</strong>接口继承了Authenticator，另外还有一个<code>ModularRealmAuthenticator实现</code>，其委托给多个Realm进行验证，验证规则通过<code>AuthenticationStrategy接口</code>指定，默认提供的实现：</p><ul><li><strong>FirstSuccessfulStrategy</strong>：只要有<code>一个Realm验证成功</code>即可，<code>只返回第一个Realm</code>身份验证成功的认证信息，其他的忽略；</li><li><strong>AtLeastOneSuccessfulStrategy</strong>：只要有<code>一个Realm验证成功</code>即可，和FirstSuccessfulStrategy不同，<code>返回所有Realm</code>身份验证成功的认证信息；</li><li><strong>AllSuccessfulStrategy</strong>：<code>所有Realm验证成功</code>才算成功，且<code>返回所有Realm</code>身份验证成功的认证信息，如果有一个失败就失败了。<blockquote><p><code>ModularRealmAuthenticator</code>默认使用<code>AtLeastOneSuccessfulStrategy</code>策略。</p></blockquote></li></ul><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><blockquote><p>假设有三个realm：<br>myRealm1： 用户名/密码为root/root时成功，且返回身份/凭据为root/root；<br>myRealm2： 用户名/密码为gojay/test时成功，且返回身份/凭据为gojay/test；<br>myRealm3： 用户名/密码为root/root时成功，且返回身份/凭据为<a href="mailto:root@foxmail.com" target="_blank" rel="noopener">root@foxmail.com</a>/root；</p></blockquote><h3 id="ini配置文件-1"><a href="#ini配置文件-1" class="headerlink" title="ini配置文件"></a>ini配置文件</h3><blockquote><p>shiro-authenticator-all-success.ini</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[main]</span><br><span class="line">#指定securityManager的authenticator实现</span><br><span class="line">authenticator=org.apache.shiro.authc.pam.ModularRealmAuthenticator</span><br><span class="line">securityManager.authenticator=$authenticator</span><br><span class="line"></span><br><span class="line">#指定securityManager.authenticator的authenticationStrategy</span><br><span class="line">allSuccessfulStrategy=org.apache.shiro.authc.pam.AllSuccessfulStrategy</span><br><span class="line">securityManager.authenticator.authenticationStrategy=$allSuccessfulStrategy</span><br><span class="line"></span><br><span class="line">myRealm1=com.github.gojay001.realm.MyRealm1</span><br><span class="line">myRealm2=com.github.gojay001.realm.MyRealm2</span><br><span class="line">myRealm3=com.github.gojay001.realm.MyRealm3</span><br><span class="line">securityManager.realms=$myRealm1,$myRealm3</span><br></pre></td></tr></table></figure><h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><blockquote><p>com.github.gojay001.test.AuthenticatorTest</p></blockquote><h4 id="首先通用化登录逻辑"><a href="#首先通用化登录逻辑" class="headerlink" title="首先通用化登录逻辑"></a>首先通用化登录逻辑</h4><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">private void login(String configFile) &#123;  </span><br><span class="line">    //1、获取SecurityManager工厂，此处使用Ini配置文件初始化SecurityManager  </span><br><span class="line">    Factory&lt;org.apache.shiro.mgt.SecurityManager&gt; factory =  </span><br><span class="line">            new IniSecurityManagerFactory(configFile);  </span><br><span class="line">  </span><br><span class="line">    //2、得到SecurityManager实例 并绑定给SecurityUtils  </span><br><span class="line">    org.apache.shiro.mgt.SecurityManager securityManager = factory.getInstance();  </span><br><span class="line">    SecurityUtils.setSecurityManager(securityManager);  </span><br><span class="line">  </span><br><span class="line">    //3、得到Subject及创建用户名/密码身份验证Token（即用户身份/凭证）  </span><br><span class="line">    Subject subject = SecurityUtils.getSubject();  </span><br><span class="line">    UsernamePasswordToken token = new UsernamePasswordToken(&quot;root&quot;, &quot;root&quot;);  </span><br><span class="line">  </span><br><span class="line">    subject.login(token);  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="测试AllSuccessfulStrategy成功"><a href="#测试AllSuccessfulStrategy成功" class="headerlink" title="测试AllSuccessfulStrategy成功"></a>测试AllSuccessfulStrategy成功</h4><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test  </span><br><span class="line">public void testAllSuccessfulStrategyWithSuccess() &#123;  </span><br><span class="line">    login(&quot;classpath:shiro-authenticator-all-success.ini&quot;);  </span><br><span class="line">    Subject subject = SecurityUtils.getSubject();  </span><br><span class="line">  </span><br><span class="line">    //得到一个身份集合，其包含了Realm验证成功的身份信息  </span><br><span class="line">    PrincipalCollection principalCollection = subject.getPrincipals();  </span><br><span class="line">    Assert.assertEquals(2, principalCollection.asList().size());  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="测试AllSuccessfulStrategy失败"><a href="#测试AllSuccessfulStrategy失败" class="headerlink" title="测试AllSuccessfulStrategy失败"></a>测试AllSuccessfulStrategy失败</h4><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">@Test(expected = UnknownAccountException.class)  </span><br><span class="line">public void testAllSuccessfulStrategyWithFail() &#123;  </span><br><span class="line">    login(&quot;classpath:shiro-authenticator-all-fail.ini&quot;);  </span><br><span class="line">    Subject subject = SecurityUtils.getSubject();  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>shiro-authenticator-all-fail.ini</code> 与 <code>shiro-authenticator-all-success.ini</code> 不同的配置是使用了 <code>securityManager.realms=$myRealm1,$myRealm2</code> ；即myRealm验证失败。</p></blockquote><blockquote><p> 对于 <code>AtLeastOneSuccessfulStrategy</code> 和 <code>FirstSuccessfulStrategy</code> 的区别：唯一不同点一个是<code>返回所有</code>验证成功的Realm的认证信息；另一个是<code>只返回第一个</code>验证成功的Realm的认证信息.<code>示例代码同上</code></p></blockquote><h3 id="自定义AuthenticationStrategy实现"><a href="#自定义AuthenticationStrategy实现" class="headerlink" title="自定义AuthenticationStrategy实现"></a>自定义AuthenticationStrategy实现</h3><p>首先看其API：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">//在所有Realm验证之前调用  </span><br><span class="line">AuthenticationInfo beforeAllAttempts(  </span><br><span class="line">Collection&lt;? extends Realm&gt; realms, AuthenticationToken token)   </span><br><span class="line">throws AuthenticationException;  </span><br><span class="line">//在每个Realm之前调用  </span><br><span class="line">AuthenticationInfo beforeAttempt(  </span><br><span class="line">Realm realm, AuthenticationToken token, AuthenticationInfo aggregate)   </span><br><span class="line">throws AuthenticationException;  </span><br><span class="line">//在每个Realm之后调用  </span><br><span class="line">AuthenticationInfo afterAttempt(  </span><br><span class="line">Realm realm, AuthenticationToken token,   </span><br><span class="line">AuthenticationInfo singleRealmInfo, AuthenticationInfo aggregateInfo, Throwable t)  </span><br><span class="line">throws AuthenticationException;  </span><br><span class="line">//在所有Realm之后调用  </span><br><span class="line">AuthenticationInfo afterAllAttempts(  </span><br><span class="line">AuthenticationToken token, AuthenticationInfo aggregate)   </span><br><span class="line">throws AuthenticationException;</span><br></pre></td></tr></table></figure><p>因为每个<code>AuthenticationStrategy</code>实例都是无状态的，所有每次都通过接口将相应的认证信息传入下一次流程；<br>通过如上接口可以进行如合并/返回第一个验证成功的认证信息。<br>自定义实现时一般<code>继承 org.apache.shiro.authc.pam.AbstractAuthenticationStrategy</code> 即可。<code>参考代码同上</code></p><blockquote><p>到此基本的身份验证就结束了。</p></blockquote><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><h3 id="Assert过时"><a href="#Assert过时" class="headerlink" title="Assert过时"></a>Assert过时</h3><blockquote><p><strong>Assert in junit.framework has been deprecated</strong><br><strong>解决</strong>：将 <code>import junit.framework.Assert;</code> 改为 <code>import org.junit.Assert;</code> </p></blockquote><h3 id="SLF4J加载失败"><a href="#SLF4J加载失败" class="headerlink" title="SLF4J加载失败"></a>SLF4J加载失败</h3><blockquote><p>SLF4J: Failed to load class “org.slf4j.impl.StaticLoggerBinder”<br><strong>解决</strong>：Maven引入slf4j-nop包：</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;slf4j-nop&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.7.6&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="implements不需要-Override"><a href="#implements不需要-Override" class="headerlink" title="implements不需要@Override"></a>implements不需要@Override</h3><h3 id="alibaba的druid版本更新"><a href="#alibaba的druid版本更新" class="headerlink" title="alibaba的druid版本更新"></a>alibaba的druid版本更新</h3><h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><h3 id="用户登录流程"><a href="#用户登录流程" class="headerlink" title="用户登录流程"></a>用户登录流程</h3><ul><li>Subject.login(token)</li><li>SecurityManager</li><li>Authenticator</li><li>AuthenticatorStrategy</li><li>Realm</li></ul><h3 id="Realm-1"><a href="#Realm-1" class="headerlink" title="Realm"></a>Realm</h3><ul><li>单Realm</li><li>多Realm</li><li>JDBCRealm</li></ul><h3 id="AuthenticatorStrategy"><a href="#AuthenticatorStrategy" class="headerlink" title="AuthenticatorStrategy"></a>AuthenticatorStrategy</h3><ul><li>FirstSuccessfulStrategy</li><li>AtLeastOneSuccessfulStrategy</li><li>AllSuccessfulStrategy</li><li>自定义Strategy</li></ul><blockquote><p>参考代码：<a href="https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter2" target="_blank" rel="noopener">https://github.com/Gojay001/Demo/tree/master/ShiroTest/ShiroTest-chapter2</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;身份验证&lt;/strong&gt;：在应用中能&lt;code&gt;证明他就是他本人&lt;/code&gt;。一般提供一些标识信息来表明他就是他本人，如提供身份证，用户名/密码来证明。&lt;br&gt;在 shiro 中，用户需要提供 &lt;code&gt;principals&lt;/code&gt; （身份）和 &lt;code&gt;credentials&lt;/code&gt;（证明）给 shiro，从而应用能验证用户身份。 &lt;/p&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Shiro" scheme="https://gojay.top/categories/JavaWeb/Shiro/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Shiro" scheme="https://gojay.top/tags/Shiro/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Shiro（一）-Shiro简介</title>
    <link href="https://gojay.top/2017/11/19/%E8%B7%9F%E6%88%91%E5%AD%A6Shiro%EF%BC%88%E4%B8%80%EF%BC%89-Shiro%E7%AE%80%E4%BB%8B/"/>
    <id>https://gojay.top/2017/11/19/跟我学Shiro（一）-Shiro简介/</id>
    <published>2017-11-19T03:06:53.000Z</published>
    <updated>2020-06-17T12:26:30.906Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="认识"><a href="#认识" class="headerlink" title="认识"></a>认识</h2><ul><li>Apache Shiro是Java的一个安全框架。  </li><li>对比<code>Spring Security</code>小而简单。  </li><li>可同时用在JavaSE、JavaEE环境中。  </li><li>主要完成认证、授权、加密、会话管理、与Web集成、缓存等。<a id="more"></a></li></ul><h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><p><img src="https://i.loli.net/2019/08/04/2icLvSGIotKdmC9.png" alt="1-1.png"></p><ul><li><strong>Authentication</strong>：身份认证/登录，验证用户是不是拥有相应的身份；</li><li><strong>Authorization</strong>：授权，即权限验证，验证某个已认证的用户是否拥有某个权限；</li><li><strong>Session Manager</strong>：会话管理，即用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；</li><li><strong>Cryptography</strong>：加密，保护数据的安全性，如密码加密存储到数据库，而不是明文存储；</li><li><strong>Web Support</strong>：Web 支持，可以非常容易的集成到 Web 环境；</li><li><strong>Concurrency</strong>：shiro 支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去；</li><li><strong>Testing</strong>：提供测试支持；</li><li><strong>Run As</strong>：允许一个用户假装为另一个用户（如果他们允许）的身份进行访问；</li><li><strong>Remember Me</strong>：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了。</li></ul><blockquote><p>Shiro不会去维护用户、维护权限；这些需要自己设计提供，然后通过相应的接口注入给Shiro。</p></blockquote><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="从外部看Shiro："><a href="#从外部看Shiro：" class="headerlink" title="从外部看Shiro："></a>从外部看Shiro：</h3><p><img src="https://i.loli.net/2019/08/04/HUeCOTW3RNdk5wu.png" alt="1-2.png"></p><ul><li><strong>Subject</strong>：主体；代表了与当前应用交互的用户，如网络爬虫、机器人等；所有 <code>Subject</code> 都绑定到 <code>SecurityManager</code>，与 <code>Subject</code> 的所有交互都会委托给 <code>SecurityManager</code>；可以把 <code>Subject</code> 认为是一个门面，<code>SecurityManager</code> 才是实际的执行者；</li><li><strong>SecurityManager</strong>：安全管理器；即所有与安全有关的操作都会与 <code>SecurityManager</code> 交互，且它管理着所有 <code>Subject</code>；它是 Shiro 的核心，负责与其他组件进行交互，可以把它看成<code>Spring NVC</code>中的 <code>DispatcherServlet</code> 前端控制器；</li><li><strong>Realm</strong>：域；<code>Shiro</code> 从 <code>Realm</code> 获取安全数据（如用户、角色、权限），就是说 <code>SecurityManager</code> 要验证用户身份，那么它需要从 <code>Realm</code> 获取相应的用户进行比较以确定用户身份是否合法；也需要从 <code>Realm</code> 得到用户相应的角色/权限进行验证用户是否能进行操作；可以把 Realm 看成 <code>DataSource</code>，即安全数据源。</li></ul><blockquote><p>最简单的一个 Shiro 应用：</p><ol><li>应用代码通过 <code>Subject</code> 来进行认证和授权，而 <code>Subject</code> 又委托给 <code>SecurityManager</code>；</li><li>我们需要给 Shiro 的 <code>SecurityManager</code> 注入 <code>Realm</code>，从而让 <code>SecurityManager</code> 能得到合法的用户及其权限进行判断。<br>可以看出：Shiro不提供维护用户/权限，而是通过Realm让开发人员自己注入。</li></ol></blockquote><h3 id="从内部看Shiro："><a href="#从内部看Shiro：" class="headerlink" title="从内部看Shiro："></a>从内部看Shiro：</h3><p><img src="https://i.loli.net/2019/08/04/mtUHo7nAzGLOZbB.png" alt="1-3.png"></p><ul><li><strong>Subject</strong>：主体；可以看到主体可以是任何可以与应用交互的“用户”；</li><li><strong>SecurityManager</strong> ：安全管理器；所有具体的交互都通过 <code>SecurityManager</code> 进行控制；它管理着所有 <code>Subject</code>，且负责进行认证和授权、及会话、缓存的管理，是Shiro的心脏；</li><li><strong>Authenticator</strong>：认证器；负责主体认证的，这是一个扩展点，如果用户觉得 Shiro 默认的不好，可以自定义实现；其需要认证策略<code>Authentication Strategy</code>，即什么情况下算用户认证通过了； * <strong>Authrizer</strong>：授权器；用来决定主体是否有权限进行相应的操作，即控制着用户能访问应用中的哪些功能；</li><li><strong>Realm</strong>：可以有 1 个或多个 <code>Realm</code>，可以认为是安全实体数据源，即用于获取安全实体的，由用户提供；Shiro 不知道用户/权限存储在哪及以何种格式存储，所以我们一般在应用中都需要实现自己的 <code>Realm</code>；</li><li><strong>SessionManager</strong>：Session需要有人去管理它的生命周期，这个组件就是<code>SessionManager</code>；Shiro 抽象了一个自己的 Session 来管理主体与应用之间交互的数据；这样的话，比如我们在 Web 环境用，刚开始是一台 Web 服务器；接着又上了台 EJB 服务器；这时想把两台服务器的会话数据放到一个地方，这个时候就可以实现自己的分布式会话（如把数据放到 Memcached 服务器）；</li><li><strong>SessionDAO</strong>：数据访问对象<code>DAO</code>，用于会话的 CRUD，比如我们想把 Session 保存到数据库，那么可以实现自己的 SessionDAO，通过如JDBC写到数据库；比如想把 <code>Session</code> 放到<code>Memcached</code>中，可以实现自己的 <code>Memcached SessionDAO</code>；另外 <code>SessionDAO</code> 中可以使用 <code>Cache</code> 进行缓存，以提高性能；</li><li><strong>CacheManager</strong>：缓存控制器；来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少去改变，放到缓存中后可以提高访问的性能；</li><li><strong>Cryptography</strong>：密码模块;Shiro 提高了一些常见的加密组件用于如密码加密/解密的。</li></ul><blockquote><p>到此 Shiro 架构及其组件就认识完了。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;h2 id=&quot;认识&quot;&gt;&lt;a href=&quot;#认识&quot; class=&quot;headerlink&quot; title=&quot;认识&quot;&gt;&lt;/a&gt;认识&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Apache Shiro是Java的一个安全框架。  &lt;/li&gt;
&lt;li&gt;对比&lt;code&gt;Spring Security&lt;/code&gt;小而简单。  &lt;/li&gt;
&lt;li&gt;可同时用在JavaSE、JavaEE环境中。  &lt;/li&gt;
&lt;li&gt;主要完成认证、授权、加密、会话管理、与Web集成、缓存等。&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Shiro" scheme="https://gojay.top/categories/JavaWeb/Shiro/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Shiro" scheme="https://gojay.top/tags/Shiro/"/>
    
  </entry>
  
  <entry>
    <title>Learning JavaWeb Path</title>
    <link href="https://gojay.top/2017/11/18/Learning-JavaWeb-Path/"/>
    <id>https://gojay.top/2017/11/18/Learning-JavaWeb-Path/</id>
    <published>2017-11-18T07:25:34.000Z</published>
    <updated>2020-06-17T12:26:30.899Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基本概念的理解"><a href="#基本概念的理解" class="headerlink" title="基本概念的理解"></a>基本概念的理解</h1><hr><a id="more"></a><ul><li><strong>绝对路径</strong>：绝对路径就是你的主页上的文件或目录在硬盘上真正的路径，(URL和物理路径)例如：<code>C:\xyz\test.txt</code>；<code>http://www.test.com/index.html</code>；</li><li><strong>相对路径</strong>：相对与某个基准目录的路径，例如：<code>&quot;/&quot;</code>代表Web应用的<code>根目录</code>，<code>&quot;./&quot;</code>代表<code>当前目录</code>,<code>&quot;../&quot;</code>代表<code>上级目录</code>。</li></ul><blockquote><p>另外关于URI，URL,URN等内容，请参考RFC相关文档标准。</p></blockquote><h1 id="关于JSP-Servlet中的相对路径和绝对路径"><a href="#关于JSP-Servlet中的相对路径和绝对路径" class="headerlink" title="关于JSP/Servlet中的相对路径和绝对路径"></a>关于JSP/Servlet中的相对路径和绝对路径</h1><hr><h2 id="服务器端的地址"><a href="#服务器端的地址" class="headerlink" title="服务器端的地址"></a>服务器端的地址</h2><blockquote><p>服务器端的相对地址指的是相对于你的web应用的地址，这个地址是在服务器端解析的（不同于<code>html</code>和<code>javascript</code>中的相对地址，他们是由客户端浏览器解析的）；<br>在jsp和servlet中的相对地址应该是相对于你的web应用，即相对于<code>http://192.168.0.1/webapp/</code>的。<br>用到的地方：  </p><ul><li><strong>forward</strong>：servlet中的request.getRequestDispatcher(address);这个address是在服务器端解析的。<code>request.getRequestDispatcher(“/pages/a.jsp”)</code>的绝对路径地址：<code>http://192.168.0.1/webapp/pages/a.jsp</code>；</li><li><strong>sendRedirect</strong>：在jsp中<code>&lt;%response.sendRedirect(&quot;/user/a.jsp&quot;);%&gt;</code>。</li></ul></blockquote><h2 id="客户端的地址"><a href="#客户端的地址" class="headerlink" title="客户端的地址"></a>客户端的地址</h2><blockquote><p>所有的html页面中的相对地址都是相对于服务器根目录<code>http://192.168.0.1/</code>的，而<code>不是</code>根目录下的该Web应用的目录：<code>http://192.168.0.1/webapp/</code>。</p></blockquote><ul><li>HTML中的<strong>form表单的action属性</strong>的地址应该是相对于服务器根目录<code>http://192.168.0.1/</code>；如果提交到a.jsp为：<code>action＝&quot;/webapp/user/a.jsp&quot;</code>或<code>action=&quot;&lt;%=request.getContextPath()%&gt;&quot;/user/a.jsp</code>；</li><li><strong>Javascript</strong>也是在客户端解析的，所以其相对路径和form表单一样。<blockquote><p>因此，一般情况下，在<code>JSP/HTML</code>页面等引用的<code>CSS</code>、<code>Javascript</code>、<code>Action</code>等属性前面最好都加上<code>&lt;%=request.getContextPath()%&gt;</code>,以确保所引用的文件都属于Web应用中的目录。<br>另外，应该尽量避免使用类似<code>&quot;.&quot;</code>,<code>&quot;./&quot;</code>,<code>&quot;../&quot;</code>等类似的相对该文件位置的相对路径，这样当文件移动时，很容易出问题。</p></blockquote></li></ul><h2 id="站点根目录和css路径问题"><a href="#站点根目录和css路径问题" class="headerlink" title="站点根目录和css路径问题"></a>站点根目录和css路径问题</h2><blockquote><p>当在<code>jsp中引入css</code>时，如果其相对路径相对于当前jsp文件的，而在一个和这个jsp的<code>路径不一样的servlet中forward</code>这个jsp时，就会发现这个css样式根本没有起作用。<br>这是因为在servlet中转发时<code>css的路径</code>就是相对于这个<code>servlet的相对路径</code>，而非jsp的路径了。<br>所以这时候不能在jsp中用这样的路径：<code>&lt;link href=&quot;one.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;</code>或者<code>&lt;link href=&quot;../../one.css&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;&gt;</code>。<br>这个时候要用站点根目录，就是相对于<code>http://192.168.0.1/</code>的目录，以<code>&quot;/&quot;</code>开头。<br>因此上述错误应更正为<code>href=”/test/one.css”</code> 类似的站点根目录的相对目录。</p></blockquote><h1 id="获得JSP-Servlet中当前应用的相对路径和绝对路径"><a href="#获得JSP-Servlet中当前应用的相对路径和绝对路径" class="headerlink" title="获得JSP/Servlet中当前应用的相对路径和绝对路径"></a>获得JSP/Servlet中当前应用的相对路径和绝对路径</h1><hr><h2 id="JSP中获得当前应用的相对路径和绝对路径"><a href="#JSP中获得当前应用的相对路径和绝对路径" class="headerlink" title="JSP中获得当前应用的相对路径和绝对路径"></a>JSP中获得当前应用的相对路径和绝对路径</h2><ul><li><strong>根目录</strong>所对应的绝对路径:<code>request.getRequestURI()</code>;</li><li><strong>文件</strong>的绝对路径:<code>application.getRealPath(request.getRequestURI())</code>;</li><li><strong>当前web应用</strong>的绝对路径:<code>application.getRealPath(&quot;/&quot;)</code>;</li><li><strong>请求文件的上层目录</strong>:<code>new File(application.getRealPath(request.getRequestURI())).getParent()</code>;</li></ul><h2 id="Servlet中获得当前应用的相对路径和绝对路径"><a href="#Servlet中获得当前应用的相对路径和绝对路径" class="headerlink" title="Servlet中获得当前应用的相对路径和绝对路径"></a>Servlet中获得当前应用的相对路径和绝对路径</h2><ul><li><strong>根目录</strong>所对应的绝对路径:<code>request.getServletPath()</code>;</li><li><strong>文件</strong>的绝对路径:<code>request.getSession().getServletContext().getRealPath(request.getRequestURI())</code>;</li><li><strong>当前web应用</strong>的绝对路径:<code>servletConfig.getServletContext().getRealPath(&quot;/&quot;)</code>;<blockquote><p><strong>ServletContext对象</strong>获得几种方式：<br>javax.servlet.http.HttpSession.getServletContext();<br>javax.servlet.jsp.PageContext.getServletContext();<br>javax.servlet.ServletConfig.getServletContext(); </p></blockquote></li></ul><h1 id="JAVA的Class中获得相对路径，绝对路径"><a href="#JAVA的Class中获得相对路径，绝对路径" class="headerlink" title="JAVA的Class中获得相对路径，绝对路径"></a>JAVA的Class中获得相对路径，绝对路径</h1><hr><h2 id="单独的Java类中获得绝对路径"><a href="#单独的Java类中获得绝对路径" class="headerlink" title="单独的Java类中获得绝对路径"></a>单独的Java类中获得绝对路径</h2><blockquote><p>根据<code>java.io.File</code>的Doc文挡，可知:<br> 默认情况下<code>new File(&quot;/&quot;)</code>代表的目录为：<code>System.getProperty(&quot;user.dir&quot;)</code>;<br>程序获得执行类的当前路径:</p></blockquote><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">import java.io.File;</span><br><span class="line">public class FileTest &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;      </span><br><span class="line">        System.out.println(Thread.currentThread().getContextClassLoader().getResource(&quot;&quot;));    </span><br><span class="line">        System.out.println(FileTest.class.getClassLoader().getResource(&quot;&quot;));</span><br><span class="line">　      System.out.println(ClassLoader.getSystemResource(&quot;&quot;));        </span><br><span class="line">        System.out.println(FileTest.class.getResource(&quot;&quot;));        </span><br><span class="line">        System.out.println(FileTest.class.getResource(&quot;/&quot;));//Class文件所在路径  </span><br><span class="line">        System.out.println(new File(&quot;/&quot;).getAbsolutePath());   </span><br><span class="line">        System.out.println(System.getProperty(&quot;user.dir&quot;));    </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="服务器中的Java类获得当前路径"><a href="#服务器中的Java类获得当前路径" class="headerlink" title="服务器中的Java类获得当前路径"></a>服务器中的Java类获得当前路径</h2><h3 id="Weblogic"><a href="#Weblogic" class="headerlink" title="Weblogic"></a>Weblogic</h3><blockquote><p>WebApplication的系统文件根目录是你的weblogic安装所在根目录。<br>例如：如果你的weblogic安装在<code>c:\bea\weblogic700.....</code><br>那么，你的文件根路径就是<code>c:\</code><br>所以，有两种方式能够让你访问你的服务器端的文件:  </p><ol><li>使用<strong>绝对路径</strong>：<br>比如将你的参数文件放在c:\yourconfig\yourconf.properties，直接使用<code>new FileInputStream(&quot;yourconfig/yourconf.properties&quot;)</code>;  </li><li>使用<strong>相对路径</strong>：<br>相对路径的根目录就是你的<code>webapplication的根路径</code>，即WEB-INF的上一级目录，将你的参数文件放在<code>yourwebapp\yourconfig\yourconf.properties</code>，这样使用：<code>new FileInputStream(&quot;./yourconfig/yourconf.properties&quot;)</code>;</li></ol></blockquote><h3 id="Tomcat"><a href="#Tomcat" class="headerlink" title="Tomcat"></a>Tomcat</h3><blockquote><p>在类中输出<code>System.getProperty(&quot;user.dir&quot;);</code>显示的是<code>%Tomcat_Home%/bin</code></p></blockquote><h3 id="如何读相对路径哪"><a href="#如何读相对路径哪" class="headerlink" title="如何读相对路径哪?"></a>如何读相对路径哪?</h3><blockquote><p>在Java文件中<code>getResource</code>或<code>getResourceAsStream</code>均可。<br>例：<code>getClass().getResourceAsStream(filePath)</code>;filePath可以是”/filename”,这里的/代表web发布根路径下<code>WEB-INF/classes</code>。</p></blockquote><p>参考文档：<a href="http://huttoncs.iteye.com/blog/2270670" target="_blank" rel="noopener">java路径问题</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;基本概念的理解&quot;&gt;&lt;a href=&quot;#基本概念的理解&quot; class=&quot;headerlink&quot; title=&quot;基本概念的理解&quot;&gt;&lt;/a&gt;基本概念的理解&lt;/h1&gt;&lt;hr&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Project" scheme="https://gojay.top/categories/JavaWeb/Project/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Path" scheme="https://gojay.top/tags/Path/"/>
    
  </entry>
  
  <entry>
    <title>Learning JavaWeb Encoding</title>
    <link href="https://gojay.top/2017/11/15/Learning-JavaWeb-Encoding/"/>
    <id>https://gojay.top/2017/11/15/Learning-JavaWeb-Encoding/</id>
    <published>2017-11-15T07:51:41.000Z</published>
    <updated>2020-06-17T12:26:30.899Z</updated>
    
    <content type="html"><![CDATA[<h1 id="项目配置"><a href="#项目配置" class="headerlink" title="项目配置"></a>项目配置</h1><hr><a id="more"></a><h2 id="页面乱码"><a href="#页面乱码" class="headerlink" title="页面乱码"></a>页面乱码</h2><p>页面乱码只需设置相关 <code>字符集编码</code> 即可。  </p><h3 id="JSP页面："><a href="#JSP页面：" class="headerlink" title="JSP页面："></a><strong>JSP页面</strong>：</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;%@ page pageEncoding=&quot;UTF-8&quot; contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; %&gt;</span><br></pre></td></tr></table></figure><p><code>pageEncoding</code> :该页面编码格式；<br><code>charset</code> :页面解码格式；</p><h3 id="HTML页面："><a href="#HTML页面：" class="headerlink" title="HTML页面："></a><strong>HTML页面</strong>：</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;meta http-equiv=Content-Type content=&quot;text/html;charset=utf-8&quot;&gt;</span><br></pre></td></tr></table></figure><h2 id="传值乱码"><a href="#传值乱码" class="headerlink" title="传值乱码"></a>传值乱码</h2><p><code>页面</code> 到 <code>controller</code> 传值乱码需要在 <strong>web.xml</strong> 配置字符编码过滤器。  </p><h3 id="直接应用-spring-中字符编码过滤器："><a href="#直接应用-spring-中字符编码过滤器：" class="headerlink" title="直接应用 spring 中字符编码过滤器："></a>直接应用 <code>spring</code> 中字符编码过滤器：</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;!--字符编码--&gt;</span><br><span class="line">&lt;filter&gt;</span><br><span class="line">    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;</span><br><span class="line">    &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;</span><br><span class="line">    &lt;init-param&gt;</span><br><span class="line">        &lt;param-name&gt;encoding&lt;/param-name&gt;</span><br><span class="line">        &lt;param-value&gt;UTF-8&lt;/param-value&gt;</span><br><span class="line">    &lt;/init-param&gt;</span><br><span class="line">    &lt;init-param&gt;</span><br><span class="line">        &lt;param-name&gt;forceEncoding&lt;/param-name&gt;</span><br><span class="line">        &lt;param-value&gt;true&lt;/param-value&gt;</span><br><span class="line">    &lt;/init-param&gt;</span><br><span class="line">&lt;/filter&gt;</span><br><span class="line">&lt;filter-mapping&gt;</span><br><span class="line">    &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;</span><br><span class="line">    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;</span><br><span class="line">&lt;/filter-mapping&gt;</span><br></pre></td></tr></table></figure><ul><li><p>分析源码发现作用相当于servlet中：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">request.setCharacterEncoding(&quot;UTF-8&quot;);  </span><br><span class="line">response.setCharacterEncoding(&quot;UTF-8&quot;);</span><br></pre></td></tr></table></figure></li><li><p>Spring自带过滤器主要针对POST请求，对GET请求无效。<br>对于GET请求的参数乱码，解决方法是采用数据还原：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">String userName = request.getParameter(&quot;userName&quot;);     </span><br><span class="line">userName = new String(userName.getBytes(&quot;iso8859-1&quot;),&quot;UTF-8&quot;);</span><br></pre></td></tr></table></figure></li><li><p><code>&lt;url-pattern&gt;</code> 中匹配说明：<br><code>/</code>: 不会匹配到<em>.jsp，但会匹配/login等路径类型的url；<br><code>/*</code>: 会匹配/login、</em>.jsp、*.html等路径；  </p></li></ul><h3 id="根据源码可-自己编写-字符编码过滤器："><a href="#根据源码可-自己编写-字符编码过滤器：" class="headerlink" title="根据源码可 自己编写 字符编码过滤器："></a>根据源码可 <code>自己编写</code> 字符编码过滤器：</h3><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">public class CharacterEncodingFilter implements Filter &#123;</span><br><span class="line">    private String encoding = null;</span><br><span class="line">    private FilterConfig filterConfig = null;</span><br><span class="line">    @Override</span><br><span class="line">    public void init(FilterConfig config) throws ServletException &#123;</span><br><span class="line">        this.filterConfig = config;</span><br><span class="line">        this.encoding = config.getInitParameter(&quot;encoding&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)</span><br><span class="line">            throws ServletException, IOException &#123;</span><br><span class="line">        if (encoding != null) &#123;</span><br><span class="line">            request.setCharacterEncoding(encoding);</span><br><span class="line">            response.setContentType(&quot;text/html;charset=&quot; + encoding);</span><br><span class="line">        &#125;</span><br><span class="line">        chain.doFilter(request, response);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="存入数据库乱码"><a href="#存入数据库乱码" class="headerlink" title="存入数据库乱码"></a>存入数据库乱码</h2><p>需要在 <code>数据库配置文件</code> 设置参数。</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">url=jdbc:mysql://gojay001.mysql.rds.aliyuncs.com:3306/trade?useUnicode=true&amp;characterEncoding=utf8</span><br></pre></td></tr></table></figure><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><hr><h2 id="Tomcat配置"><a href="#Tomcat配置" class="headerlink" title="Tomcat配置"></a>Tomcat配置</h2><p>在tomcat的 <code>conf/server.xml</code> 中配置Get请求默认编码：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;  </span><br><span class="line">      connectionTimeout=&quot;20000&quot;  </span><br><span class="line">      redirectPort=&quot;8443&quot;   </span><br><span class="line">      URIEncoding=&quot;UTF-8&quot;  </span><br><span class="line">      useBodyEncodingForURI=&quot;true&quot;  </span><br><span class="line">      /&gt;</span><br></pre></td></tr></table></figure><h2 id="数据库配置"><a href="#数据库配置" class="headerlink" title="数据库配置"></a>数据库配置</h2><p>安装mysql之后默认的字符编码为 <code>latin1</code> :  </p><ol><li><p>查看: </p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">$ show variables like &apos;%char%&apos;;</span><br></pre></td></tr></table></figure></li><li><p><code>vi /etc/my.cf</code> 修改为下面内容后重启mysql：</p><figure class="highlight plain hljs"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">character_set_server=utf8</span><br><span class="line">lower_case_table_names=1</span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/lib/mysql/mysql.sock</span><br><span class="line">user=mysql</span><br><span class="line">symbolic-links=0</span><br><span class="line">sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES </span><br><span class="line">[mysqld_safe]</span><br><span class="line">default-character-set = utf8</span><br><span class="line">log-error=/var/log/mysqld.log</span><br><span class="line">pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line">[client]</span><br><span class="line">default-character-set = utf8</span><br><span class="line">[mysql.server]</span><br><span class="line">default-character-set = utf8</span><br><span class="line">[mysql]</span><br><span class="line">default-character-set = utf8</span><br></pre></td></tr></table></figure></li></ol><h1 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h1><hr><ul><li><strong>HTML文件</strong>显示乱码：<br>将编码格式保存为UTF-8包含BOM。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;项目配置&quot;&gt;&lt;a href=&quot;#项目配置&quot; class=&quot;headerlink&quot; title=&quot;项目配置&quot;&gt;&lt;/a&gt;项目配置&lt;/h1&gt;&lt;hr&gt;
    
    </summary>
    
      <category term="JavaWeb" scheme="https://gojay.top/categories/JavaWeb/"/>
    
      <category term="Project" scheme="https://gojay.top/categories/JavaWeb/Project/"/>
    
    
      <category term="Java" scheme="https://gojay.top/tags/Java/"/>
    
      <category term="Encoding" scheme="https://gojay.top/tags/Encoding/"/>
    
  </entry>
  
</feed>
