<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
<meta name="baidu-site-verification" content="y4OgBadCWI">
<meta name="google-site-verification" content="bbMnXM8ufhf7fGNfsWyXLlLCJ9Z_mhUMvRlQ0Ax8juA">
<title>Overview - Gojay&#39;s Records</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



    <meta name="description" content="There are the overall of papers about Deep Learning.https://github.com/Gojay001/DeepLearning-pwcn">
<meta name="keywords" content="DL,Overview">
<meta property="og:type" content="article">
<meta property="og:title" content="Overview">
<meta property="og:url" content="https://gojay.top/2020/04/01/Overview/index.html">
<meta property="og:site_name" content="Gojay&#39;s Records">
<meta property="og:description" content="There are the overall of papers about Deep Learning.https://github.com/Gojay001/DeepLearning-pwcn">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://gojay.top/gallery/thumbnails/DL.jpg">
<meta property="og:updated_time" content="2021-06-04T07:47:32.091Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Overview">
<meta name="twitter:description" content="There are the overall of papers about Deep Learning.https://github.com/Gojay001/DeepLearning-pwcn">
<meta name="twitter:image" content="https://gojay.top/gallery/thumbnails/DL.jpg">







<link rel="icon" href="/images/head.jpg">


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-dark.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
<script>
var _hmt = _hmt || [];
(function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?f29d95e89a802be01b6ea105a1b4e115";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script>

    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    

    
    
    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    

    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-3-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                Gojay&#39;s Records
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item" href="/">Home</a>
                
                <a class="navbar-item" href="/archives/">Archives</a>
                
                <a class="navbar-item" href="/categories/">Categories</a>
                
                <a class="navbar-item" href="/tags/">Tags</a>
                
                <a class="navbar-item" href="https://www.zhihu.com/people/gojay/activities">ZhiHu</a>
                
                <a class="navbar-item" href="/about/">About</a>
                
                <a class="navbar-item" href="/rss.xml">RSS</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    
                    <a class="navbar-item" target="_blank" title="Gojay GitHub" href="https://www.github.com/Gojay001">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main"><div class="card">
    
    <div class="card-image">
        <span class="image is-7by1">
            <img class="thumbnail lazy" data-src="/gallery/thumbnails/DL.jpg" alt="Overview">
        </span>
    </div>
    
    <div class="card-content article ">
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <i class="fas fa-angle-double-right"></i>Overview
            
        </h1>
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                
                <div class="level-item tag is-danger" style="background-color: #3273dc;">Pin</div>
                
                <time class="level-item has-text-grey" datetime="2020-04-01T02:32:41.000Z"><i class="far fa-calendar-alt">&nbsp;</i>2020-04-01</time>
                
                <time class="level-item has-text-grey is-hidden-mobile" datetime="2021-06-04T07:47:32.091Z"><i class="far fa-calendar-check">&nbsp;</i>2021-06-04</time>
                
                
                <div class="level-item">
                <i class="far fa-folder-open has-text-grey"></i>&nbsp;
                <a class="has-link-grey -link" href="/categories/DeepLearning/">DeepLearning</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/DeepLearning/Overview/">Overview</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    <i class="far fa-clock"></i>&nbsp;
                    
                    
                    21 minutes read (About 3143 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <div class="content">
            <blockquote>
<p>There are the overall of papers about <strong>Deep Learning</strong>.<br><a href="https://github.com/Gojay001/DeepLearning-pwcn" target="_blank" rel="noopener">https://github.com/Gojay001/DeepLearning-pwcn</a></p>
</blockquote>
<a id="more"></a>

<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><hr>
<ul>
<li><a href="#Image-Classification">Classification</a><ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>NIN(Network In Network)</li>
<li>VGG</li>
<li>GoogLeNet(Inception-v1)</li>
<li>ResNet</li>
<li>Inception-v4</li>
<li>DenseNet</li>
<li>DLA(Deep Layer Aggregation)</li>
<li>ShuffleNet</li>
<li>MobileNetV3</li>
</ul>
</li>
<li><a href="#Object-Detection">Detection</a><ul>
<li>One-stage<ul>
<li>SSD</li>
<li>YOLO</li>
<li>YOLOv2</li>
<li>RetinaNet</li>
<li>YOLOv3</li>
<li>CornerNet</li>
<li>CenterNet</li>
<li>YOLOv4</li>
<li>YOLOF</li>
</ul>
</li>
<li>Two-stage<ul>
<li>R-CNN</li>
<li>SPP</li>
<li>Fast R-CNN</li>
<li>Faster R-CNN</li>
<li>FPN</li>
</ul>
</li>
</ul>
</li>
<li><a href="#Object-Segmentation">Segmentation</a><ul>
<li>FCN</li>
<li>U-Net</li>
<li>Seg-Net</li>
<li>DeepLab V1</li>
<li>PSPNet</li>
<li>DeepLab V2</li>
<li>Mask R-CNN</li>
<li>DeepLab V3</li>
<li>PointNet</li>
<li>PointNet++</li>
<li>DeepLab V3+</li>
<li>DGCNet</li>
<li>SETR</li>
<li>Segmenter</li>
<li>SegFormer</li>
</ul>
</li>
<li><a href="#Object-Tracking">Tracking</a><ul>
<li>MOT<ul>
<li>SORT</li>
<li>DeepSORT</li>
<li>Tracktor</li>
<li>Flow-Fuse Tracker</li>
<li>JRMOT</li>
<li>Tracklet</li>
<li>FairMOT</li>
<li>DMCT(Deep Multi-Camera Tracking)</li>
<li>CenterPoint</li>
</ul>
</li>
<li>VOT<ul>
<li>DepthTrack</li>
<li>BinocularTrack</li>
<li>SiamFC</li>
<li>SiamRPN</li>
<li>SiamRPN++</li>
<li>SiamMask</li>
<li>GlobalTrack</li>
<li>PAMCC-AOT</li>
<li>SiamCAR</li>
<li>SiamBAN</li>
<li>SiamAttn</li>
<li>TSDM</li>
<li>RE-SiamNets</li>
</ul>
</li>
</ul>
</li>
<li><a href="#Few-Shot-Segmentation">FSS</a><ul>
<li>OSLSM</li>
<li>co-FCN</li>
<li>AMP(Adaptive Masked Proxies)</li>
<li>SG-One(Similarity Guidance)</li>
<li>CENet(Combinatorial Embedding Network)</li>
<li>PANet(Prototype Alignment)</li>
<li>CANet(Class Agnostic)</li>
<li>PGNet(Pyramid Graph Network)</li>
<li>CRNet(Cross-Reference Network)</li>
<li>FGN(Fully Guided Network)</li>
<li>OTB(On the Texture Bias)</li>
<li>LTM(Local Transformation Module)</li>
<li>SimPropNet(Similarity Propagation)</li>
<li>PPNet(Part-aware Prototype)</li>
<li>PFENet(Prior Guided Feature Enrichment Network)</li>
<li>PMMs(Prototype Mixture Models)</li>
<li>GFS-Seg(Generalized Few-Shot)</li>
<li>SCL(Self-Corss Learning)</li>
<li>ASGNet(Adaptive Superpixel-guided Network)</li>
</ul>
</li>
<li><a href="#Attention-or-Transformer">Attention</a><ul>
<li>Transformer</li>
<li>Non-local</li>
<li>Image Transformer</li>
<li>ViT(Vision Transformer)</li>
<li>Swin Transformer</li>
<li>ResT</li>
<li>DS-Net(Dual Stream Network)</li>
</ul>
</li>
<li><a href="#Salient-Object-Detection">RGBD-SOT</a><ul>
<li>UC-Net</li>
<li>JL-DCF(Joint Learning and Densely-Cooperative Fusion)</li>
<li>SA-Gate(Separation-and-Aggregation Gate)</li>
<li>BiANet(Bilateral Attention Network)</li>
</ul>
</li>
<li><a href="#Unsupervised-Learning">Unsupervised</a><ul>
<li>SimSiam</li>
</ul>
</li>
<li><a href="#3D-Object-Detection">Detection-3D</a><ul>
<li>PV-RCNN</li>
</ul>
</li>
<li><a href="#Few-Shot-Learning">FSL</a><ul>
<li>RN(Relation Network)</li>
</ul>
</li>
<li><a href="#Generative-Adversarial-Network">GAN</a><ul>
<li>GAN</li>
<li>BeautyGAN</li>
</ul>
</li>
<li><a href="#Optimization">Optimization</a><ul>
<li>ReLU</li>
<li>Momentum</li>
<li>Dropout</li>
<li>Adam</li>
<li>BN</li>
<li>GDoptimization</li>
</ul>
</li>
<li><a href="#Survey">Survey</a><ul>
<li>3D-Detection-Survey-2019</li>
<li>FSL-Survey-2019</li>
<li>MOT-Survey-2020</li>
</ul>
</li>
</ul>
<h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">LeNet-5</td>
<td align="center"><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Gradient-based learning applied to document recognition</a></td>
<td align="center">IEEE(1998)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">AlexNet</td>
<td align="center"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a></td>
<td align="center">NIPS(2012)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2019/08/31/NIN-Network-In-Network/">NIN</a></td>
<td align="center"><a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network In Network</a></td>
<td align="center">arXiv(2013)</td>
<td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/NIN/Code" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">VGG</td>
<td align="center"><a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></td>
<td align="center">ICLR(2015)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2019/09/05/GoogLeNet/">GoogLeNet</a></td>
<td align="center"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Going deeper with convolutions</a></td>
<td align="center">CVPR(2015)</td>
<td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/GoogLeNet/Code" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2019/09/08/ResNet/">ResNet</a></td>
<td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a></td>
<td align="center">CVPR(2016)</td>
<td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Classification/ResNet/Code" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">Inception-v4</td>
<td align="center"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14806/14311" target="_blank" rel="noopener">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></td>
<td align="center">AAAI(2017)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">DenseNet</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Densely Connected Convolutional Networks</a></td>
<td align="center">CVPR(2017)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">DLA</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Deep_Layer_Aggregation_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Deep Layer Aggregation</a></td>
<td align="center">CVPR(2018)</td>
<td align="center"><a href="https://github.com/ucbdrive/dla" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">ShuffleNet</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_ShuffleNet_An_Extremely_CVPR_2018_paper.pdf" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></td>
<td align="center">CVPR(2018)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">MobileNetV3</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Howard_Searching_for_MobileNetV3_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Searching for MobileNetV3</a></td>
<td align="center">ICCV(2019)</td>
<td align="center">[code]</td>
</tr>
</tbody></table>
<blockquote>
<p>More information can be found in <a href="https://github.com/weiaicunzai/awesome-image-classification" target="_blank" rel="noopener">Awesome - Image Classification</a>.</p>
</blockquote>
<h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">R-CNN</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf" target="_blank" rel="noopener">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a></td>
<td align="center">CVPR(2014)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">SPP</td>
<td align="center"><a href="https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></td>
<td align="center">TPAMI(2015)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">Fast R-CNN</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">Fast R-CNN</a></td>
<td align="center">ICCV(2015)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2019/10/19/Faster-R-CNN/">Faster R-CNN</a></td>
<td align="center"><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></td>
<td align="center">NIPS(2015)</td>
<td align="center"><a href="https://github.com/Gojay001/faster-rcnn.pytorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SSD</td>
<td align="center"><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></td>
<td align="center">ECCV(2016)</td>
<td align="center"><a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">Caffe</a></td>
</tr>
<tr>
<td align="left">YOLO</td>
<td align="center"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></td>
<td align="center">CVPR(2016)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">YOLOv2</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a></td>
<td align="center">CVPR(2017)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">FPN</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Feature Pyramid Networks for Object Detection</a></td>
<td align="center">CVPR(2017)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">RetinaNet</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Focal Loss for Dense Object Detection</a></td>
<td align="center">ICCV(2017)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">YOLOv3</td>
<td align="center"><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener">YOLOv3: An Incremental Improvement</a></td>
<td align="center">arXiv(2018)</td>
<td align="center"><a href="https://github.com/pjreddie/darknet" target="_blank" rel="noopener">Offical</a></td>
</tr>
<tr>
<td align="left">CornerNet</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.pdf" target="_blank" rel="noopener">CornerNet: Detecting Objects as Paired Keypoints</a></td>
<td align="center">ECCV(2018)</td>
<td align="center"><a href="https://github.com/princeton-vl/CornerNet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">CenterNet</td>
<td align="center"><a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">Objects as Points</a></td>
<td align="center">arXiv(2019)</td>
<td align="center"><a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">YOLOv4</td>
<td align="center"><a href="https://arxiv.org/abs/2004.10934" target="_blank" rel="noopener">YOLOv4: Optimal Speed and Accuracy of Object Detection</a></td>
<td align="center">arXiv(2020)</td>
<td align="center"><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">Offical</a></td>
</tr>
<tr>
<td align="left">YOLOF</td>
<td align="center"><a href="https://arxiv.org/pdf/2103.09460.pdf" target="_blank" rel="noopener">You Only Look One-level Feature</a></td>
<td align="center">arXiv(2021)</td>
<td align="center"><a href="https://github.com/megvii-model/YOLOF" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<blockquote>
<p>More information can be found in <a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">awesome-object-detection</a>.</p>
</blockquote>
<h2 id="Object-Segmentation"><a href="#Object-Segmentation" class="headerlink" title="Object Segmentation"></a>Object Segmentation</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">FCN</td>
<td align="center"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf" target="_blank" rel="noopener">Fully convolutional networks for semantic segmentation</a></td>
<td align="center">CVPR(2015)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">U-Net</td>
<td align="center"><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></td>
<td align="center">MICCAI(2015)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">Seg-Net</td>
<td align="center"><a href="https://arxiv.org/abs/1505.07293" target="_blank" rel="noopener">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Robust Semantic Pixel-Wise Labelling</a></td>
<td align="center">arXiv(2015)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DeepLab V1</td>
<td align="center"><a href="https://arxiv.org/abs/1412.7062" target="_blank" rel="noopener">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></td>
<td align="center">arXiv(2014) / ICLR(2015)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">PSPNet</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Pyramid_Scene_Parsing_CVPR_2017_paper.pdf" target="_blank" rel="noopener">Pyramid Scene Parsing Network</a></td>
<td align="center">CVPR(2017)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DeepLab V2</td>
<td align="center"><a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></td>
<td align="center">arXiv(2016) / TPAMI(2017)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/08/17/Mask-R-CNN/">Mask R-CNN</a></td>
<td align="center"><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf" target="_blank" rel="noopener">Mask R-CNN</a></td>
<td align="center">ICCV / TPAMI(2017)</td>
<td align="center"><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DeepLab V3</td>
<td align="center"><a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">Rethinking Atrous Convolution for Semantic Image Segmentation</a></td>
<td align="center">arXiv(2017)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">PointNet</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</a></td>
<td align="center">CVPR(2017)</td>
<td align="center"><a href="https://github.com/fxia22/pointnet.pytorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">PointNet++</td>
<td align="center"><a href="https://proceedings.neurips.cc/paper/2017/file/d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf" target="_blank" rel="noopener">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></td>
<td align="center">NIPS(2017)</td>
<td align="center"><a href="https://github.com/erikwijmans/Pointnet2_PyTorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DeepLab V3+</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></td>
<td align="center">ECCV(2018)</td>
<td align="center"><a href="https://github.com/yassouali/pytorch_segmentation" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DGCNet</td>
<td align="center"><a href="https://arxiv.org/pdf/1909.06121.pdf" target="_blank" rel="noopener">Dual Graph Convolutional Network for Semantic Segmentation</a></td>
<td align="center">BMVC(2019)</td>
<td align="center"><a href="https://github.com/lxtGH/GALD-DGCNet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SETR</td>
<td align="center"><a href="http://arxiv.org/abs/2012.15840" target="_blank" rel="noopener">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</a></td>
<td align="center">CVPR(2021)</td>
<td align="center"><a href="https://github.com/fudan-zvg/SETR" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">Segmenter</td>
<td align="center"><a href="http://arxiv.org/abs/2105.05633" target="_blank" rel="noopener">Segmenter: Transformer for Semantic Segmentation</a></td>
<td align="center">arXiv(2021)</td>
<td align="center"><a href="https://github.com/rstrudel/segmenter" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SegFormer</td>
<td align="center"><a href="http://arxiv.org/abs/2105.15203" target="_blank" rel="noopener">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</a></td>
<td align="center">arXiv(2021)</td>
<td align="center"><a href="https://github.com/NVlabs/SegFormer" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<h2 id="Object-Tracking"><a href="#Object-Tracking" class="headerlink" title="Object Tracking"></a>Object Tracking</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><a href="https://gojay.top/2020/06/14/SORT/">SORT</a></td>
<td align="center"><a href="https://arxiv.org/abs/1602.00763" target="_blank" rel="noopener">Simple Online and Realtime Tracking</a></td>
<td align="center">ICIP(2016)</td>
<td align="center"><a href="https://github.com/abewley/sort" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DepthTrack</td>
<td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/tree/master/Tracking/Binocular%20camera/DepthTrack.pdf" target="_blank" rel="noopener">Real-time depth-based tracking using a binocular camera</a></td>
<td align="center">WCICA(2016)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/06/20/DeepSORT/">DeepSORT</a></td>
<td align="center"><a href="https://arxiv.org/abs/1703.07402" target="_blank" rel="noopener">Simple Online and Realtime Tracking with a Deep Association Metric</a></td>
<td align="center">ICIP(2017)</td>
<td align="center"><a href="https://github.com/nwojke/deep_sort" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">BinocularTrack</td>
<td align="center"><a href="https://github.com/Gojay001/DeepLearning-pwcn/blob/master/Tracking/Binocular%20camera/BinocularTrack.pdf" target="_blank" rel="noopener">Research on Target Tracking Algorithm Based on Parallel Binocular Camera</a></td>
<td align="center">ITAIC(2019)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">SiamFC</td>
<td align="center"><a href="https://arxiv.org/abs/1606.09549" target="_blank" rel="noopener">Fully-Convolutional Siamese Networks for Object Tracking</a></td>
<td align="center">ECCV(2016)</td>
<td align="center"><a href="https://github.com/zllrunning/SiameseX.PyTorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SiamRPN</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf" target="_blank" rel="noopener">High Performance Visual Tracking with Siamese Region Proposal Network</a></td>
<td align="center">CVPR(2018)</td>
<td align="center"><a href="https://github.com/huanglianghua/siamrpn-pytorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/05/09/SiamRPN++/">SiamRPN++</a></td>
<td align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_SiamRPN_Evolution_of_Siamese_Visual_Tracking_With_Very_Deep_Networks_CVPR_2019_paper.pdf" target="_blank" rel="noopener">SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks</a></td>
<td align="center">CVPR(2019)</td>
<td align="center"><a href="https://github.com/STVIR/pysot" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2019/11/26/SiamMask/">SiamMask</a></td>
<td align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf" target="_blank" rel="noopener">Fast Online Object Tracking and Segmentation: A Unifying Approach</a></td>
<td align="center">CVPR(2019)</td>
<td align="center"><a href="https://github.com/Gojay001/SiamMask" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2019/11/09/Tracktor/">Tracktor</a></td>
<td align="center"><a href="https://arxiv.org/abs/1903.05625" target="_blank" rel="noopener">Tracking without bells and whistles</a></td>
<td align="center">ICCV(2019)</td>
<td align="center"><a href="https://github.com/Gojay001/tracking_wo_bnw" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/01/04/GlobalTrack/">GlobalTrack</a></td>
<td align="center"><a href="https://arxiv.org/abs/1912.08531" target="_blank" rel="noopener">GlobalTrack: A Simple and Strong Baseline for Long-term Tracking</a></td>
<td align="center">AAAI(2020)</td>
<td align="center"><a href="https://github.com/huanglianghua/GlobalTrack" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SiamCAR</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_SiamCAR_Siamese_Fully_Convolutional_Classification_and_Regression_for_Visual_Tracking_CVPR_2020_paper.pdf" target="_blank" rel="noopener">SiamCAR: Siamese Fully Convolutional Classification and Regression for Visual Tracking</a></td>
<td align="center">CVPR(2020)</td>
<td align="center"><a href="https://github.com/ohhhyeahhh/SiamCAR" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SiamBAN</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Siamese_Box_Adaptive_Network_for_Visual_Tracking_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Siamese Box Adaptive Network for Visual Tracking</a></td>
<td align="center">CVPR(2020)</td>
<td align="center"><a href="https://github.com/hqucv/siamban" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SiamAttn</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yu_Deformable_Siamese_Attention_Networks_for_Visual_Object_Tracking_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Deformable Siamese Attention Networks for Visual Object Tracking</a></td>
<td align="center">CVPR(2020)</td>
<td align="center"><a href="https://github.com/msight-tech/research-siamattn" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/02/25/PAMCC-AOT/">PAMCC-AOT</a></td>
<td align="center"><a href="https://arxiv.org/abs/2001.05161" target="_blank" rel="noopener">Pose-Assisted Multi-Camera Collaboration for Active Object Tracking</a></td>
<td align="center">AAAI(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/03/05/FFT-Flow-Fuse-Tracker/">FFT</a></td>
<td align="center"><a href="https://arxiv.org/abs/2001.11180" target="_blank" rel="noopener">Multiple Object Tracking by Flowing and Fusing</a></td>
<td align="center">arXiv(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/02/28/JRMOT/">JRMOT</a></td>
<td align="center"><a href="https://arxiv.org/abs/2002.08397" target="_blank" rel="noopener">JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</a></td>
<td align="center">arXiv(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/03/26/Tracklet/">Tracklet</a></td>
<td align="center"><a href="https://arxiv.org/abs/2003.02795" target="_blank" rel="noopener">Multi-object Tracking via End-to-end Tracklet Searching and Ranking</a></td>
<td align="center">arXiv(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/05/23/TSDM/">TSDM</a></td>
<td align="center"><a href="https://arxiv.org/abs/2005.04063" target="_blank" rel="noopener">TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator</a></td>
<td align="center">arXiv(2020)</td>
<td align="center"><a href="https://github.com/Gojay001/TSDM" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/05/25/FairMOT/">FairMOT</a></td>
<td align="center"><a href="https://arxiv.org/abs/2004.01888" target="_blank" rel="noopener">A Simple Baseline for Multi-Object Tracking</a></td>
<td align="center">arXiv(2020)</td>
<td align="center"><a href="https://github.com/Gojay001/FairMOT" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DMCT</td>
<td align="center"><a href="https://arxiv.org/abs/2003.11753" target="_blank" rel="noopener">Real-time 3D Deep Multi-Camera Tracking</a></td>
<td align="center">arXiv(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">RE-SiamNets</td>
<td align="center"><a href="https://arxiv.org/abs/2012.13078" target="_blank" rel="noopener">Rotation Equivariant Siamese Networks for Tracking</a></td>
<td align="center">CVPR(2021)</td>
<td align="center"><a href="https://github.com/dkgupta90/re-siamnet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">CenterPoint</td>
<td align="center"><a href="https://arxiv.org/pdf/2006.11275.pdf" target="_blank" rel="noopener">Center-based 3D Object Detection and Tracking</a></td>
<td align="center">CVPR(2021)</td>
<td align="center"><a href="https://github.com/tianweiy/CenterPoint" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<h2 id="Few-Shot-Segmentation"><a href="#Few-Shot-Segmentation" class="headerlink" title="Few-Shot Segmentation"></a>Few-Shot Segmentation</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><a href="https://gojay.top/2020/10/19/OSLSM/">OSLSM</a></td>
<td align="center"><a href="https://arxiv.org/abs/1709.03410" target="_blank" rel="noopener">One-Shot Learning for Semantic Segmentation</a></td>
<td align="center">BMVC(2017)</td>
<td align="center"><a href="https://github.com/lzzcd001/OSLSM" target="_blank" rel="noopener">Caffe</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/10/19/co-FCN/">co-FCN</a></td>
<td align="center"><a href="https://openreview.net/pdf?id=SkMjFKJwG" target="_blank" rel="noopener">Conditional Networks for Few-Shot Semantic Segmentation</a></td>
<td align="center">ICLR(2018)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">AMP</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Siam_AMP_Adaptive_Masked_Proxies_for_Few-Shot_Segmentation_ICCV_2019_paper.pdf" target="_blank" rel="noopener">AMP: Adaptive Masked Proxies for Few-Shot Segmentation</a></td>
<td align="center">ICCV(2019)</td>
<td align="center"><a href="https://github.com/MSiam/AdaptiveMaskedProxies" target="_blank" rel="noopener">Pytorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/10/20/SG-One/">SG-One</a></td>
<td align="center"><a href="https://arxiv.org/abs/1810.09091" target="_blank" rel="noopener">SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation</a></td>
<td align="center">arXiv(2018) / TCYB(2020)</td>
<td align="center"><a href="https://github.com/xiaomengyc/SG-One" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">CENet</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Combinatorial_Embedding_Networks_for_Deep_Graph_Matching_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Learning Combinatorial Embedding Networks for Deep Graph Matching</a></td>
<td align="center">ICCV(2019)</td>
<td align="center"><a href="https://github.com/Thinklab-SJTU/PCA-GM" target="_blank" rel="noopener">Pytorch</a></td>
</tr>
<tr>
<td align="left">PANet</td>
<td align="center"><a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf" target="_blank" rel="noopener">PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</a></td>
<td align="center">ICCV(2019)</td>
<td align="center"><a href="https://github.com/kaixin96/PANet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/10/20/CANet/">CANet</a></td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_CANet_Class-Agnostic_Segmentation_Networks_With_Iterative_Refinement_and_Attentive_Few-Shot_CVPR_2019_paper.pdf" target="_blank" rel="noopener">CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning</a></td>
<td align="center">CVPR(2019)</td>
<td align="center"><a href="https://github.com/icoz69/CaNet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/07/28/PGNet/">PGNet</a></td>
<td align="center"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhang_Pyramid_Graph_Networks_With_Connection_Attentions_for_Region-Based_One-Shot_Semantic_ICCV_2019_paper.pdf" target="_blank" rel="noopener">Pyramid Graph Networks with Connection Attentions for Region-Based One-Shot Semantic Segmentation</a></td>
<td align="center">ICCV(2019)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/07/10/CRNet/">CRNet</a></td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_CRNet_Cross-Reference_Networks_for_Few-Shot_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">CRNet: Cross-Reference Networks for Few-Shot Segmentation</a></td>
<td align="center">CVPR(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">FGN</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_FGN_Fully_Guided_Network_for_Few-Shot_Instance_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">FGN: Fully Guided Network for Few-Shot Instance Segmentation</a></td>
<td align="center">CVPR(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">OTB</td>
<td align="center"><a href="https://arxiv.org/abs/2003.04052" target="_blank" rel="noopener">On the Texture Bias for Few-Shot CNN Segmentation</a></td>
<td align="center">arXiv(2020)</td>
<td align="center"><a href="https://github.com/rezazad68/fewshot-segmentation" target="_blank" rel="noopener">TensorFlow</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/07/29/LTM/">LTM</a></td>
<td align="center"><a href="https://arxiv.org/abs/1910.05886" target="_blank" rel="noopener">A New Local Transformation Module for Few-Shot Segmentation</a></td>
<td align="center">MMMM(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">SimPropNet</td>
<td align="center"><a href="https://arxiv.org/abs/2004.15014" target="_blank" rel="noopener">SimPropNet: Improved Similarity Propagation for Few-shot Image Segmentation</a></td>
<td align="center">IJCAI(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/12/02/PPNet/">PPNet</a></td>
<td align="center"><a href="https://arxiv.org/abs/2007.06309" target="_blank" rel="noopener">Part-aware Prototype Network for Few-shot Semantic Segmentation</a></td>
<td align="center">ECCV(2020)</td>
<td align="center"><a href="https://github.com/Xiangyi1996/PPNet-PyTorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">PFENet</td>
<td align="center"><a href="https://arxiv.org/abs/2008.01449" target="_blank" rel="noopener">PFENet: Prior Guided Feature Enrichment Network for Few-shot Segmentation</a></td>
<td align="center">TPAMI(2020)</td>
<td align="center"><a href="https://github.com/Jia-Research-Lab/PFENet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">PMMs</td>
<td align="center"><a href="https://arxiv.org/abs/2008.03898" target="_blank" rel="noopener">Prototype Mixture Models for Few-shot Semantic Segmentation</a></td>
<td align="center">ECCV(2020)</td>
<td align="center"><a href="https://github.com/Yang-Bob/PMMs" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">GFS-Seg</td>
<td align="center"><a href="https://arxiv.org/abs/2010.05210" target="_blank" rel="noopener">Generalized Few-Shot Semantic Segmentation</a></td>
<td align="center">arXiv(2020)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">SCL</td>
<td align="center"><a href="https://arxiv.org/pdf/2103.16129.pdf" target="_blank" rel="noopener">Self-Guided and Cross-Guided Learning for Few-Shot Segmentation</a></td>
<td align="center">CVPR(2021)</td>
<td align="center"><a href="https://github.com/zbf1991/SCL" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">ASGNet</td>
<td align="center"><a href="https://arxiv.org/pdf/2104.01893.pdf" target="_blank" rel="noopener">Adaptive Prototype Learning and Allocation for Few-Shot Segmentation</a></td>
<td align="center">CVPR(2021)</td>
<td align="center"><a href="https://github.com/Reagan1311/ASGNet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<blockquote>
<p>More information can be found in <a href="https://github.com/xiaomengyc/Few-Shot-Semantic-Segmentation-Papers" target="_blank" rel="noopener">Few-Shot-Semantic-Segmentation-Papers</a>.</p>
</blockquote>
<h2 id="Attention-or-Transformer"><a href="#Attention-or-Transformer" class="headerlink" title="Attention or Transformer"></a>Attention or Transformer</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Transformer</td>
<td align="center"><a href="http://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a></td>
<td align="center">arXiv(2017)</td>
<td align="center"><a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">TensorFlow</a></td>
</tr>
<tr>
<td align="left">Non-local</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Non-local Neural Networks</a></td>
<td align="center">CVPR(2018)</td>
<td align="center"><a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/05/15/Image-Transformer/">ImageTransformer</a></td>
<td align="center"><a href="https://arxiv.org/abs/1802.05751" target="_blank" rel="noopener">Image Transformer</a></td>
<td align="center">arXiv(2018)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">ViT</td>
<td align="center"><a href="http://arxiv.org/abs/2010.11929" target="_blank" rel="noopener">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></td>
<td align="center">arXiv(2020)</td>
<td align="center"><a href="https://github.com/google-research/vision_transformer" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">Swin Transformer</td>
<td align="center"><a href="https://arxiv.org/abs/2103.14030" target="_blank" rel="noopener">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></td>
<td align="center">arXiv(2021)</td>
<td align="center"><a href="https://github.com/microsoft/Swin-Transformer" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">ResT</td>
<td align="center"><a href="http://arxiv.org/abs/2105.13677" target="_blank" rel="noopener">ResT: An Efficient Transformer for Visual Recognition</a></td>
<td align="center">arXiv(2021)</td>
<td align="center"><a href="https://github.com/wofmanaf/ResT" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">DS-Net</td>
<td align="center"><a href="http://arxiv.org/abs/2105.14734" target="_blank" rel="noopener">Dual-stream Network for Visual Recognition</a></td>
<td align="center">arXiv(2021)</td>
<td align="center">[code]</td>
</tr>
</tbody></table>
<h2 id="Salient-Object-Detection"><a href="#Salient-Object-Detection" class="headerlink" title="Salient Object Detection"></a>Salient Object Detection</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">UC-Net</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.pdf" target="_blank" rel="noopener">UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders</a></td>
<td align="center">CVPR(2020)</td>
<td align="center"><a href="https://github.com/JingZhang617/UCNet" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">JL-DCF</td>
<td align="center"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.pdf" target="_blank" rel="noopener">JL-DCF: Joint Learning and Densely-Cooperative Fusion Framework for RGB-D Salient Object Detection</a></td>
<td align="center">CVPR(2020)</td>
<td align="center"><a href="https://github.com/jiangyao-scu/JL-DCF-pytorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">SA-Gate</td>
<td align="center"><a href="https://arxiv.org/pdf/2007.09183.pdf" target="_blank" rel="noopener">Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation</a></td>
<td align="center">ECCV(2020)</td>
<td align="center"><a href="https://github.com/charlesCXK/RGBD_Semantic_Segmentation_PyTorch" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
<tr>
<td align="left">BiANet</td>
<td align="center"><a href="https://arxiv.org/pdf/2004.14582.pdf" target="_blank" rel="noopener">Bilateral Attention Network for RGB-D Salient Object Detection</a></td>
<td align="center">TIP(2021)</td>
<td align="center">[Code]</td>
</tr>
</tbody></table>
<h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">SimSiam</td>
<td align="center"><a href="https://arxiv.org/abs/2011.10566" target="_blank" rel="noopener">Exploring Simple Siamese Representation Learning</a></td>
<td align="center">CVPR(2021)</td>
<td align="center"><a href="https://github.com/PatrickHua/SimSiam" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<h2 id="3D-Object-Detection"><a href="#3D-Object-Detection" class="headerlink" title="3D Object Detection"></a>3D Object Detection</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><a href="https://gojay.top/2020/06/23/PV-RCNN/">PV-RCNN</a></td>
<td align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_PV-RCNN_Point-Voxel_Feature_Set_Abstraction_for_3D_Object_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection</a></td>
<td align="center">CVPR(2020)</td>
<td align="center"><a href="https://github.com/sshaoshuai/PV-RCNN" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<h2 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><a href="https://gojay.top/2019/08/21/RN-Realation-Network/">RN</a></td>
<td align="center"><a href="https://arxiv.org/abs/1711.06025" target="_blank" rel="noopener">Learning to Compare: Relation Network for Few-Shot Learning</a></td>
<td align="center">CVPR(2018)</td>
<td align="center"><a href="https://github.com/Gojay001/LearningToCompare_FSL" target="_blank" rel="noopener">PyTorch</a></td>
</tr>
</tbody></table>
<h2 id="Generative-Adversarial-Network"><a href="#Generative-Adversarial-Network" class="headerlink" title="Generative Adversarial Network"></a>Generative Adversarial Network</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">GAN</td>
<td align="center"><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener">Generative Adversarial Networks</a></td>
<td align="center">arXiv(2014)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">BeautyGAN</td>
<td align="center"><a href="http://liusi-group.com/pdf/BeautyGAN-camera-ready_2.pdf" target="_blank" rel="noopener">BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network</a></td>
<td align="center">ACM MM(2018)</td>
<td align="center"><a href="http://liusi-group.com/projects/BeautyGAN" target="_blank" rel="noopener">TensorFlow</a></td>
</tr>
</tbody></table>
<h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
<th align="center">Code</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ReLU</td>
<td align="center"><a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf" target="_blank" rel="noopener">Deep Sparse Rectifier Neural Networks</a></td>
<td align="center">JMLR(2011)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">Momentum</td>
<td align="center"><a href="http://proceedings.mlr.press/v28/sutskever13.html" target="_blank" rel="noopener">On the importance of initialization and momentum in deep learning</a></td>
<td align="center">ICML(2013)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">Dropout</td>
<td align="center"><a href="https://dl.acm.org/doi/10.5555/2627435.2670313" target="_blank" rel="noopener">Dropout: a simple way to prevent neural networks from overfitting</a></td>
<td align="center">JMLR(2014)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">Adam</td>
<td align="center"><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization</a></td>
<td align="center">ICLR(2015)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">BN</td>
<td align="center"><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></td>
<td align="center">ICML(2015)</td>
<td align="center">[code]</td>
</tr>
<tr>
<td align="left">GDoptimization</td>
<td align="center"><a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></td>
<td align="center">arXiv(2016)</td>
<td align="center">[code]</td>
</tr>
</tbody></table>
<h2 id="Survey"><a href="#Survey" class="headerlink" title="Survey"></a>Survey</h2><table>
<thead>
<tr>
<th align="left">Title</th>
<th align="center">Paper</th>
<th align="center">Conf</th>
</tr>
</thead>
<tbody><tr>
<td align="left">3D-Detection-Survey-2019</td>
<td align="center"><a href="http://wrap.warwick.ac.uk/114314/1/WRAP-survey-3D-object-detection-methods-autonomous-driving-applications-Arnold-2019.pdf" target="_blank" rel="noopener">A Survey on 3D Object Detection Methods for Autonomous Driving Applications</a></td>
<td align="center">ITS(2019)</td>
</tr>
<tr>
<td align="left"><a href="https://gojay.top/2020/07/07/FSL-Survey-2019/">FSL-Survey-2019</a></td>
<td align="center"><a href="https://arxiv.org/abs/1904.05046" target="_blank" rel="noopener">Generalizing from a Few Examples: A Survey on Few-Shot Learning</a></td>
<td align="center">CSUR(2019)</td>
</tr>
<tr>
<td align="left">MOT-Survey-2020</td>
<td align="center"><a href="https://arxiv.org/abs/1907.12740" target="_blank" rel="noopener">Deep Learning in Video Multi-Object Tracking: A Survey</a></td>
<td align="center">Neurocomputing(2020)</td>
</tr>
</tbody></table>

        </div>
        
            <ul class="post-copyright">
            <li><strong>Title：</strong><a href="https://gojay.top/2020/04/01/Overview/">Overview</a></li>
            <li><strong>Author：</strong><a href="https://gojay.top">Gojay</a></li>
            <li><strong>Link：</strong><a href="https://gojay.top/2020/04/01/Overview/">https://gojay.top/2020/04/01/Overview/</a></li>
            <li><strong>Date：</strong>2020-04-01</li>
            <li><strong>Copyright：</strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.
            </li>
            </ul>
        
        
        <hr style="height:1px;margin:1rem 0">
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <i class="fas fa-tags has-text-grey"></i>&nbsp;
                    <a class="has-link-grey -link" href="/tags/DL/">DL</a>,&nbsp;<a class="has-link-grey -link" href="/tags/Overview/">Overview</a>
                </div>
            </div>
        </div>
        
        
        
        <div class="social-share"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css">
<script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script>
        
    </div>
</div>





<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/2020/05/09/SiamRPN++/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">SiamRPN++</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/2020/03/26/Tracklet/">
                <span class="level-item">Tracklet</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="valine-thread" class="content"></div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
    new Valine({
        el: '#valine-thread' ,
        notify: true,
        verify: true,
        app_id: '6kgdHh2dtxHKbi9fwyPjsA7t-gzGzoHsz',
        app_key: 'N4XHHmLsjyHwx2ENVEGEn9Br',
        placeholder: 'Say something...'
    });
</script>

    </div>
</div>

</div>
                




<div class="column is-4-tablet is-4-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget column-left is-sticky" id="toc">
    <div class="card-content">
        <div class="menu" style="max-height: 800px; overflow: auto;">
            <h3 class="menu-label">
                Catalogue
            </h3>
            <ul class="menu-list"><li>
        <a class="is-flex" href="#Contents">
        <span class="has-mr-6">1</span>
        <span>Contents</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Image-Classification">
        <span class="has-mr-6">1.1</span>
        <span>Image Classification</span>
        </a></li><li>
        <a class="is-flex" href="#Object-Detection">
        <span class="has-mr-6">1.2</span>
        <span>Object Detection</span>
        </a></li><li>
        <a class="is-flex" href="#Object-Segmentation">
        <span class="has-mr-6">1.3</span>
        <span>Object Segmentation</span>
        </a></li><li>
        <a class="is-flex" href="#Object-Tracking">
        <span class="has-mr-6">1.4</span>
        <span>Object Tracking</span>
        </a></li><li>
        <a class="is-flex" href="#Few-Shot-Segmentation">
        <span class="has-mr-6">1.5</span>
        <span>Few-Shot Segmentation</span>
        </a></li><li>
        <a class="is-flex" href="#Attention-or-Transformer">
        <span class="has-mr-6">1.6</span>
        <span>Attention or Transformer</span>
        </a></li><li>
        <a class="is-flex" href="#Salient-Object-Detection">
        <span class="has-mr-6">1.7</span>
        <span>Salient Object Detection</span>
        </a></li><li>
        <a class="is-flex" href="#Unsupervised-Learning">
        <span class="has-mr-6">1.8</span>
        <span>Unsupervised Learning</span>
        </a></li><li>
        <a class="is-flex" href="#3D-Object-Detection">
        <span class="has-mr-6">1.9</span>
        <span>3D Object Detection</span>
        </a></li><li>
        <a class="is-flex" href="#Few-Shot-Learning">
        <span class="has-mr-6">1.10</span>
        <span>Few-Shot Learning</span>
        </a></li><li>
        <a class="is-flex" href="#Generative-Adversarial-Network">
        <span class="has-mr-6">1.11</span>
        <span>Generative Adversarial Network</span>
        </a></li><li>
        <a class="is-flex" href="#Optimization">
        <span class="has-mr-6">1.12</span>
        <span>Optimization</span>
        </a></li><li>
        <a class="is-flex" href="#Survey">
        <span class="has-mr-6">1.13</span>
        <span>Survey</span>
        </a></li></ul></li></ul>
        </div>
    </div>
</div>


    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    Gojay&#39;s Records
                
                </a>
                <p class="is-size-7">
                &copy; 2021 Gojay&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a href="http://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                <br>
                
                <span id="busuanzi_container_site_pv">
                <span id="busuanzi_value_site_pv">0</span> visits
                </span>
                
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">1</span> users
                </span>
                
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                        
                        <i class="fab fa-creative-commons"></i>&nbsp;<i class="fab fa-creative-commons-by"></i>&nbsp;<i class="fab fa-creative-commons-nc"></i>&nbsp;<i class="fab fa-creative-commons-sa"></i>&nbsp;
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Gojay GitHub" href="https://www.github.com/Gojay001">
                        
                        <i class="fab fa-github"></i>&nbsp;
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>
    <script src="/js/clipboard.js" defer></script>
    

    
    
    

    


<script src="/js/main.js" defer></script>

<script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@12.0.0/dist/lazyload.min.js"></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy",
    load_delay: 300,
});
</script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something...">
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>